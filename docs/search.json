[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter12/chapter12.html#short-video-introduction",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter12/chapter12.html#short-video-introduction",
    "title": "TITLE",
    "section": "SHORT VIDEO INTRODUCTION",
    "text": "SHORT VIDEO INTRODUCTION"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter12/chapter12.html#lv1_1",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter12/chapter12.html#lv1_1",
    "title": "TITLE",
    "section": "Lv1_1",
    "text": "Lv1_1\n\nThis is level1 contents\nThis is level1 contents\nThis is level1 contents\nThis is level1 contents\nThis is level1 contents"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter12/chapter12.html#lv2_2",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter12/chapter12.html#lv2_2",
    "title": "TITLE",
    "section": "Lv2_2",
    "text": "Lv2_2"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter12/chapter12.html#discussion",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter12/chapter12.html#discussion",
    "title": "TITLE",
    "section": "Discussion",
    "text": "Discussion"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter12/chapter12.html#summary",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter12/chapter12.html#summary",
    "title": "TITLE",
    "section": "Summary",
    "text": "Summary"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter12/chapter12.html#terms",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter12/chapter12.html#terms",
    "title": "TITLE",
    "section": "Terms",
    "text": "Terms\n:::"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter12/chapter12.html#more-to-read",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter12/chapter12.html#more-to-read",
    "title": "TITLE",
    "section": "More to Read",
    "text": "More to Read"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter12/chapter12.html#assignment",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter12/chapter12.html#assignment",
    "title": "TITLE",
    "section": "Assignment",
    "text": "Assignment\n\n\n\nWhat to do\n\n\n\nRequirement\n\nPDF format\nfile name should be include your student id and name\n\nstuID_name_title.pdf (e.g. 1111111_ChungilChae_SelfIntroduction.pdf)\n\n\nDue date\n\nby Feb23(Sun) 11:59PM\nNO LATE SUBMISSION ALLOWED!!!!"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter13/chapter13.html#short-video-introduction",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter13/chapter13.html#short-video-introduction",
    "title": "TITLE",
    "section": "SHORT VIDEO INTRODUCTION",
    "text": "SHORT VIDEO INTRODUCTION"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter13/chapter13.html#lv1_1",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter13/chapter13.html#lv1_1",
    "title": "TITLE",
    "section": "Lv1_1",
    "text": "Lv1_1\n\nThis is level1 contents\nThis is level1 contents\nThis is level1 contents\nThis is level1 contents\nThis is level1 contents"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter13/chapter13.html#lv2_2",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter13/chapter13.html#lv2_2",
    "title": "TITLE",
    "section": "Lv2_2",
    "text": "Lv2_2"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter13/chapter13.html#discussion",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter13/chapter13.html#discussion",
    "title": "TITLE",
    "section": "Discussion",
    "text": "Discussion"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter13/chapter13.html#summary",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter13/chapter13.html#summary",
    "title": "TITLE",
    "section": "Summary",
    "text": "Summary"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter13/chapter13.html#terms",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter13/chapter13.html#terms",
    "title": "TITLE",
    "section": "Terms",
    "text": "Terms\n:::"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter13/chapter13.html#more-to-read",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter13/chapter13.html#more-to-read",
    "title": "TITLE",
    "section": "More to Read",
    "text": "More to Read"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter13/chapter13.html#assignment",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter13/chapter13.html#assignment",
    "title": "TITLE",
    "section": "Assignment",
    "text": "Assignment\n\n\n\nWhat to do\n\n\n\nRequirement\n\nPDF format\nfile name should be include your student id and name\n\nstuID_name_title.pdf (e.g. 1111111_ChungilChae_SelfIntroduction.pdf)\n\n\nDue date\n\nby Feb23(Sun) 11:59PM\nNO LATE SUBMISSION ALLOWED!!!!"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter3/chapter3.html#short-video-introduction",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter3/chapter3.html#short-video-introduction",
    "title": "Ch3 Researching and Writing the Literature Review",
    "section": "SHORT VIDEO INTRODUCTION",
    "text": "SHORT VIDEO INTRODUCTION"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter3/chapter3.html#literature-review-and-its-purpose",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter3/chapter3.html#literature-review-and-its-purpose",
    "title": "Ch3 Researching and Writing the Literature Review",
    "section": "Literature Review and Its Purpose",
    "text": "Literature Review and Its Purpose\n\nLiterature review: the synthesis of the entire body of all the original research studies in a specific topic of interest; not just reporting them but engaging, discussing, and synthesizing the body of work in its entirety.\nIts purpose is to demonstrate the researcher’s grasp on the topic of interest and to support their argument for the necessity of the proposed study"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter3/chapter3.html#using-libraries-and-online-databases",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter3/chapter3.html#using-libraries-and-online-databases",
    "title": "Ch3 Researching and Writing the Literature Review",
    "section": "Using Libraries and Online Databases",
    "text": "Using Libraries and Online Databases\n\nAccess online databases through a library’s website\nSchools often have librarians for specific fields to help navigate databases in a specific area of inquiry."
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter3/chapter3.html#using-search-engines",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter3/chapter3.html#using-search-engines",
    "title": "Ch3 Researching and Writing the Literature Review",
    "section": "Using Search Engines",
    "text": "Using Search Engines\n\nMost databases have similar search engines that are easy to navigate\nUse “advanced search” rather than quick searches\nBoolean operators: using AND, OR, NOT to combine keywords in searching"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter3/chapter3.html#using-interlibrary-loan",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter3/chapter3.html#using-interlibrary-loan",
    "title": "Ch3 Researching and Writing the Literature Review",
    "section": "Using Interlibrary Loan",
    "text": "Using Interlibrary Loan\n\nInterlibrary loan: the ability to access inventory of books and articles available in a library other than your own.\nWKU Interlibrary Loan\n\nhttps://library.wku.edu.cn/en/interlibrary-loan-request-form"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter3/chapter3.html#writing-annotated-bibliographies",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter3/chapter3.html#writing-annotated-bibliographies",
    "title": "Ch3 Researching and Writing the Literature Review",
    "section": "Writing Annotated Bibliographies",
    "text": "Writing Annotated Bibliographies\n\nAnnotated bibliography: a brief summary of the article or book read, including explanations and comments on the citations."
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter3/chapter3.html#creating-a-guiding-table",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter3/chapter3.html#creating-a-guiding-table",
    "title": "Ch3 Researching and Writing the Literature Review",
    "section": "Creating a Guiding Table",
    "text": "Creating a Guiding Table\n\nCreating a table helps to understand the studies better and grasp what information needs to go into the literature review.\nAssigning each study an ID number can help organize them and can be the first column of the table.\nOther columns include author names, publication and study dates, locations, theories used, information on the participants, and so on."
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter3/chapter3.html#using-the-conceptual-graph",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter3/chapter3.html#using-the-conceptual-graph",
    "title": "Ch3 Researching and Writing the Literature Review",
    "section": "Using the Conceptual Graph",
    "text": "Using the Conceptual Graph\n\nReviewing the literature can cause adjustments to be made on the conceptual graph of constructs and variables."
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter3/chapter3.html#organizing-your-work",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter3/chapter3.html#organizing-your-work",
    "title": "Ch3 Researching and Writing the Literature Review",
    "section": "Organizing Your Work",
    "text": "Organizing Your Work\n\nResearchers have their own styles of organizing literature.\nNo matter the style, one should organize the literature logically"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter3/chapter3.html#conceptualizing-the-literature-patterns",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter3/chapter3.html#conceptualizing-the-literature-patterns",
    "title": "Ch3 Researching and Writing the Literature Review",
    "section": "Conceptualizing the Literature: Patterns",
    "text": "Conceptualizing the Literature: Patterns\n\nFinding patterns in the literature is a simple and effective way to organize a literature review"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter3/chapter3.html#reading-critically",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter3/chapter3.html#reading-critically",
    "title": "Ch3 Researching and Writing the Literature Review",
    "section": "Reading Critically",
    "text": "Reading Critically\n\nActive reading, where one reads the information on the page while simultaneously considering the information given and coming up with additional ideas, leads to critical thinking."
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter3/chapter3.html#analyzing-studies",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter3/chapter3.html#analyzing-studies",
    "title": "Ch3 Researching and Writing the Literature Review",
    "section": "Analyzing Studies",
    "text": "Analyzing Studies\n\nFirst look at the concepts, theories, and perspectives the researchers are using; ask questions regarding their intentions and if they considered other ideas.\nNext look closely at the methodologies employed"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter3/chapter3.html#important-questions-to-consider-in-each-step-of-the-reviewsnyder2019literature.",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter3/chapter3.html#important-questions-to-consider-in-each-step-of-the-reviewsnyder2019literature.",
    "title": "Ch3 Researching and Writing the Literature Review",
    "section": "Important questions to consider in each step of the review(Snyder, 2019).",
    "text": "Important questions to consider in each step of the review(Snyder, 2019).\n\nPhase 1: design\n\nIs this review needed and what is the contribution of conducting this review?\nWhat is the potential audience of this review?\nWhat is the speci c purpose and research question(s) this review will be addressing?\nWhat is an appropriate method to use of this review’s specific purpose?\nWhat is the search strategy for this speci c review? (including search terms, databases, inclusion and exclusion criteria etc.)"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter3/chapter3.html#systematic-reviews-versus-narrative-literature-reviews",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter3/chapter3.html#systematic-reviews-versus-narrative-literature-reviews",
    "title": "Ch3 Researching and Writing the Literature Review",
    "section": "Systematic Reviews Versus Narrative Literature Reviews",
    "text": "Systematic Reviews Versus Narrative Literature Reviews\n\nThere are four major factors that make a systematic review different from a literature review:\n\nthe focus is different;\nselection bias is reduced;\nrigorous steps and procedures are followed, and\nassessment and analyses of studies are involved.\n\nSelection bias: the researcher’s tendency to look closely at scientific work that aligns with their ideas and overlooks work that may oppose them.\nMethodology: how data are collected\nMeta-analyses: statistically recalculate the data from original studies based on standard criteria."
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter3/chapter3.html#types-of-reviewgrant2009typology",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter3/chapter3.html#types-of-reviewgrant2009typology",
    "title": "Ch3 Researching and Writing the Literature Review",
    "section": "14 Types of ReviewGrant & Booth (2009)",
    "text": "14 Types of ReviewGrant & Booth (2009)\n\n\n\nCritical review\nLiterature review\nMapping review/systematic map\nMeta-analysis\nMixed studies review/mixed methods review\nOverview\nQualitative systematic review / Qualitative evidence synthesis\n\n\n\nRapid review\nScoping review\nState-of-the-art review\nSystematic review\nSystematic search and review\nSystematized review\nUmbrella review"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter3/chapter3.html#discussion",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter3/chapter3.html#discussion",
    "title": "Ch3 Researching and Writing the Literature Review",
    "section": "Discussion",
    "text": "Discussion\n\nWhat type of information is commonly included when writing an annotated bibliography? (Name all that apply.)\nHow is the systematic review different from a literature review?\nWhat distinguishes meta-analyses from all other types of reviews?\nWhat is the role of hypotheses on our literature review?\nHow can we start our literature review?\nWhat is selection bias and what types of reviews are more prone to include selection bias?"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter3/chapter3.html#summary",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter3/chapter3.html#summary",
    "title": "Ch3 Researching and Writing the Literature Review",
    "section": "Summary",
    "text": "Summary\n\nWriting a literature review is a rewarding task with added perks, such as\n\n\npainting a picture on what the scientific literature in your topic really looks like;\n\n\npulling out studies that are relevant to your topic and showing your organizational skills in making sense of them;\n\n\ndisplaying your creativity in designing the literature review that best supports and complements your study;\n\n\nexercising critical thinking by looking at the studies from various perspectives; and\n\n\nwriting a logical, critical, and analytical literature review that supports your proposed study and naturally ends with your research question(s) or hypotheses.\n\n\n\n\n\nWriting a literature review is a rewarding task with added perks, such as\n\n\npainting a picture on what the scientific literature in your topic really looks like;\n\n\npulling out studies that are relevant to your topic and showing your organizational skills in making sense of them;\n\n\ndisplaying your creativity in designing the literature review that best supports and complements your study;\n\n\nexercising critical thinking by looking at the studies from various perspectives; and\n\n\nwriting a logical, critical, and analytical literature review that supports your proposed study and naturally ends with your research question(s) or hypotheses.\n\n\nIn this chapter, you were introduced to some helpful tips on how to organize the literature and, most importantly, how to initially conceptualize the tasks. This organization is unique to each researcher, but some helpful tips were provided in how to look for patterns, similarities in methodologies or findings, as well as distinctive differences or opposing results from studies.\nYour literature review will culminate with the hypotheses or research questions you have formulated. For every alternative hypothesis, we include a null hypothesis because the null hypothesis is the one we will eventually try to reject. This chapter also introduced systematic reviews of literature and how they are different from the traditional literature reviews.\nA systematic review of literature tends to be protected from selection biases—biases related to how we choose studies to include in our review. A systematic review also follows rigorous steps and procedures compared to the traditional review. After all, a systematic review of literature is best conceptualized as a research design rather than a literature review.\nIt usually has its own traditional literature review. Systematic reviews are also more organized and full of details on the studies they have investigated. This allows researchers to paint a complete picture on the literature of a specific topic. Meta-analyses are similar to systematic reviews with an added feature of statistically recalculating the data from original studies."
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter3/chapter3.html#terms",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter3/chapter3.html#terms",
    "title": "Ch3 Researching and Writing the Literature Review",
    "section": "Terms",
    "text": "Terms\n\nAnnotated bibliography: a brief summary of the article or book you read, including the focus of the study, the methodology used, the findings, and any other important information that directly relates to your research topic.\nBoolean operators: operators that are used to conduct searches in the library and other databases (i.e., AND, OR, NOT).\nInterlibrary loan: the possibility of borrowing articles and books from other libraries that may not be available in your local or university library.\nLiterature review: the body of literature surrounding a specific topic of interest to the researcher."
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter3/chapter3.html#more-to-read",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter3/chapter3.html#more-to-read",
    "title": "Ch3 Researching and Writing the Literature Review",
    "section": "More to Read",
    "text": "More to Read\n\nGrant, M. J., & Booth, A. (2009). A typology of reviews: An analysis of 14 review types and associated methodologies. Health Information & Libraries Journal, 26(2), 91–108.\nSnyder, H. (2019). Literature review as a research methodology: An overview and guidelines. Journal of Business Research, 104, 333–339."
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter3/chapter3.html#assignment",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter3/chapter3.html#assignment",
    "title": "Ch3 Researching and Writing the Literature Review",
    "section": "Assignment",
    "text": "Assignment\n\n\n\nWhat to do\n\nYou need to submit “Literature Synthesis Report” that contains\n\nMotivation,\nimportance,\nsynthesis,\nfindings,\nconceptual graph,\nsummary table, and\nreference\n\n\n\n\n\nRequirement\n\nPDF format\nfile name should be include your student id and name\n\nstuID_name_title.pdf (e.g. 1111111_ChungilChae_SelfIntroduction.pdf)\n\n\nDue date\n\nby DATE(Sun) 11:59PM\nNO LATE SUBMISSION ALLOWED!!!!"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter5/chapter5.html#short-video-introduction",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter5/chapter5.html#short-video-introduction",
    "title": "TITLE",
    "section": "SHORT VIDEO INTRODUCTION",
    "text": "SHORT VIDEO INTRODUCTION"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter5/chapter5.html#lv1_1",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter5/chapter5.html#lv1_1",
    "title": "TITLE",
    "section": "Lv1_1",
    "text": "Lv1_1\n\nThis is level1 contents\nThis is level1 contents\nThis is level1 contents\nThis is level1 contents\nThis is level1 contents"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter5/chapter5.html#lv2_2",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter5/chapter5.html#lv2_2",
    "title": "TITLE",
    "section": "Lv2_2",
    "text": "Lv2_2"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter5/chapter5.html#discussion",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter5/chapter5.html#discussion",
    "title": "TITLE",
    "section": "Discussion",
    "text": "Discussion"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter5/chapter5.html#summary",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter5/chapter5.html#summary",
    "title": "TITLE",
    "section": "Summary",
    "text": "Summary"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter5/chapter5.html#terms",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter5/chapter5.html#terms",
    "title": "TITLE",
    "section": "Terms",
    "text": "Terms\n:::"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter5/chapter5.html#more-to-read",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter5/chapter5.html#more-to-read",
    "title": "TITLE",
    "section": "More to Read",
    "text": "More to Read"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter5/chapter5.html#assignment",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter5/chapter5.html#assignment",
    "title": "TITLE",
    "section": "Assignment",
    "text": "Assignment\n\n\n\nWhat to do\n\n\n\nRequirement\n\nPDF format\nfile name should be include your student id and name\n\nstuID_name_title.pdf (e.g. 1111111_ChungilChae_SelfIntroduction.pdf)\n\n\nDue date\n\nby Feb23(Sun) 11:59PM\nNO LATE SUBMISSION ALLOWED!!!!"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter10/chapter10.html#short-video-introduction",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter10/chapter10.html#short-video-introduction",
    "title": "TITLE",
    "section": "SHORT VIDEO INTRODUCTION",
    "text": "SHORT VIDEO INTRODUCTION"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter10/chapter10.html#lv1_1",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter10/chapter10.html#lv1_1",
    "title": "TITLE",
    "section": "Lv1_1",
    "text": "Lv1_1\n\nThis is level1 contents\nThis is level1 contents\nThis is level1 contents\nThis is level1 contents\nThis is level1 contents"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter10/chapter10.html#lv2_2",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter10/chapter10.html#lv2_2",
    "title": "TITLE",
    "section": "Lv2_2",
    "text": "Lv2_2"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter10/chapter10.html#discussion",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter10/chapter10.html#discussion",
    "title": "TITLE",
    "section": "Discussion",
    "text": "Discussion"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter10/chapter10.html#summary",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter10/chapter10.html#summary",
    "title": "TITLE",
    "section": "Summary",
    "text": "Summary"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter10/chapter10.html#terms",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter10/chapter10.html#terms",
    "title": "TITLE",
    "section": "Terms",
    "text": "Terms\n:::"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter10/chapter10.html#more-to-read",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter10/chapter10.html#more-to-read",
    "title": "TITLE",
    "section": "More to Read",
    "text": "More to Read"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter10/chapter10.html#assignment",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter10/chapter10.html#assignment",
    "title": "TITLE",
    "section": "Assignment",
    "text": "Assignment\n\n\n\nWhat to do\n\n\n\nRequirement\n\nPDF format\nfile name should be include your student id and name\n\nstuID_name_title.pdf (e.g. 1111111_ChungilChae_SelfIntroduction.pdf)\n\n\nDue date\n\nby Feb23(Sun) 11:59PM\nNO LATE SUBMISSION ALLOWED!!!!"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#short-video-introduction",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#short-video-introduction",
    "title": "Ch3 Researching and Writing the Literature Review",
    "section": "SHORT VIDEO INTRODUCTION",
    "text": "SHORT VIDEO INTRODUCTION"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#professor",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#professor",
    "title": "Ch3 Researching and Writing the Literature Review",
    "section": "Professor",
    "text": "Professor\nChad (Dr. Chungil Chae)\n\n\n\n\n\nChad (Chungil Chae)\nCBPM B223 | cchae@kean.edu\nAssistant Professor at CBPM, WKU since 2020 Fall\nCall ma Chad, but in formal situation and space, Dr.Chae or Prof.Chae\nTeaching business analytics major courses\n\nMGS 3001: Python for Business\nMGS 3101: Foundation of Business Analytics\nMGS 3701: Data Mining\nMGS 4701: Application of Business Analytics"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#teaching-assistant",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#teaching-assistant",
    "title": "Ch3 Researching and Writing the Literature Review",
    "section": "Teaching Assistant",
    "text": "Teaching Assistant\nXuwu Zhao\n\n\n\n\n\nJack (Xuwu Zhao)\n1308347@wku.edu.cn\nSophomore major in Finance\nPlease feel free to call me Jack. I am truly honored to have the opportunity to serve as your teaching assistant for the GE2021 Research and Technology course this semester.\nSpecification\n\nawarded the Dean’s Scholarship-First Class\nZhejiang Provincial Scholarship for the 2023-2024 academic year\ninterests extend to Management Science and Accounting"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#chapter-1-the-purpose-of-research",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#chapter-1-the-purpose-of-research",
    "title": "Ch3 Researching and Writing the Literature Review",
    "section": "Chapter 1: The Purpose of Research",
    "text": "Chapter 1: The Purpose of Research\n\nDescribe the purpose of scientific research\nDescribe two theories of knowledge: falsifiability and the scientific revolution\nCompare and contrast qualitative, quantitative, and mixed methods\nExplain the importance of ethics and objectivity in research"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#chapter-2-formulating-a-research-question",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#chapter-2-formulating-a-research-question",
    "title": "Ch3 Researching and Writing the Literature Review",
    "section": "Chapter 2: Formulating a Research Question",
    "text": "Chapter 2: Formulating a Research Question\n\nChoose a research topic.\nExplain how to operationalize research constructs\nDescribe the different types of variables\nFormulate the various types of hypotheses\nCreate a visualization of a research question"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#chapter-3-researching-and-writing-the-literature",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#chapter-3-researching-and-writing-the-literature",
    "title": "Ch3 Researching and Writing the Literature Review",
    "section": "Chapter 3: Researching and Writing the Literature",
    "text": "Chapter 3: Researching and Writing the Literature\n\nDescribe the purpose of a literature review\nLearn about online databases as resources\nExamine different ways of organizing a literature review\nDiscuss how to think critically and analyze studies\nIdentify the correct placement of study hypotheses\nCompare and contrast a systematic review of the literature and a literature review"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#chapter-4-quantitative-designs",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#chapter-4-quantitative-designs",
    "title": "Ch3 Researching and Writing the Literature Review",
    "section": "Chapter 4: Quantitative Designs",
    "text": "Chapter 4: Quantitative Designs\n\nDescribe the purpose of exploratory, descriptive, and explanatory studies.\nCompare and contrast cross-sectional and longitudinal studies.\nExplain the differences between nomothetic research and idiographic research.\nDiscuss each type of experimental design and its advantages and disadvantages."
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#chapter-5-measurement-errors-reliability-validity",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#chapter-5-measurement-errors-reliability-validity",
    "title": "Ch3 Researching and Writing the Literature Review",
    "section": "Chapter 5: Measurement Errors, Reliability, Validity",
    "text": "Chapter 5: Measurement Errors, Reliability, Validity\n\nRecognize measurement errors and describe how to categorize them.\nCompare and contrast the interrater reliability, test-retest reliability, and internal consistency reliability.\nAnalyze the different types of validity: face validity, content validity, construct validity, criterion validity, concurrent validity, and predictive validity."
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#chapter-6-sampling",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#chapter-6-sampling",
    "title": "Ch3 Researching and Writing the Literature Review",
    "section": "Chapter 6: Sampling",
    "text": "Chapter 6: Sampling\n\nExplain the purpose of sampling.\nCompare and contrast probability and nonprobability sampling.\nDescribe the types of nonprobability sampling.\nSummarize the types of probability sampling.\nUnderstand sampling error, confidence interval, and saturation."
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#chapter-7-data-collection-for-quantitative-research",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#chapter-7-data-collection-for-quantitative-research",
    "title": "Ch3 Researching and Writing the Literature Review",
    "section": "Chapter 7: Data Collection for Quantitative Research",
    "text": "Chapter 7: Data Collection for Quantitative Research\n\nFamiliarize yourself with the details of experimental design.\nFamiliarize yourself with the details of quasi-experimental design.\nUnderstand the steps of designing a survey study.\nConsider additional data collection sources."
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#chapter-8-secondary-data",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#chapter-8-secondary-data",
    "title": "Ch3 Researching and Writing the Literature Review",
    "section": "Chapter 8: Secondary Data",
    "text": "Chapter 8: Secondary Data\n\nDescribe the benefits of using secondary data.\nIdentify the major sources of secondary data.\nExplain the drawbacks of using secondary data."
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#chapter-9-entering-and-organizing-quantitative-data",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#chapter-9-entering-and-organizing-quantitative-data",
    "title": "Ch3 Researching and Writing the Literature Review",
    "section": "Chapter 9: Entering and Organizing Quantitative Data",
    "text": "Chapter 9: Entering and Organizing Quantitative Data\n\nExplain the purpose of entering and organizing data logically.\nSummarize the steps needed to prepare to enter data.\nDescribe how to organize and input variables and their information."
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#chapter-10-analyzing-quantitative-data",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#chapter-10-analyzing-quantitative-data",
    "title": "Ch3 Researching and Writing the Literature Review",
    "section": "Chapter 10: Analyzing Quantitative Data",
    "text": "Chapter 10: Analyzing Quantitative Data\n\nSummarize why statistics are used in research methods\nConduct univariate analysis\nExplain the measures of central tendency\nDefine measures of variability and dispersion\nDescribe the various ways to graphically represent data\nConduct bivariate analysis"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#chapter-11-qualitative-designs-and-data-collection",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#chapter-11-qualitative-designs-and-data-collection",
    "title": "Ch3 Researching and Writing the Literature Review",
    "section": "Chapter 11: Qualitative Designs And Data Collection",
    "text": "Chapter 11: Qualitative Designs And Data Collection\n\nPlan a qualitative study in a community that is not well known or understood.\nUnderstand that qualitative research and quantitative research are not opposed.\nDo participant observation, write field notes, and manage them in a database.\nCollect other data, like artifacts, photos, and videos; texts can also be collected."
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#chapter-12-entering-coding-and-analyzing-qualitative-data",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#chapter-12-entering-coding-and-analyzing-qualitative-data",
    "title": "Ch3 Researching and Writing the Literature Review",
    "section": "Chapter 12: Entering, Coding, and Analyzing Qualitative Data",
    "text": "Chapter 12: Entering, Coding, and Analyzing Qualitative Data\n\nDescribe how to enter, clean, and organize qualitative data.\nExamine and code raw data.\nAnalyze qualitative data."
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#chapter-13-results-and-discussion",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#chapter-13-results-and-discussion",
    "title": "Ch3 Researching and Writing the Literature Review",
    "section": "Chapter 13: Results and Discussion",
    "text": "Chapter 13: Results and Discussion\n\nWrite a results section.\nDescribe how to present results in quantitative studies.\nExplain how to present results in qualitative studies.\nPrepare visual representations of results.\nCompose a discussion section.\nDevelop a recommendations section based on methodology or topic."
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#chapter-14-presenting-your-research",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#chapter-14-presenting-your-research",
    "title": "Ch3 Researching and Writing the Literature Review",
    "section": "Chapter 14: Presenting Your Research",
    "text": "Chapter 14: Presenting Your Research\n\nExplain how best to present your study to an audience.\nDiscuss how to apply to regional and national conferences.\nDescribe how to get an article published in a peer-reviewed journal."
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#class-information",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#class-information",
    "title": "Ch3 Researching and Writing the Literature Review",
    "section": "Class Information",
    "text": "Class Information\n\nGE 2021 W09: Research Technology\nClass time: T, TH 1:00 pm - 2:15 pm\nClass room: CBPM C423"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#in-class",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#in-class",
    "title": "Ch3 Researching and Writing the Literature Review",
    "section": "In CLass",
    "text": "In CLass\n\nYou are expected to read chapter and course material before class\nBased on your class participation, you will get extra score\nComputer and other digital device is allowed ONLY students uses it for class related purpose.\nIn case instuctor find unauthorized useage of digital device, you will be asked to leave class."
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#attendence-and-absent",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#attendence-and-absent",
    "title": "Ch3 Researching and Writing the Literature Review",
    "section": "Attendence and Absent",
    "text": "Attendence and Absent\n\nDON”T SENT ME EMAIL or ANY MESSAGE about YOUR ABSENT in ADVANCE\nMore than three times of absents automatically will be marked as F\nAttendence will be managed in student performance application\nWhen instructor or TA check your attendence and if you are not in class, no matter what reason, your attendence will be marked as absent.\nHowever, if you have proper and official evidence that WKU allow for absent, bring it to your instructor for revise your absent mark to attendece."
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#integration",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#integration",
    "title": "Ch3 Researching and Writing the Literature Review",
    "section": "Integration",
    "text": "Integration\n\nPlagiarism is not tolerated\n\nRight after find plagiarism, it will be reported to Office of Vice Chancellor for Academic Affairs directly\nStudent will be kicked out from class immediately\nRead Academic Integrity Policy"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#generative-ai-use-in-ge-2021",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#generative-ai-use-in-ge-2021",
    "title": "Ch3 Researching and Writing the Literature Review",
    "section": "Generative AI Use in GE 2021",
    "text": "Generative AI Use in GE 2021\nStudents in GE 202X are permitted to use AI tools, including, but not limited to, ChatGhT, in this course to generate ideas and brainstorm.\n\nThink of generative AI as an always-available brainstorming partner. However, you should note that the material generated by these programs may be inaccurate, incomplete, or otherwise problematic. Beware that use may also stifle your independent thinking and creativity.\nAcademic work involves developing essential skills such as critical thinking, problem-solving, and effective communication, which cannot be fully developed by relying solely on Artificial Intelligence (AI)."
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter6/chapter6.html#short-video-introduction",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter6/chapter6.html#short-video-introduction",
    "title": "TITLE",
    "section": "SHORT VIDEO INTRODUCTION",
    "text": "SHORT VIDEO INTRODUCTION"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter6/chapter6.html#lv1_1",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter6/chapter6.html#lv1_1",
    "title": "TITLE",
    "section": "Lv1_1",
    "text": "Lv1_1\n\nThis is level1 contents\nThis is level1 contents\nThis is level1 contents\nThis is level1 contents\nThis is level1 contents"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter6/chapter6.html#lv2_2",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter6/chapter6.html#lv2_2",
    "title": "TITLE",
    "section": "Lv2_2",
    "text": "Lv2_2"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter6/chapter6.html#discussion",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter6/chapter6.html#discussion",
    "title": "TITLE",
    "section": "Discussion",
    "text": "Discussion"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter6/chapter6.html#summary",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter6/chapter6.html#summary",
    "title": "TITLE",
    "section": "Summary",
    "text": "Summary"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter6/chapter6.html#terms",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter6/chapter6.html#terms",
    "title": "TITLE",
    "section": "Terms",
    "text": "Terms\n:::"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter6/chapter6.html#more-to-read",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter6/chapter6.html#more-to-read",
    "title": "TITLE",
    "section": "More to Read",
    "text": "More to Read"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter6/chapter6.html#assignment",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter6/chapter6.html#assignment",
    "title": "TITLE",
    "section": "Assignment",
    "text": "Assignment\n\n\n\nWhat to do\n\n\n\nRequirement\n\nPDF format\nfile name should be include your student id and name\n\nstuID_name_title.pdf (e.g. 1111111_ChungilChae_SelfIntroduction.pdf)\n\n\nDue date\n\nby Feb23(Sun) 11:59PM\nNO LATE SUBMISSION ALLOWED!!!!"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter8/chapter8.html#short-video-introduction",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter8/chapter8.html#short-video-introduction",
    "title": "TITLE",
    "section": "SHORT VIDEO INTRODUCTION",
    "text": "SHORT VIDEO INTRODUCTION"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter8/chapter8.html#lv1_1",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter8/chapter8.html#lv1_1",
    "title": "TITLE",
    "section": "Lv1_1",
    "text": "Lv1_1\n\nThis is level1 contents\nThis is level1 contents\nThis is level1 contents\nThis is level1 contents\nThis is level1 contents"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter8/chapter8.html#lv2_2",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter8/chapter8.html#lv2_2",
    "title": "TITLE",
    "section": "Lv2_2",
    "text": "Lv2_2"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter8/chapter8.html#discussion",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter8/chapter8.html#discussion",
    "title": "TITLE",
    "section": "Discussion",
    "text": "Discussion"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter8/chapter8.html#summary",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter8/chapter8.html#summary",
    "title": "TITLE",
    "section": "Summary",
    "text": "Summary"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter8/chapter8.html#terms",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter8/chapter8.html#terms",
    "title": "TITLE",
    "section": "Terms",
    "text": "Terms\n:::"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter8/chapter8.html#more-to-read",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter8/chapter8.html#more-to-read",
    "title": "TITLE",
    "section": "More to Read",
    "text": "More to Read"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter8/chapter8.html#assignment",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter8/chapter8.html#assignment",
    "title": "TITLE",
    "section": "Assignment",
    "text": "Assignment\n\n\n\nWhat to do\n\n\n\nRequirement\n\nPDF format\nfile name should be include your student id and name\n\nstuID_name_title.pdf (e.g. 1111111_ChungilChae_SelfIntroduction.pdf)\n\n\nDue date\n\nby Feb23(Sun) 11:59PM\nNO LATE SUBMISSION ALLOWED!!!!"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 04 - Dimension reduction.html",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 04 - Dimension reduction.html",
    "title": "Chapter 4: Dimension Reduction",
    "section": "",
    "text": "2019 Galit Shmueli, Peter C. Bruce, Peter Gedeck\n\nCode included in\nData Mining for Business Analytics: Concepts, Techniques, and Applications in Python (First Edition) Galit Shmueli, Peter C. Bruce, Peter Gedeck, and Nitin R. Patel. 2019."
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 04 - Dimension reduction.html#import-required-packages",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 04 - Dimension reduction.html#import-required-packages",
    "title": "Chapter 4: Dimension Reduction",
    "section": "Import required packages",
    "text": "Import required packages\n\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nfrom sklearn.decomposition import PCA\nfrom sklearn import preprocessing\nimport matplotlib.pylab as plt\n\nimport dmba\n\n%matplotlib inline\n\nno display found. Using non-interactive Agg backend"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 04 - Dimension reduction.html#table-4.3",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 04 - Dimension reduction.html#table-4.3",
    "title": "Chapter 4: Dimension Reduction",
    "section": "Table 4.3",
    "text": "Table 4.3\n\nbostonHousing_df = dmba.load_data('BostonHousing.csv')\nbostonHousing_df = bostonHousing_df.rename(columns={'CAT. MEDV': 'CAT_MEDV'})\nbostonHousing_df.head(9)\n\n\n\n\n\n\n\n\nCRIM\nZN\nINDUS\nCHAS\nNOX\nRM\nAGE\nDIS\nRAD\nTAX\nPTRATIO\nLSTAT\nMEDV\nCAT_MEDV\n\n\n\n\n0\n0.00632\n18.0\n2.31\n0\n0.538\n6.575\n65.2\n4.0900\n1\n296\n15.3\n4.98\n24.0\n0\n\n\n1\n0.02731\n0.0\n7.07\n0\n0.469\n6.421\n78.9\n4.9671\n2\n242\n17.8\n9.14\n21.6\n0\n\n\n2\n0.02729\n0.0\n7.07\n0\n0.469\n7.185\n61.1\n4.9671\n2\n242\n17.8\n4.03\n34.7\n1\n\n\n3\n0.03237\n0.0\n2.18\n0\n0.458\n6.998\n45.8\n6.0622\n3\n222\n18.7\n2.94\n33.4\n1\n\n\n4\n0.06905\n0.0\n2.18\n0\n0.458\n7.147\n54.2\n6.0622\n3\n222\n18.7\n5.33\n36.2\n1\n\n\n5\n0.02985\n0.0\n2.18\n0\n0.458\n6.430\n58.7\n6.0622\n3\n222\n18.7\n5.21\n28.7\n0\n\n\n6\n0.08829\n12.5\n7.87\n0\n0.524\n6.012\n66.6\n5.5605\n5\n311\n15.2\n12.43\n22.9\n0\n\n\n7\n0.14455\n12.5\n7.87\n0\n0.524\n6.172\n96.1\n5.9505\n5\n311\n15.2\n19.15\n27.1\n0\n\n\n8\n0.21124\n12.5\n7.87\n0\n0.524\n5.631\n100.0\n6.0821\n5\n311\n15.2\n29.93\n16.5\n0\n\n\n\n\n\n\n\n\nbostonHousing_df.describe()\n\n\n\n\n\n\n\n\nCRIM\nZN\nINDUS\nCHAS\nNOX\nRM\nAGE\nDIS\nRAD\nTAX\nPTRATIO\nLSTAT\nMEDV\nCAT_MEDV\n\n\n\n\ncount\n506.000000\n506.000000\n506.000000\n506.000000\n506.000000\n506.000000\n506.000000\n506.000000\n506.000000\n506.000000\n506.000000\n506.000000\n506.000000\n506.000000\n\n\nmean\n3.613524\n11.363636\n11.136779\n0.069170\n0.554695\n6.284634\n68.574901\n3.795043\n9.549407\n408.237154\n18.455534\n12.653063\n22.532806\n0.166008\n\n\nstd\n8.601545\n23.322453\n6.860353\n0.253994\n0.115878\n0.702617\n28.148861\n2.105710\n8.707259\n168.537116\n2.164946\n7.141062\n9.197104\n0.372456\n\n\nmin\n0.006320\n0.000000\n0.460000\n0.000000\n0.385000\n3.561000\n2.900000\n1.129600\n1.000000\n187.000000\n12.600000\n1.730000\n5.000000\n0.000000\n\n\n25%\n0.082045\n0.000000\n5.190000\n0.000000\n0.449000\n5.885500\n45.025000\n2.100175\n4.000000\n279.000000\n17.400000\n6.950000\n17.025000\n0.000000\n\n\n50%\n0.256510\n0.000000\n9.690000\n0.000000\n0.538000\n6.208500\n77.500000\n3.207450\n5.000000\n330.000000\n19.050000\n11.360000\n21.200000\n0.000000\n\n\n75%\n3.677083\n12.500000\n18.100000\n0.000000\n0.624000\n6.623500\n94.075000\n5.188425\n24.000000\n666.000000\n20.200000\n16.955000\n25.000000\n0.000000\n\n\nmax\n88.976200\n100.000000\n27.740000\n1.000000\n0.871000\n8.780000\n100.000000\n12.126500\n24.000000\n711.000000\n22.000000\n37.970000\n50.000000\n1.000000\n\n\n\n\n\n\n\nCompute mean, standard deviation, min, max, median, length, and missing values of CRIM\n\nprint('Mean : ', bostonHousing_df.CRIM.mean())\nprint('Std. dev : ', bostonHousing_df.CRIM.std())\nprint('Min : ', bostonHousing_df.CRIM.min())\nprint('Max : ', bostonHousing_df.CRIM.max())\nprint('Median : ', bostonHousing_df.CRIM.median())\nprint('Length : ', len(bostonHousing_df.CRIM))\n\nprint('Number of missing values : ', bostonHousing_df.CRIM.isnull().sum())\n\nMean :  3.613523557312254\nStd. dev :  8.60154510533249\nMin :  0.00632\nMax :  88.9762\nMedian :  0.25651\nLength :  506\nNumber of missing values :  0\n\n\nCompute mean, standard dev., min, max, median, length, and missing values for all variables\n\npd.DataFrame({'mean': bostonHousing_df.mean(),\n              'sd': bostonHousing_df.std(),\n              'min': bostonHousing_df.min(),\n              'max': bostonHousing_df.max(),\n              'median': bostonHousing_df.median(),\n              'length': len(bostonHousing_df),\n              'miss.val': bostonHousing_df.isnull().sum(),\n             })\n\n\n\n\n\n\n\n\nmean\nsd\nmin\nmax\nmedian\nlength\nmiss.val\n\n\n\n\nCRIM\n3.613524\n8.601545\n0.00632\n88.9762\n0.25651\n506\n0\n\n\nZN\n11.363636\n23.322453\n0.00000\n100.0000\n0.00000\n506\n0\n\n\nINDUS\n11.136779\n6.860353\n0.46000\n27.7400\n9.69000\n506\n0\n\n\nCHAS\n0.069170\n0.253994\n0.00000\n1.0000\n0.00000\n506\n0\n\n\nNOX\n0.554695\n0.115878\n0.38500\n0.8710\n0.53800\n506\n0\n\n\nRM\n6.284634\n0.702617\n3.56100\n8.7800\n6.20850\n506\n0\n\n\nAGE\n68.574901\n28.148861\n2.90000\n100.0000\n77.50000\n506\n0\n\n\nDIS\n3.795043\n2.105710\n1.12960\n12.1265\n3.20745\n506\n0\n\n\nRAD\n9.549407\n8.707259\n1.00000\n24.0000\n5.00000\n506\n0\n\n\nTAX\n408.237154\n168.537116\n187.00000\n711.0000\n330.00000\n506\n0\n\n\nPTRATIO\n18.455534\n2.164946\n12.60000\n22.0000\n19.05000\n506\n0\n\n\nLSTAT\n12.653063\n7.141062\n1.73000\n37.9700\n11.36000\n506\n0\n\n\nMEDV\n22.532806\n9.197104\n5.00000\n50.0000\n21.20000\n506\n0\n\n\nCAT_MEDV\n0.166008\n0.372456\n0.00000\n1.0000\n0.00000\n506\n0"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 04 - Dimension reduction.html#table-4.4",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 04 - Dimension reduction.html#table-4.4",
    "title": "Chapter 4: Dimension Reduction",
    "section": "Table 4.4",
    "text": "Table 4.4\n\nbostonHousing_df.corr().round(2)\n\n\n\n\n\n\n\n\nCRIM\nZN\nINDUS\nCHAS\nNOX\nRM\nAGE\nDIS\nRAD\nTAX\nPTRATIO\nLSTAT\nMEDV\nCAT_MEDV\n\n\n\n\nCRIM\n1.00\n-0.20\n0.41\n-0.06\n0.42\n-0.22\n0.35\n-0.38\n0.63\n0.58\n0.29\n0.46\n-0.39\n-0.15\n\n\nZN\n-0.20\n1.00\n-0.53\n-0.04\n-0.52\n0.31\n-0.57\n0.66\n-0.31\n-0.31\n-0.39\n-0.41\n0.36\n0.37\n\n\nINDUS\n0.41\n-0.53\n1.00\n0.06\n0.76\n-0.39\n0.64\n-0.71\n0.60\n0.72\n0.38\n0.60\n-0.48\n-0.37\n\n\nCHAS\n-0.06\n-0.04\n0.06\n1.00\n0.09\n0.09\n0.09\n-0.10\n-0.01\n-0.04\n-0.12\n-0.05\n0.18\n0.11\n\n\nNOX\n0.42\n-0.52\n0.76\n0.09\n1.00\n-0.30\n0.73\n-0.77\n0.61\n0.67\n0.19\n0.59\n-0.43\n-0.23\n\n\nRM\n-0.22\n0.31\n-0.39\n0.09\n-0.30\n1.00\n-0.24\n0.21\n-0.21\n-0.29\n-0.36\n-0.61\n0.70\n0.64\n\n\nAGE\n0.35\n-0.57\n0.64\n0.09\n0.73\n-0.24\n1.00\n-0.75\n0.46\n0.51\n0.26\n0.60\n-0.38\n-0.19\n\n\nDIS\n-0.38\n0.66\n-0.71\n-0.10\n-0.77\n0.21\n-0.75\n1.00\n-0.49\n-0.53\n-0.23\n-0.50\n0.25\n0.12\n\n\nRAD\n0.63\n-0.31\n0.60\n-0.01\n0.61\n-0.21\n0.46\n-0.49\n1.00\n0.91\n0.46\n0.49\n-0.38\n-0.20\n\n\nTAX\n0.58\n-0.31\n0.72\n-0.04\n0.67\n-0.29\n0.51\n-0.53\n0.91\n1.00\n0.46\n0.54\n-0.47\n-0.27\n\n\nPTRATIO\n0.29\n-0.39\n0.38\n-0.12\n0.19\n-0.36\n0.26\n-0.23\n0.46\n0.46\n1.00\n0.37\n-0.51\n-0.44\n\n\nLSTAT\n0.46\n-0.41\n0.60\n-0.05\n0.59\n-0.61\n0.60\n-0.50\n0.49\n0.54\n0.37\n1.00\n-0.74\n-0.47\n\n\nMEDV\n-0.39\n0.36\n-0.48\n0.18\n-0.43\n0.70\n-0.38\n0.25\n-0.38\n-0.47\n-0.51\n-0.74\n1.00\n0.79\n\n\nCAT_MEDV\n-0.15\n0.37\n-0.37\n0.11\n-0.23\n0.64\n-0.19\n0.12\n-0.20\n-0.27\n-0.44\n-0.47\n0.79\n1.00"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 04 - Dimension reduction.html#table-4.5",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 04 - Dimension reduction.html#table-4.5",
    "title": "Chapter 4: Dimension Reduction",
    "section": "Table 4.5",
    "text": "Table 4.5\n\nbostonHousing_df.CHAS.value_counts()\n\n0    471\n1     35\nName: CHAS, dtype: int64"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 04 - Dimension reduction.html#table-4.6",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 04 - Dimension reduction.html#table-4.6",
    "title": "Chapter 4: Dimension Reduction",
    "section": "Table 4.6",
    "text": "Table 4.6\nCreate bins of size 1 for variable using the method pd.cut. By default, the method creates a categorical variable, e.g. (6,7]. The argument labels=False determines integers instead, e.g. 6.\n\nbostonHousing_df['RM_bin'] = pd.cut(bostonHousing_df.RM, range(0, 10), labels=False)\nbostonHousing_df.head(5)\n\n\n\n\n\n\n\n\nCRIM\nZN\nINDUS\nCHAS\nNOX\nRM\nAGE\nDIS\nRAD\nTAX\nPTRATIO\nLSTAT\nMEDV\nCAT_MEDV\nRM_bin\n\n\n\n\n0\n0.00632\n18.0\n2.31\n0\n0.538\n6.575\n65.2\n4.0900\n1\n296\n15.3\n4.98\n24.0\n0\n6\n\n\n1\n0.02731\n0.0\n7.07\n0\n0.469\n6.421\n78.9\n4.9671\n2\n242\n17.8\n9.14\n21.6\n0\n6\n\n\n2\n0.02729\n0.0\n7.07\n0\n0.469\n7.185\n61.1\n4.9671\n2\n242\n17.8\n4.03\n34.7\n1\n7\n\n\n3\n0.03237\n0.0\n2.18\n0\n0.458\n6.998\n45.8\n6.0622\n3\n222\n18.7\n2.94\n33.4\n1\n6\n\n\n4\n0.06905\n0.0\n2.18\n0\n0.458\n7.147\n54.2\n6.0622\n3\n222\n18.7\n5.33\n36.2\n1\n7\n\n\n\n\n\n\n\nCompute the average of MEDV by (binned) RM and CHAS. First group the data frame using the groupby method, then restrict the analysis to MEDV and determine the mean for each group.\n\nbostonHousing_df.groupby(['RM_bin', 'CHAS'])['MEDV'].mean()\n\nRM_bin  CHAS\n3       0       25.300000\n4       0       15.407143\n5       0       17.200000\n        1       22.218182\n6       0       21.769170\n        1       25.918750\n7       0       35.964444\n        1       44.066667\n8       0       45.700000\n        1       35.950000\nName: MEDV, dtype: float64"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 04 - Dimension reduction.html#table-4.7",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 04 - Dimension reduction.html#table-4.7",
    "title": "Chapter 4: Dimension Reduction",
    "section": "Table 4.7",
    "text": "Table 4.7\n\nbostonHousing_df = dmba.load_data('BostonHousing.csv')\nbostonHousing_df = bostonHousing_df.rename(columns={'CAT. MEDV': 'CAT_MEDV'})\nbostonHousing_df['RM_bin'] = pd.cut(bostonHousing_df.RM, range(0, 10), labels=False)\n\n\nmlt = pd.melt(bostonHousing_df, id_vars=['RM_bin', 'CHAS'], \n              value_vars=['MEDV'])\nmlt.head()\n\n\n\n\n\n\n\n\nRM_bin\nCHAS\nvariable\nvalue\n\n\n\n\n0\n6\n0\nMEDV\n24.0\n\n\n1\n6\n0\nMEDV\n21.6\n\n\n2\n7\n0\nMEDV\n34.7\n\n\n3\n6\n0\nMEDV\n33.4\n\n\n4\n7\n0\nMEDV\n36.2\n\n\n\n\n\n\n\n\npd.pivot_table(mlt, values='value', index=['RM_bin'], columns=['CHAS'],\n               aggfunc=np.mean, margins=True)\n\n\n\n\n\n\n\nCHAS\n0\n1\nAll\n\n\nRM_bin\n\n\n\n\n\n\n\n3\n25.300000\nNaN\n25.300000\n\n\n4\n15.407143\nNaN\n15.407143\n\n\n5\n17.200000\n22.218182\n17.551592\n\n\n6\n21.769170\n25.918750\n22.015985\n\n\n7\n35.964444\n44.066667\n36.917647\n\n\n8\n45.700000\n35.950000\n44.200000\n\n\nAll\n22.093843\n28.440000\n22.532806\n\n\n\n\n\n\n\n\npd.pivot_table(bostonHousing_df, values='MEDV', index=['RM_bin'], columns=['CHAS'], aggfunc=np.mean, margins=True)\n\n\n\n\n\n\n\nCHAS\n0\n1\nAll\n\n\nRM_bin\n\n\n\n\n\n\n\n3\n25.300000\nNaN\n25.300000\n\n\n4\n15.407143\nNaN\n15.407143\n\n\n5\n17.200000\n22.218182\n17.551592\n\n\n6\n21.769170\n25.918750\n22.015985\n\n\n7\n35.964444\n44.066667\n36.917647\n\n\n8\n45.700000\n35.950000\n44.200000\n\n\nAll\n22.093843\n28.440000\n22.532806"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 04 - Dimension reduction.html#figure-4.1",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 04 - Dimension reduction.html#figure-4.1",
    "title": "Chapter 4: Dimension Reduction",
    "section": "Figure 4.1",
    "text": "Figure 4.1\nUse the method pd.crosstab for cross tabulation of two variables. In a second step, we convert the counts into percentages along the columns.\n\ntbl = pd.crosstab(bostonHousing_df.CAT_MEDV, bostonHousing_df.ZN)\npropTbl = tbl / tbl.sum()\npropTbl.round(2)\n\n\n\n\n\n\n\nZN\n0.0\n12.5\n17.5\n18.0\n20.0\n21.0\n22.0\n25.0\n28.0\n30.0\n...\n55.0\n60.0\n70.0\n75.0\n80.0\n82.5\n85.0\n90.0\n95.0\n100.0\n\n\nCAT_MEDV\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n0.91\n1.0\n0.0\n1.0\n0.24\n1.0\n0.9\n1.0\n1.0\n1.0\n...\n0.67\n0.75\n1.0\n0.33\n0.67\n0.5\n1.0\n0.0\n0.0\n0.0\n\n\n1\n0.09\n0.0\n1.0\n0.0\n0.76\n0.0\n0.1\n0.0\n0.0\n0.0\n...\n0.33\n0.25\n0.0\n0.67\n0.33\n0.5\n0.0\n1.0\n1.0\n1.0\n\n\n\n\n2 rows × 26 columns\n\n\n\n\ntbl\n\n\n\n\n\n\n\nZN\n0.0\n12.5\n17.5\n18.0\n20.0\n21.0\n22.0\n25.0\n28.0\n30.0\n...\n55.0\n60.0\n70.0\n75.0\n80.0\n82.5\n85.0\n90.0\n95.0\n100.0\n\n\nCAT_MEDV\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n339\n10\n0\n1\n5\n4\n9\n10\n3\n6\n...\n2\n3\n3\n1\n10\n1\n2\n0\n0\n0\n\n\n1\n33\n0\n1\n0\n16\n0\n1\n0\n0\n0\n...\n1\n1\n0\n2\n5\n1\n0\n5\n4\n1\n\n\n\n\n2 rows × 26 columns\n\n\n\nThe dataframe method plot allows creating various graphs. The graph is created using matplotlib and therefore can be further manipulated.\n\nax = propTbl.transpose().plot(kind='bar', stacked=True, width=0.9)\nax.set_yticklabels(['{:,.0%}'.format(x) for x in ax.get_yticks()])\nplt.title('Distribution of CAT.MEDV by ZN')\nplt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\nplt.tight_layout()\nplt.show()\n\nUserWarning: FixedFormatter should only be used together with FixedLocator\n  ax.set_yticklabels(['{:,.0%}'.format(x) for x in ax.get_yticks()])"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 04 - Dimension reduction.html#table-4.10",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 04 - Dimension reduction.html#table-4.10",
    "title": "Chapter 4: Dimension Reduction",
    "section": "Table 4.10",
    "text": "Table 4.10\nCompute principal components on two dimensions\n\ncereals_df = dmba.load_data('Cereals.csv')\npcs = PCA(n_components=2)\npcs.fit(cereals_df[['calories', 'rating']])\n\nPCA(n_components=2)\n\n\nThe importance of components can be assessed using the explained variance.\n\npcsSummary = pd.DataFrame({'Standard deviation': np.sqrt(pcs.explained_variance_),\n                           'Proportion of variance': pcs.explained_variance_ratio_,\n                           'Cumulative proportion': np.cumsum(pcs.explained_variance_ratio_)})\npcsSummary = pcsSummary.transpose()\npcsSummary.columns = ['PC1', 'PC2']\npcsSummary.round(4)\n\n\n\n\n\n\n\n\nPC1\nPC2\n\n\n\n\nStandard deviation\n22.3165\n8.8844\n\n\nProportion of variance\n0.8632\n0.1368\n\n\nCumulative proportion\n0.8632\n1.0000\n\n\n\n\n\n\n\nThe components_ field of pcs gives the individual components. The columns in this matrix are the principal components PC1, PC2. The rows are variables in the order they are found in the input matrix, calories and rating.\n\npcsComponents_df = pd.DataFrame(pcs.components_.transpose(), columns=['PC1', 'PC2'], \n                                index=['calories', 'rating'])\npcsComponents_df\n\n\n\n\n\n\n\n\nPC1\nPC2\n\n\n\n\ncalories\n-0.847053\n0.531508\n\n\nrating\n0.531508\n0.847053\n\n\n\n\n\n\n\nUse the transform method to get the scores.\n\nscores = pd.DataFrame(pcs.transform(cereals_df[['calories', 'rating']]), \n                      columns=['PC1', 'PC2'])\nscores.head()\n\n\n\n\n\n\n\n\nPC1\nPC2\n\n\n\n\n0\n44.921528\n2.197183\n\n\n1\n-15.725265\n-0.382416\n\n\n2\n40.149935\n-5.407212\n\n\n3\n75.310772\n12.999126\n\n\n4\n-7.041508\n-5.357686"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 04 - Dimension reduction.html#table-4.11",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 04 - Dimension reduction.html#table-4.11",
    "title": "Chapter 4: Dimension Reduction",
    "section": "Table 4.11",
    "text": "Table 4.11\nPerform a principal component analysis of the whole table ignoring the first three non-numerical columns.\n\npcs = PCA()\npcs.fit(cereals_df.iloc[:, 3:].dropna(axis=0))\npcsSummary_df = pd.DataFrame({'Standard deviation': np.sqrt(pcs.explained_variance_),\n                           'Proportion of variance': pcs.explained_variance_ratio_,\n                           'Cumulative proportion': np.cumsum(pcs.explained_variance_ratio_)})\npcsSummary_df = pcsSummary_df.transpose()\npcsSummary_df.columns = ['PC{}'.format(i) for i in range(1, len(pcsSummary_df.columns) + 1)]\npcsSummary_df.round(4)\n\n\n\n\n\n\n\n\nPC1\nPC2\nPC3\nPC4\nPC5\nPC6\nPC7\nPC8\nPC9\nPC10\nPC11\nPC12\nPC13\n\n\n\n\nStandard deviation\n83.7641\n70.9143\n22.6437\n19.1815\n8.4232\n2.0917\n1.6994\n0.7796\n0.6578\n0.3704\n0.1864\n0.063\n0.0\n\n\nProportion of variance\n0.5395\n0.3867\n0.0394\n0.0283\n0.0055\n0.0003\n0.0002\n0.0000\n0.0000\n0.0000\n0.0000\n0.000\n0.0\n\n\nCumulative proportion\n0.5395\n0.9262\n0.9656\n0.9939\n0.9993\n0.9997\n0.9999\n1.0000\n1.0000\n1.0000\n1.0000\n1.000\n1.0\n\n\n\n\n\n\n\n\npcsComponents_df = pd.DataFrame(pcs.components_.transpose(), columns=pcsSummary_df.columns, \n                                index=cereals_df.iloc[:, 3:].columns)\npcsComponents_df.iloc[:,:5]\n\n\n\n\n\n\n\n\nPC1\nPC2\nPC3\nPC4\nPC5\n\n\n\n\ncalories\n-0.077984\n-0.009312\n0.629206\n-0.601021\n0.454959\n\n\nprotein\n0.000757\n0.008801\n0.001026\n0.003200\n0.056176\n\n\nfat\n0.000102\n0.002699\n0.016196\n-0.025262\n-0.016098\n\n\nsodium\n-0.980215\n0.140896\n-0.135902\n-0.000968\n0.013948\n\n\nfiber\n0.005413\n0.030681\n-0.018191\n0.020472\n0.013605\n\n\ncarbo\n-0.017246\n-0.016783\n0.017370\n0.025948\n0.349267\n\n\nsugars\n-0.002989\n-0.000253\n0.097705\n-0.115481\n-0.299066\n\n\npotass\n0.134900\n0.986562\n0.036782\n-0.042176\n-0.047151\n\n\nvitamins\n-0.094293\n0.016729\n0.691978\n0.714118\n-0.037009\n\n\nshelf\n0.001541\n0.004360\n0.012489\n0.005647\n-0.007876\n\n\nweight\n-0.000512\n0.000999\n0.003806\n-0.002546\n0.003022\n\n\ncups\n-0.000510\n-0.001591\n0.000694\n0.000985\n0.002148\n\n\nrating\n0.075296\n0.071742\n-0.307947\n0.334534\n0.757708"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 04 - Dimension reduction.html#table-4.12",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 04 - Dimension reduction.html#table-4.12",
    "title": "Chapter 4: Dimension Reduction",
    "section": "Table 4.12",
    "text": "Table 4.12\nWe kept the Tableau graphic in the book, however these code snippets demonstrate how a similar graph could be generated using Python\nUse the preprocessing functionality in scikit-learn to standardize the data before the PCA\n\npcs = PCA()\npcs.fit(preprocessing.scale(cereals_df.iloc[:, 3:].dropna(axis=0)))\npcsSummary_df = pd.DataFrame({'Standard deviation': np.sqrt(pcs.explained_variance_),\n                           'Proportion of variance': pcs.explained_variance_ratio_,\n                           'Cumulative proportion': np.cumsum(pcs.explained_variance_ratio_)})\npcsSummary_df = pcsSummary_df.transpose()\npcsSummary_df.columns = ['PC{}'.format(i) for i in range(1, len(pcsSummary_df.columns) + 1)]\npcsSummary_df.round(4)\n\n\n\n\n\n\n\n\nPC1\nPC2\nPC3\nPC4\nPC5\nPC6\nPC7\nPC8\nPC9\nPC10\nPC11\nPC12\nPC13\n\n\n\n\nStandard deviation\n1.9192\n1.7864\n1.3912\n1.0166\n1.0015\n0.8555\n0.8251\n0.6496\n0.5658\n0.3051\n0.2537\n0.1399\n0.0\n\n\nProportion of variance\n0.2795\n0.2422\n0.1469\n0.0784\n0.0761\n0.0555\n0.0517\n0.0320\n0.0243\n0.0071\n0.0049\n0.0015\n0.0\n\n\nCumulative proportion\n0.2795\n0.5217\n0.6685\n0.7470\n0.8231\n0.8786\n0.9303\n0.9623\n0.9866\n0.9936\n0.9985\n1.0000\n1.0\n\n\n\n\n\n\n\n\npcsComponents_df = pd.DataFrame(pcs.components_.transpose(), columns=pcsSummary_df.columns, \n                                index=cereals_df.iloc[:, 3:].columns)\npcsComponents_df.iloc[:,:5]\n\n\n\n\n\n\n\n\nPC1\nPC2\nPC3\nPC4\nPC5\n\n\n\n\ncalories\n-0.299542\n-0.393148\n0.114857\n-0.204359\n0.203899\n\n\nprotein\n0.307356\n-0.165323\n0.277282\n-0.300743\n0.319749\n\n\nfat\n-0.039915\n-0.345724\n-0.204890\n-0.186833\n0.586893\n\n\nsodium\n-0.183397\n-0.137221\n0.389431\n-0.120337\n-0.338364\n\n\nfiber\n0.453490\n-0.179812\n0.069766\n-0.039174\n-0.255119\n\n\ncarbo\n-0.192449\n0.149448\n0.562452\n-0.087835\n0.182743\n\n\nsugars\n-0.228068\n-0.351434\n-0.355405\n0.022707\n-0.314872\n\n\npotass\n0.401964\n-0.300544\n0.067620\n-0.090878\n-0.148360\n\n\nvitamins\n-0.115980\n-0.172909\n0.387859\n0.604111\n-0.049287\n\n\nshelf\n0.171263\n-0.265050\n-0.001531\n0.638879\n0.329101\n\n\nweight\n-0.050299\n-0.450309\n0.247138\n-0.153429\n-0.221283\n\n\ncups\n-0.294636\n0.212248\n0.140000\n-0.047489\n0.120816\n\n\nrating\n0.438378\n0.251539\n0.181842\n-0.038316\n0.057584\n\n\n\n\n\n\n\n\nfrom adjustText import adjust_text\ncereals_red_df = cereals_df.dropna(axis=0)\ncereals_red_df = cereals_red_df.reset_index(drop=True)\n\nscores = pd.DataFrame(pcs.fit_transform(preprocessing.scale(cereals_red_df.iloc[:, 3:].dropna(axis=0))), \n                      columns=[f'PC{i}' for i in range(1, 14)])\ncereals_pca_df = pd.concat([cereals_red_df['name'].dropna(axis=0), scores[['PC1', 'PC2']]], axis=1)\nax = cereals_pca_df.plot.scatter(x='PC1', y='PC2', figsize=(6, 6))\npoints = cereals_pca_df[['PC1','PC2','name']]\n\ntexts = []\nfor _, (x, y, s) in points.iterrows():\n        texts.append(ax.text(x, y, s))\nadjust_text(texts, force_text=0.05, arrowprops=dict(arrowstyle=\"-|&gt;\", color='r', alpha=0.5))\n\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom sklearn.cluster import KMeans\n\ncereals_pca_df = pd.concat([cereals_df['name'], scores], axis=1) #ignore_index=True)\nax = cereals_pca_df.plot.scatter(x='PC1', y='PC2', figsize=(8, 12))\npoints = pd.DataFrame(points)\n\ndef extractPoint(df, texts):\n    for _, (x, y, s) in df.sample(1)[['PC1','PC2','name']].iterrows():\n        texts.append(ax.text(x, y, s))\n\nkmeans = KMeans(n_clusters=20, random_state=0).fit(points[['PC1', 'PC2']])\npoints['cluster'] = kmeans.labels_\ntexts = []\npoints.groupby(['cluster']).apply(lambda g: extractPoint(g, texts))\n\nadjust_text(texts, force_text=0.05, arrowprops=dict(arrowstyle=\"-|&gt;\", color='grey', alpha=0.5))\nplt.show()"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 04 - Dimension reduction.html#table-4.13",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 04 - Dimension reduction.html#table-4.13",
    "title": "Chapter 4: Dimension Reduction",
    "section": "Table 4.13",
    "text": "Table 4.13\n\nwine_df = dmba.load_data('Wine.csv')\nwine_df = wine_df.drop(columns=['Type'])\n\n\npcs = PCA()\npcs.fit(wine_df.dropna(axis=0))\npcsSummary_df = pd.DataFrame({'Standard deviation': np.sqrt(pcs.explained_variance_),\n                           'Proportion of variance': pcs.explained_variance_ratio_,\n                           'Cumulative proportion': np.cumsum(pcs.explained_variance_ratio_)})\npcsSummary_df = pcsSummary_df.transpose()\npcsSummary_df.columns = ['PC{}'.format(i) for i in range(1, len(pcsSummary_df.columns) + 1)]\npcsSummary_df.round(4)\n\n\n\n\n\n\n\n\nPC1\nPC2\nPC3\nPC4\nPC5\nPC6\nPC7\nPC8\nPC9\nPC10\nPC11\nPC12\nPC13\n\n\n\n\nStandard deviation\n314.9632\n13.1353\n3.0722\n2.2341\n1.1085\n0.9171\n0.5282\n0.3891\n0.3348\n0.2678\n0.1938\n0.1452\n0.0906\n\n\nProportion of variance\n0.9981\n0.0017\n0.0001\n0.0001\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\nCumulative proportion\n0.9981\n0.9998\n0.9999\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n\n\n\n\n\n\n\n\npcsComponents_df = pd.DataFrame(pcs.components_.transpose(), columns=pcsSummary_df.columns, \n                                index=wine_df.columns)\npcsComponents_df.iloc[:,:5]\n\n\n\n\n\n\n\n\nPC1\nPC2\nPC3\nPC4\nPC5\n\n\n\n\nAlcohol\n0.001659\n0.001203\n-0.016874\n-0.141447\n0.020337\n\n\nMalic_Acid\n-0.000681\n0.002155\n-0.122003\n-0.160390\n-0.612883\n\n\nAsh\n0.000195\n0.004594\n-0.051987\n0.009773\n0.020176\n\n\nAsh_Alcalinity\n-0.004671\n0.026450\n-0.938593\n0.330965\n0.064352\n\n\nMagnesium\n0.017868\n0.999344\n0.029780\n0.005394\n-0.006149\n\n\nTotal_Phenols\n0.000990\n0.000878\n0.040485\n0.074585\n0.315245\n\n\nFlavanoids\n0.001567\n-0.000052\n0.085443\n0.169087\n0.524761\n\n\nNonflavanoid_Phenols\n-0.000123\n-0.001354\n-0.013511\n-0.010806\n-0.029648\n\n\nProanthocyanins\n0.000601\n0.005004\n0.024659\n0.050121\n0.251183\n\n\nColor_Intensity\n0.002327\n0.015100\n-0.291398\n-0.878894\n0.331747\n\n\nHue\n0.000171\n-0.000763\n0.025978\n0.060035\n0.051524\n\n\nOD280_OD315\n0.000705\n-0.003495\n0.070324\n0.178200\n0.260639\n\n\nProline\n0.999823\n-0.017774\n-0.004529\n0.003113\n-0.002299"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 05 - Evaluating performance.html",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 05 - Evaluating performance.html",
    "title": "Chapter 5: Evaluating Predictive Performance",
    "section": "",
    "text": "2019 Galit Shmueli, Peter C. Bruce, Peter Gedeck\n\nCode included in\nData Mining for Business Analytics: Concepts, Techniques, and Applications in Python (First Edition) Galit Shmueli, Peter C. Bruce, Peter Gedeck, and Nitin R. Patel. 2019."
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 05 - Evaluating performance.html#import-required-packages",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 05 - Evaluating performance.html#import-required-packages",
    "title": "Chapter 5: Evaluating Predictive Performance",
    "section": "Import required packages",
    "text": "Import required packages\n\nimport math\nfrom pathlib import Path\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import accuracy_score, roc_curve, auc\nimport matplotlib.pylab as plt\n\nimport dmba\nfrom dmba import regressionSummary, classificationSummary, liftChart, gainsChart\n\n%matplotlib inline\n\nno display found. Using non-interactive Agg backend"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 05 - Evaluating performance.html#table-5.1",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 05 - Evaluating performance.html#table-5.1",
    "title": "Chapter 5: Evaluating Predictive Performance",
    "section": "Table 5.1",
    "text": "Table 5.1\nLoad file and generate training and validation sets.\n\n# Load data frame and select columns for regression analysis\ncar_df = dmba.load_data('ToyotaCorolla.csv')\n\n# create a list of predictor variables by remvoing output variables and text columns\nexcludeColumns = ('Price', 'Id', 'Model', 'Fuel_Type', 'Color')\npredictors = [s for s in car_df.columns if s not in excludeColumns]\noutcome = 'Price'\n\n# partition data\nX = car_df[predictors]\ny = car_df[outcome]\ntrain_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size=0.4, random_state=1)\n\n# train linear regression model\nreg = LinearRegression()\nreg.fit(train_X, train_y)\n\n# evaluate performance\n# training\nregressionSummary(train_y, reg.predict(train_X))\n# validation\nregressionSummary(valid_y, reg.predict(valid_X))\n\n\nRegression statistics\n\n                      Mean Error (ME) : -0.0000\n       Root Mean Squared Error (RMSE) : 1121.0606\n            Mean Absolute Error (MAE) : 811.6770\n          Mean Percentage Error (MPE) : -0.8630\nMean Absolute Percentage Error (MAPE) : 8.0054\n\nRegression statistics\n\n                      Mean Error (ME) : 97.1891\n       Root Mean Squared Error (RMSE) : 1382.0352\n            Mean Absolute Error (MAE) : 880.1396\n          Mean Percentage Error (MPE) : 0.0138\nMean Absolute Percentage Error (MAPE) : 8.8744"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 05 - Evaluating performance.html#figure-5.1",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 05 - Evaluating performance.html#figure-5.1",
    "title": "Chapter 5: Evaluating Predictive Performance",
    "section": "Figure 5.1",
    "text": "Figure 5.1\n\npred_error_train = pd.DataFrame({\n    'residual': train_y - reg.predict(train_X), \n    'data set': 'training'\n})\npred_error_valid = pd.DataFrame({\n    'residual': valid_y - reg.predict(valid_X), \n    'data set': 'validation'\n})\nboxdata_df = pred_error_train.append(pred_error_valid, ignore_index=True)\n\nfig, axes = plt.subplots(nrows=1, ncols=3)\nfig.set_size_inches(9, 4)\ncommon = {'bins': 100, 'range': [-6500, 6500]}\npred_error_train.hist(ax=axes[0], **common)\npred_error_valid.hist(ax=axes[1], **common)\nboxdata_df.boxplot(ax=axes[2], by='data set')\n\naxes[0].set_title('training')\naxes[1].set_title('validation')\naxes[2].set_title(' ')\naxes[2].set_ylim(-6500, 6500)\nplt.suptitle('Prediction errors') \nplt.subplots_adjust(bottom=0.15, top=0.85, wspace=0.35)\n\nplt.show()"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 05 - Evaluating performance.html#figure-5.2",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 05 - Evaluating performance.html#figure-5.2",
    "title": "Chapter 5: Evaluating Predictive Performance",
    "section": "Figure 5.2",
    "text": "Figure 5.2\n\n# sort the actual values in descending order of the prediction\ndf = pd.DataFrame({\n    'predicted': reg.predict(valid_X),\n    'actual': valid_y, \n})\ndf = df.sort_values(by=['predicted'], ascending=False)\n\nfig, axes = plt.subplots(nrows=1, ncols=2)\nax = gainsChart(df['actual'], ax=axes[0])\nax.set_ylabel('Cumulative Price')\nax.set_title('Cumulative Gains Chart')\n\nax = liftChart(df['actual'], ax=axes[1], labelBars=False)\nax.set_ylabel('Lift')\n\nplt.tight_layout()\nplt.show()\n\n\n# group the sorted predictions into 10 roughly equal groups and calculate the mean\nsorted_act_v = df['actual']\ngroups = [int(10 * i / len(sorted_act_v)) for i in range(len(sorted_act_v))]\nmeanPercentile = sorted_act_v.groupby(groups).mean()\nmeanResponse = meanPercentile / sorted_act_v.mean()\nmeanResponse.index = (meanResponse.index + 1) * 10\nprint('Lift based on meanResponse', meanResponse[10])\nrandom10 = sorted_act_v.cumsum().iloc[-1] / 10  # expected cumulative price without lift for 10% sales\ncumPred10 = sorted_act_v.cumsum().iloc[57]  # cumulative price based on model for top 10%\nprint('Expected cumulative price for 10% random sales', random10)\nprint('Cumulative price for top 10% sales', cumPred10)\nprint('Lift calculated based on gains chart', cumPred10 / random10)\n\n\n\n\n\n\n\n\nLift based on meanResponse 1.7618191980414206\nExpected cumulative price for 10% random sales 613291.8\nCumulative price for top 10% sales 1089905\nLift calculated based on gains chart 1.777139364980911"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 05 - Evaluating performance.html#table-5.5",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 05 - Evaluating performance.html#table-5.5",
    "title": "Chapter 5: Evaluating Predictive Performance",
    "section": "Table 5.5",
    "text": "Table 5.5\n\nowner_df = dmba.load_data('ownerExample.csv')\nowner_df.head()\nclass_names = ['nonowner', 'owner']\n\n\npredicted = ['owner' if p &gt; 0.5 else 'nonowner' for p in owner_df.Probability]\nclassificationSummary(owner_df.Class, predicted, class_names=class_names)\n\nConfusion Matrix (Accuracy 0.8750)\n\n         Prediction\n  Actual nonowner    owner\nnonowner       10        2\n   owner        1       11\n\n\n\npredicted = ['owner' if p &gt; 0.25 else 'nonowner' for p in owner_df.Probability]\nclassificationSummary(owner_df.Class, predicted, class_names=class_names)\n\nConfusion Matrix (Accuracy 0.7917)\n\n         Prediction\n  Actual nonowner    owner\nnonowner        8        4\n   owner        1       11\n\n\n\npredicted = ['owner' if p &gt; 0.75 else 'nonowner' for p in owner_df.Probability]\nclassificationSummary(owner_df.Class, predicted, class_names=class_names)\n\nConfusion Matrix (Accuracy 0.7500)\n\n         Prediction\n  Actual nonowner    owner\nnonowner       11        1\n   owner        5        7"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 05 - Evaluating performance.html#figure-5.4",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 05 - Evaluating performance.html#figure-5.4",
    "title": "Chapter 5: Evaluating Predictive Performance",
    "section": "Figure 5.4",
    "text": "Figure 5.4\nCalculate the accuracy at different cutoff values and create a graph.\n\ndf = dmba.load_data('liftExample.csv')\n\ncutoffs = [i * 0.1 for i in range(0, 11)]\naccT = []\nfor cutoff in cutoffs:\n    predicted = [1 if p &gt; cutoff else 0 for p in df.prob]\n    accT.append(accuracy_score(df.actual, predicted))\n\nline_accuracy = plt.plot(cutoffs, accT, '-', label='Accuracy')[0]\nline_error = plt.plot(cutoffs, [1 - acc for acc in accT], '--', label='Overall error')[0]\nplt.ylim([0,1])\nplt.xlabel('Cutoff Value')\nplt.legend(handles=[line_accuracy, line_error])\n\nplt.show()"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 05 - Evaluating performance.html#figure-5.5",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 05 - Evaluating performance.html#figure-5.5",
    "title": "Chapter 5: Evaluating Predictive Performance",
    "section": "Figure 5.5",
    "text": "Figure 5.5\n\nfpr, tpr, _ = roc_curve(df.actual, df.prob)\nroc_auc = auc(fpr, tpr)\n\n\nplt.figure(figsize=[5, 5])\nlw = 2\nplt.plot(fpr, tpr, color='darkorange',\n         lw=lw, label='ROC curve (area = %0.4f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Sensitivity)')\nplt.legend(loc=\"lower right\")\n\nplt.show()"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 05 - Evaluating performance.html#figure-5.6",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 05 - Evaluating performance.html#figure-5.6",
    "title": "Chapter 5: Evaluating Predictive Performance",
    "section": "Figure 5.6",
    "text": "Figure 5.6\n\ndf = dmba.load_data('liftExample.csv')\ndf = df.sort_values(by=['prob'], ascending=False)\n\ngainsChart(df.actual, figsize=(4, 4))\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 05 - Evaluating performance.html#figure-5.7",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 05 - Evaluating performance.html#figure-5.7",
    "title": "Chapter 5: Evaluating Predictive Performance",
    "section": "Figure 5.7",
    "text": "Figure 5.7\nThe created figure differs from the R version of the graph due to variations in the assignment to the percentiles.\n\nliftChart(df.actual, labelBars=False)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 08 - Naive Bayes classifier.html",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 08 - Naive Bayes classifier.html",
    "title": "Chapter 8: The Naive Bayes Classifier",
    "section": "",
    "text": "2019 Galit Shmueli, Peter C. Bruce, Peter Gedeck\n\nCode included in\nData Mining for Business Analytics: Concepts, Techniques, and Applications in Python (First Edition) Galit Shmueli, Peter C. Bruce, Peter Gedeck, and Nitin R. Patel. 2019."
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 08 - Naive Bayes classifier.html#import-required-packages",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 08 - Naive Bayes classifier.html#import-required-packages",
    "title": "Chapter 8: The Naive Bayes Classifier",
    "section": "Import required packages",
    "text": "Import required packages\n\nfrom pathlib import Path\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nimport matplotlib.pylab as plt\n\nimport dmba\nfrom dmba import classificationSummary, gainsChart\n\n%matplotlib inline\n\nno display found. Using non-interactive Agg backend"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 08 - Naive Bayes classifier.html#table-8.4",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 08 - Naive Bayes classifier.html#table-8.4",
    "title": "Chapter 8: The Naive Bayes Classifier",
    "section": "Table 8.4",
    "text": "Table 8.4\n\ndelays_df = dmba.load_data('FlightDelays.csv')\ndelays_df.head()\n\n\n\n\n\n\n\n\nCRS_DEP_TIME\nCARRIER\nDEP_TIME\nDEST\nDISTANCE\nFL_DATE\nFL_NUM\nORIGIN\nWeather\nDAY_WEEK\nDAY_OF_MONTH\nTAIL_NUM\nFlight Status\n\n\n\n\n0\n1455\nOH\n1455\nJFK\n184\n01/01/2004\n5935\nBWI\n0\n4\n1\nN940CA\nontime\n\n\n1\n1640\nDH\n1640\nJFK\n213\n01/01/2004\n6155\nDCA\n0\n4\n1\nN405FJ\nontime\n\n\n2\n1245\nDH\n1245\nLGA\n229\n01/01/2004\n7208\nIAD\n0\n4\n1\nN695BR\nontime\n\n\n3\n1715\nDH\n1709\nLGA\n229\n01/01/2004\n7215\nIAD\n0\n4\n1\nN662BR\nontime\n\n\n4\n1039\nDH\n1035\nLGA\n229\n01/01/2004\n7792\nIAD\n0\n4\n1\nN698BR\nontime\n\n\n\n\n\n\n\n\ndelays_df = dmba.load_data('FlightDelays.csv')\n\n# convert to categorical\ndelays_df.DAY_WEEK = delays_df.DAY_WEEK.astype('category')\n\n# create hourly bins departure time \ndelays_df.CRS_DEP_TIME = [round(t / 100) for t in delays_df.CRS_DEP_TIME]\ndelays_df.CRS_DEP_TIME = delays_df.CRS_DEP_TIME.astype('category')\n\npredictors = ['DAY_WEEK', 'CRS_DEP_TIME', 'ORIGIN', 'DEST', 'CARRIER']\noutcome = 'Flight Status'\n\nX = pd.get_dummies(delays_df[predictors])\ny = (delays_df[outcome] == 'delayed').astype(int)\nclasses = ['ontime', 'delayed']\n\n# split into training and validation\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.40, random_state=1)\n\n# run naive Bayes\ndelays_nb = MultinomialNB(alpha=0.01)\ndelays_nb.fit(X_train, y_train)\n\n# predict probabilities\npredProb_train = delays_nb.predict_proba(X_train)\npredProb_valid = delays_nb.predict_proba(X_valid)\n\n# predict class membership\ny_valid_pred = delays_nb.predict(X_valid)\ny_train_pred = delays_nb.predict(X_train)\n\n\ndelays_df = dmba.load_data('FlightDelays.csv')\n\n# convert to categorical\ndelays_df.DAY_WEEK = delays_df.DAY_WEEK.astype('category')\ndelays_df['Flight Status'] = delays_df['Flight Status'].astype('category')\n\n# create hourly bins departure time \ndelays_df.CRS_DEP_TIME = [round(t / 100) for t in delays_df.CRS_DEP_TIME]\ndelays_df.CRS_DEP_TIME = delays_df.CRS_DEP_TIME.astype('category')\n\npredictors = ['DAY_WEEK', 'CRS_DEP_TIME', 'ORIGIN', 'DEST', 'CARRIER']\noutcome = 'Flight Status'\n\nX = pd.get_dummies(delays_df[predictors])\ny = delays_df['Flight Status']\nclasses = list(y.cat.categories)\n\n# split into training and validation\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.40, random_state=1)\n\n# run naive Bayes\ndelays_nb = MultinomialNB(alpha=0.01)\ndelays_nb.fit(X_train, y_train)\n\n# predict probabilities\npredProb_train = delays_nb.predict_proba(X_train)\npredProb_valid = delays_nb.predict_proba(X_valid)\n\n# predict class membership\ny_valid_pred = delays_nb.predict(X_valid)\ny_train_pred = delays_nb.predict(X_train)"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 08 - Naive Bayes classifier.html#table-8.5",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 08 - Naive Bayes classifier.html#table-8.5",
    "title": "Chapter 8: The Naive Bayes Classifier",
    "section": "Table 8.5",
    "text": "Table 8.5\nFirst construct a frequency table and then convert it to the propability table\n\n# split the original data frame into a train and test using the same random_state\ntrain_df, valid_df = train_test_split(delays_df, test_size=0.4, random_state=1)\n\npd.set_option('precision', 4)\n# probability of flight status\nprint(train_df['Flight Status'].value_counts() / len(train_df))\nprint()\n\nfor predictor in predictors:\n    # construct the frequency table\n    df = train_df[['Flight Status', predictor]]\n    freqTable = df.pivot_table(index='Flight Status', columns=predictor, aggfunc=len)\n\n    # divide each row by the sum of the row to get conditional probabilities\n    propTable = freqTable.apply(lambda x: x / sum(x), axis=1)\n    print(propTable)\n    print()\npd.reset_option('precision')\n\nontime     0.8023\ndelayed    0.1977\nName: Flight Status, dtype: float64\n\nDAY_WEEK            1       2       3       4       5      6       7\nFlight Status                                                       \ndelayed        0.1916  0.1494  0.1149  0.1264  0.1877  0.069  0.1609\nontime         0.1246  0.1416  0.1445  0.1794  0.1690  0.136  0.1048\n\nCRS_DEP_TIME        6       7       8       9      10      11      12      13  \\\nFlight Status                                                                   \ndelayed        0.0345  0.0536  0.0651  0.0192  0.0307  0.0115  0.0498  0.0460   \nontime         0.0623  0.0633  0.0850  0.0567  0.0519  0.0340  0.0661  0.0746   \n\nCRS_DEP_TIME       14      15      16      17      18      19      20      21  \nFlight Status                                                                  \ndelayed        0.0383  0.2031  0.0728  0.1533  0.0192  0.0996  0.0153  0.0881  \nontime         0.0576  0.1171  0.0774  0.1001  0.0349  0.0397  0.0264  0.0529  \n\nORIGIN            BWI     DCA     IAD\nFlight Status                        \ndelayed        0.0805  0.5211  0.3985\nontime         0.0604  0.6478  0.2918\n\nDEST              EWR     JFK     LGA\nFlight Status                        \ndelayed        0.3793  0.1992  0.4215\nontime         0.2663  0.1558  0.5779\n\nCARRIER            CO      DH      DL      MQ      OH      RU      UA      US\nFlight Status                                                                \ndelayed        0.0575  0.3142  0.0958  0.2222  0.0077  0.2184  0.0153  0.0690\nontime         0.0349  0.2295  0.2040  0.1171  0.0104  0.1690  0.0170  0.2181\n\n\n\n\n# P(delayed | Carrier = DL, Day_Week = 7, Dep_Time = 10, Dest = LGA, Origin = DCA)\nP_hat_delayed = 0.0958 * 0.1609 * 0.0307 * 0.4215 * 0.5211 * 0.1977\n# P(ontime | Carrier = DL, Day_Week = 7, Dep_Time = 10, Dest = LGA, Origin = DCA)\nP_hat_ontime = 0.2040 * 0.1048 * 0.0519 * 0.5779 * 0.6478 * 0.8023\nprint('P_hat_delayed ~ ', P_hat_delayed)\nprint('P_hat_ontime ~ ', P_hat_ontime)\n\nprint('P(delayed|...) = ', P_hat_delayed / (P_hat_delayed + P_hat_ontime))\nprint('P(ontime|...) = ', P_hat_ontime / (P_hat_delayed + P_hat_ontime))\n\nP_hat_delayed ~  2.0548742506526157e-05\nP_hat_ontime ~  0.00033326464123921066\nP(delayed|...) =  0.05807791183301656\nP(ontime|...) =  0.9419220881669834"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 08 - Naive Bayes classifier.html#table-8.6",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 08 - Naive Bayes classifier.html#table-8.6",
    "title": "Chapter 8: The Naive Bayes Classifier",
    "section": "Table 8.6",
    "text": "Table 8.6\n\n# Subset a specific set\ndf = pd.concat([pd.DataFrame({'actual': y_valid, 'predicted': y_valid_pred}),\n                pd.DataFrame(predProb_valid, index=y_valid.index)], axis=1)\nmask = ((X_valid.CARRIER_DL == 1) & (X_valid.DAY_WEEK_7 == 1) & (X_valid.CRS_DEP_TIME_10 == 1) & \n        (X_valid.DEST_LGA == 1) & (X_valid.ORIGIN_DCA == 1))\n\nprint(df[mask])\n\n      actual predicted         0         1\n1225  ontime    ontime  0.057989  0.942011"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 08 - Naive Bayes classifier.html#table-8.7",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 08 - Naive Bayes classifier.html#table-8.7",
    "title": "Chapter 8: The Naive Bayes Classifier",
    "section": "Table 8.7",
    "text": "Table 8.7\n\nclassificationSummary(y_train, y_train_pred, class_names=classes) \n\nprint()\n\nclassificationSummary(y_valid, y_valid_pred, class_names=classes) \n\nConfusion Matrix (Accuracy 0.7955)\n\n        Prediction\n Actual delayed  ontime\ndelayed      52     209\n ontime      61     998\n\nConfusion Matrix (Accuracy 0.7821)\n\n        Prediction\n Actual delayed  ontime\ndelayed      26     141\n ontime      51     663"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 08 - Naive Bayes classifier.html#figure-8.1",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 08 - Naive Bayes classifier.html#figure-8.1",
    "title": "Chapter 8: The Naive Bayes Classifier",
    "section": "Figure 8.1",
    "text": "Figure 8.1\n\ndf = pd.DataFrame({'actual': 1 - y_valid.cat.codes, 'prob': predProb_valid[:, 0]})\ndf = df.sort_values(by=['prob'], ascending=False).reset_index(drop=True)\n\nfig, ax = plt.subplots()\nfig.set_size_inches(4, 4)\ngainsChart(df.actual, ax=ax)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nimport numpy as np\nnp.exp(delays_nb.feature_log_prob_)\n\narray([[0.03831098, 0.02988425, 0.02298965, 0.02528785, 0.03754491,\n        0.01379685, 0.03218245, 0.00690226, 0.01073259, 0.01303079,\n        0.00383799, 0.00613619, 0.00230586, 0.00996652, 0.00920046,\n        0.00766832, 0.04060918, 0.01456292, 0.03065031, 0.00383799,\n        0.01992539, 0.00307193, 0.01762719, 0.01609505, 0.10419268,\n        0.07967856, 0.07584823, 0.03984311, 0.08427496, 0.01149866,\n        0.0628251 , 0.01915932, 0.04443951, 0.00153979, 0.04367344,\n        0.00307193, 0.01379685],\n       [0.02492933, 0.02832852, 0.02889505, 0.03588229, 0.033805  ,\n        0.02719546, 0.0209636 , 0.01246561, 0.01265445, 0.01699787,\n        0.01133254, 0.01038832, 0.00680028, 0.01322098, 0.01492058,\n        0.01152139, 0.02341857, 0.01548711, 0.02001938, 0.00698912,\n        0.00793335, 0.00528953, 0.01057716, 0.01208792, 0.12954902,\n        0.05835475, 0.05325596, 0.03116118, 0.11557455, 0.00698912,\n        0.04589103, 0.04079224, 0.02341857, 0.00207917, 0.033805  ,\n        0.00340108, 0.0436249 ]])"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 17 - Regression-based forecasting.html",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 17 - Regression-based forecasting.html",
    "title": "Chapter 17: Regression-based Forecasting",
    "section": "",
    "text": "2019 Galit Shmueli, Peter C. Bruce, Peter Gedeck\n\nCode included in\nData Mining for Business Analytics: Concepts, Techniques, and Applications in Python (First Edition) Galit Shmueli, Peter C. Bruce, Peter Gedeck, and Nitin R. Patel. 2019."
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 17 - Regression-based forecasting.html#import-required-packages",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 17 - Regression-based forecasting.html#import-required-packages",
    "title": "Chapter 17: Regression-based Forecasting",
    "section": "Import required packages",
    "text": "Import required packages\n\nfrom pathlib import Path\n\nimport math\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pylab as plt\nimport statsmodels.formula.api as sm\nfrom statsmodels.tsa import tsatools, stattools\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.graphics import tsaplots\n\nimport dmba\n\n%matplotlib inline\n\nno display found. Using non-interactive Agg backend"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 17 - Regression-based forecasting.html#figure-17.1",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 17 - Regression-based forecasting.html#figure-17.1",
    "title": "Chapter 17: Regression-based Forecasting",
    "section": "Figure 17.1",
    "text": "Figure 17.1\n\n# load data and convert to time series\nAmtrak_df = dmba.load_data('Amtrak.csv')\nAmtrak_df['Date'] = pd.to_datetime(Amtrak_df.Month, format='%d/%m/%Y')\nridership_ts = pd.Series(Amtrak_df.Ridership.values, index=Amtrak_df.Date, name='Ridership')\nridership_ts.index = pd.DatetimeIndex(ridership_ts.index, freq=ridership_ts.index.inferred_freq)\n\n# fit a linear trend model to the time series \nridership_df = tsatools.add_trend(ridership_ts, trend='ct')\nridership_lm = sm.ols(formula='Ridership ~ trend', data=ridership_df).fit()\n\n# plot the time series\nax = ridership_ts.plot()\nridership_lm.predict(ridership_df).plot(ax=ax)\nax.set_xlabel('Time')\nax.set_ylabel('Ridership (in 000s)')\nax.set_ylim(1300, 2300)\nplt.show()\n\n\n\n\n\n\n\n\n\nridership_df.head()\n\n\n\n\n\n\n\n\nRidership\nconst\ntrend\n\n\nDate\n\n\n\n\n\n\n\n1991-01-01\n1708.917\n1.0\n1.0\n\n\n1991-02-01\n1620.586\n1.0\n2.0\n\n\n1991-03-01\n1972.715\n1.0\n3.0\n\n\n1991-04-01\n1811.665\n1.0\n4.0\n\n\n1991-05-01\n1974.964\n1.0\n5.0"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 17 - Regression-based forecasting.html#figure-17.2",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 17 - Regression-based forecasting.html#figure-17.2",
    "title": "Chapter 17: Regression-based Forecasting",
    "section": "Figure 17.2",
    "text": "Figure 17.2\n\nnValid = 36\nnTrain = len(ridership_ts) - nValid\n\n# partition the data\ntrain_df = ridership_df[:nTrain]\nvalid_df = ridership_df[nTrain:]\n\n\n# Fit linear model using training set and predict on validation set\nridership_lm = sm.ols(formula='Ridership ~ trend', data=train_df).fit()\npredict_df = ridership_lm.predict(valid_df)\n\n# Create the graph\ndef singleGraphLayout(ax, ylim, train_df, valid_df):\n    ax.set_xlim('1990', '2004-6')\n    ax.set_ylim(*ylim)\n    ax.set_xlabel('Time')\n    one_month = pd.Timedelta('31 days')\n    xtrain = (min(train_df.index), max(train_df.index) - one_month)\n    xvalid = (min(valid_df.index) + one_month, max(valid_df.index) - one_month)\n    xtv = xtrain[1] + 0.5 * (xvalid[0] - xtrain[1])\n\n    ypos = 0.9 * ylim[1] + 0.1 * ylim[0]\n    ax.add_line(plt.Line2D(xtrain, (ypos, ypos), color='black', linewidth=0.5))\n    ax.add_line(plt.Line2D(xvalid, (ypos, ypos), color='black', linewidth=0.5))\n    ax.axvline(x=xtv, ymin=0, ymax=1, color='black', linewidth=0.5)\n\n    ypos = 0.925 * ylim[1] + 0.075 * ylim[0]\n    ax.text('1995', ypos, 'Training')\n    ax.text('2002-3', ypos, 'Validation')\n\ndef graphLayout(axes, train_df, valid_df):\n    singleGraphLayout(axes[0], [1300, 2550], train_df, valid_df)\n    singleGraphLayout(axes[1], [-550, 550], train_df, valid_df)\n    train_df.plot(y='Ridership', ax=axes[0], color='C0', linewidth=0.75)\n    valid_df.plot(y='Ridership', ax=axes[0], color='C0', linestyle='dashed', linewidth=0.75)\n    axes[1].axhline(y=0, xmin=0, xmax=1, color='black', linewidth=0.5)\n    axes[0].set_xlabel('')\n    axes[0].set_ylabel('Ridership (in 000s)')\n    axes[1].set_ylabel('Forecast Errors')\n    if axes[0].get_legend(): \n        axes[0].get_legend().remove()\n    # ensure that both axes have the same x-range\n    xlim = (min(axes[0].get_xlim()[0], axes[1].get_xlim()[0]), \n            max(axes[0].get_xlim()[1], axes[1].get_xlim()[1]))\n    axes[0].set_xlim(*xlim)\n    axes[1].set_xlim(*xlim)\n\nfig, axes = plt.subplots(nrows=2, ncols=1, figsize=(9, 7.5))\nridership_lm.predict(train_df).plot(ax=axes[0], color='C1')\nridership_lm.predict(valid_df).plot(ax=axes[0], color='C1', linestyle='dashed')\n    \nresidual = train_df.Ridership - ridership_lm.predict(train_df)\nresidual.plot(ax=axes[1], color='C1')\nresidual = valid_df.Ridership - ridership_lm.predict(valid_df)\nresidual.plot(ax=axes[1], color='C1', linestyle='dashed')\ngraphLayout(axes, train_df, valid_df)\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 17 - Regression-based forecasting.html#table-17.2",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 17 - Regression-based forecasting.html#table-17.2",
    "title": "Chapter 17: Regression-based Forecasting",
    "section": "Table 17.2",
    "text": "Table 17.2\n\nprint(ridership_lm.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:              Ridership   R-squared:                       0.006\nModel:                            OLS   Adj. R-squared:                 -0.002\nMethod:                 Least Squares   F-statistic:                    0.7456\nDate:                Thu, 27 May 2021   Prob (F-statistic):              0.390\nTime:                        20:03:39   Log-Likelihood:                -797.95\nNo. Observations:                 123   AIC:                             1600.\nDf Residuals:                     121   BIC:                             1606.\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept   1750.3595     29.073     60.206      0.000    1692.802    1807.917\ntrend          0.3514      0.407      0.864      0.390      -0.454       1.157\n==============================================================================\nOmnibus:                        4.293   Durbin-Watson:                   1.110\nProb(Omnibus):                  0.117   Jarque-Bera (JB):                4.340\nSkew:                          -0.435   Prob(JB):                        0.114\nKurtosis:                       2.700   Cond. No.                         144.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 17 - Regression-based forecasting.html#exponential-trend",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 17 - Regression-based forecasting.html#exponential-trend",
    "title": "Chapter 17: Regression-based Forecasting",
    "section": "Exponential Trend",
    "text": "Exponential Trend\n\nridership_lm_linear = sm.ols(formula='Ridership ~ trend', data=train_df).fit()\npredict_df_linear = ridership_lm_linear.predict(valid_df)\n\nridership_lm_expo = sm.ols(formula='np.log(Ridership) ~ trend', data=train_df).fit()\npredict_df_expo = ridership_lm_expo.predict(valid_df)\n\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(9,4))\ntrain_df.plot(y='Ridership', ax=ax, color='C0', linewidth=0.75)\nvalid_df.plot(y='Ridership', ax=ax, color='C0', linestyle='dashed', linewidth=0.75)\nsingleGraphLayout(ax, [1300, 2600], train_df, valid_df)\nridership_lm_linear.predict(train_df).plot(color='C1')\nridership_lm_linear.predict(valid_df).plot(color='C1', linestyle='dashed')\nridership_lm_expo.predict(train_df).apply(lambda row: math.exp(row)).plot(color='C2')\nridership_lm_expo.predict(valid_df).apply(lambda row: math.exp(row)).plot(color='C2', \n                                                                          linestyle='dashed')\n\nax.get_legend().remove()\n\nplt.show()\nridership_lm_expo.summary()\n\n\n\n\n\n\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\nnp.log(Ridership)\nR-squared:\n0.005\n\n\nModel:\nOLS\nAdj. R-squared:\n-0.004\n\n\nMethod:\nLeast Squares\nF-statistic:\n0.5707\n\n\nDate:\nThu, 27 May 2021\nProb (F-statistic):\n0.451\n\n\nTime:\n20:03:39\nLog-Likelihood:\n118.67\n\n\nNo. Observations:\n123\nAIC:\n-233.3\n\n\nDf Residuals:\n121\nBIC:\n-227.7\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n7.4647\n0.017\n442.535\n0.000\n7.431\n7.498\n\n\ntrend\n0.0002\n0.000\n0.755\n0.451\n-0.000\n0.001\n\n\n\n\n\n\n\n\nOmnibus:\n8.566\nDurbin-Watson:\n1.106\n\n\nProb(Omnibus):\n0.014\nJarque-Bera (JB):\n8.749\n\n\nSkew:\n-0.652\nProb(JB):\n0.0126\n\n\nKurtosis:\n3.091\nCond. No.\n144.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 17 - Regression-based forecasting.html#figure-17.4",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 17 - Regression-based forecasting.html#figure-17.4",
    "title": "Chapter 17: Regression-based Forecasting",
    "section": "Figure 17.4",
    "text": "Figure 17.4\n\nridership_lm_poly = sm.ols(formula='Ridership ~ trend + np.square(trend)', data=train_df).fit()\n\nfig, axes = plt.subplots(nrows=2, ncols=1, figsize=(9, 7.5))\n\nridership_lm_poly.predict(train_df).plot(ax=axes[0], color='C1')\nridership_lm_poly.predict(valid_df).plot(ax=axes[0], color='C1', linestyle='dashed')\n    \nresidual = train_df.Ridership - ridership_lm_poly.predict(train_df)\nresidual.plot(ax=axes[1], color='C1')\nresidual = valid_df.Ridership - ridership_lm_poly.predict(valid_df)\nresidual.plot(ax=axes[1], color='C1', linestyle='dashed')\n\ngraphLayout(axes, train_df, valid_df)\n\n\nplt.show()"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 17 - Regression-based forecasting.html#table-17.5",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 17 - Regression-based forecasting.html#table-17.5",
    "title": "Chapter 17: Regression-based Forecasting",
    "section": "Table 17.5",
    "text": "Table 17.5\n\nridership_df = tsatools.add_trend(ridership_ts, trend='c')\nridership_df['Month'] = ridership_df.index.month\n\n# partition the data\ntrain_df = ridership_df[:nTrain]\nvalid_df = ridership_df[nTrain:]\n\nridership_lm_season = sm.ols(formula='Ridership ~ C(Month)', data=train_df).fit()\n\nfig, axes = plt.subplots(nrows=2, ncols=1, figsize=(9, 7.5))\n\nridership_lm_season.predict(train_df).plot(ax=axes[0], color='C1')\nridership_lm_season.predict(valid_df).plot(ax=axes[0], color='C1', linestyle='dashed')\n    \nresidual = train_df.Ridership - ridership_lm_season.predict(train_df)\nresidual.plot(ax=axes[1], color='C1')\nresidual = valid_df.Ridership - ridership_lm_season.predict(valid_df)\nresidual.plot(ax=axes[1], color='C1', linestyle='dashed')\n\ngraphLayout(axes, train_df, valid_df)\n\nplt.show()\n\n\n\n\n\n\n\n\n\nprint(ridership_lm_season.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:              Ridership   R-squared:                       0.635\nModel:                            OLS   Adj. R-squared:                  0.599\nMethod:                 Least Squares   F-statistic:                     17.54\nDate:                Thu, 27 May 2021   Prob (F-statistic):           1.20e-19\nTime:                        20:03:40   Log-Likelihood:                -736.38\nNo. Observations:                 123   AIC:                             1497.\nDf Residuals:                     111   BIC:                             1531.\nDf Model:                          11                                         \nCovariance Type:            nonrobust                                         \n==================================================================================\n                     coef    std err          t      P&gt;|t|      [0.025      0.975]\n----------------------------------------------------------------------------------\nIntercept       1573.9722     30.578     51.475      0.000    1513.381    1634.564\nC(Month)[T.2]    -42.9302     43.243     -0.993      0.323    -128.620      42.759\nC(Month)[T.3]    260.7677     43.243      6.030      0.000     175.078     346.457\nC(Month)[T.4]    245.0919     44.311      5.531      0.000     157.286     332.897\nC(Month)[T.5]    278.2222     44.311      6.279      0.000     190.417     366.028\nC(Month)[T.6]    233.4598     44.311      5.269      0.000     145.654     321.265\nC(Month)[T.7]    345.3265     44.311      7.793      0.000     257.521     433.132\nC(Month)[T.8]    396.6595     44.311      8.952      0.000     308.854     484.465\nC(Month)[T.9]     75.7615     44.311      1.710      0.090     -12.044     163.567\nC(Month)[T.10]   200.6076     44.311      4.527      0.000     112.802     288.413\nC(Month)[T.11]   192.3552     44.311      4.341      0.000     104.550     280.161\nC(Month)[T.12]   230.4151     44.311      5.200      0.000     142.610     318.221\n==============================================================================\nOmnibus:                        0.495   Durbin-Watson:                   0.386\nProb(Omnibus):                  0.781   Jarque-Bera (JB):                0.452\nSkew:                          -0.145   Prob(JB):                        0.798\nKurtosis:                       2.937   Cond. No.                         12.5\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nridership_df = tsatools.add_trend(ridership_ts, trend='ct')\nridership_df['Month'] = ridership_df.index.month\n\n# partition the data\ntrain_df = ridership_df[:nTrain]\nvalid_df = ridership_df[nTrain:]\n\nridership_lm_trendseason = sm.ols(formula='Ridership ~ trend + np.square(trend) + C(Month)', data=train_df).fit()\n\nfig, axes = plt.subplots(nrows=2, ncols=1, figsize=(9, 7.5))\n\nridership_lm_trendseason.predict(train_df).plot(ax=axes[0], color='C1')\nridership_lm_trendseason.predict(valid_df).plot(ax=axes[0], color='C1', linestyle='dashed')\n    \nresidual = train_df.Ridership - ridership_lm_trendseason.predict(train_df)\nresidual.plot(ax=axes[1], color='C1')\nresidual = valid_df.Ridership - ridership_lm_trendseason.predict(valid_df)\nresidual.plot(ax=axes[1], color='C1', linestyle='dashed')\n\ngraphLayout(axes, train_df, valid_df)\n\nplt.show()\nprint(ridership_lm_trendseason.summary())\n\n\n\n\n\n\n\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:              Ridership   R-squared:                       0.825\nModel:                            OLS   Adj. R-squared:                  0.804\nMethod:                 Least Squares   F-statistic:                     39.42\nDate:                Thu, 27 May 2021   Prob (F-statistic):           3.75e-35\nTime:                        20:03:41   Log-Likelihood:                -691.27\nNo. Observations:                 123   AIC:                             1411.\nDf Residuals:                     109   BIC:                             1450.\nDf Model:                          13                                         \nCovariance Type:            nonrobust                                         \n====================================================================================\n                       coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------------\nIntercept         1696.9794     27.675     61.318      0.000    1642.128    1751.831\nC(Month)[T.2]      -43.2458     30.241     -1.430      0.156    -103.182      16.690\nC(Month)[T.3]      260.0149     30.242      8.598      0.000     200.076     319.954\nC(Month)[T.4]      260.6175     31.021      8.401      0.000     199.135     322.100\nC(Month)[T.5]      293.7966     31.020      9.471      0.000     232.316     355.278\nC(Month)[T.6]      248.9615     31.020      8.026      0.000     187.481     310.442\nC(Month)[T.7]      360.6340     31.020     11.626      0.000     299.153     422.115\nC(Month)[T.8]      411.6513     31.021     13.270      0.000     350.169     473.134\nC(Month)[T.9]       90.3162     31.022      2.911      0.004      28.831     151.801\nC(Month)[T.10]     214.6037     31.024      6.917      0.000     153.115     276.092\nC(Month)[T.11]     205.6711     31.026      6.629      0.000     144.178     267.165\nC(Month)[T.12]     242.9294     31.029      7.829      0.000     181.430     304.429\ntrend               -7.1559      0.729     -9.812      0.000      -8.601      -5.710\nnp.square(trend)     0.0607      0.006     10.660      0.000       0.049       0.072\n==============================================================================\nOmnibus:                        7.382   Durbin-Watson:                   0.791\nProb(Omnibus):                  0.025   Jarque-Bera (JB):                6.974\nSkew:                          -0.529   Prob(JB):                       0.0306\nKurtosis:                       3.492   Cond. No.                     8.24e+04\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 8.24e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems."
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 17 - Regression-based forecasting.html#figure-17.7",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 17 - Regression-based forecasting.html#figure-17.7",
    "title": "Chapter 17: Regression-based Forecasting",
    "section": "Figure 17.7",
    "text": "Figure 17.7\n\ntsaplots.plot_acf(train_df['1991-01-01':'1993-01-01'].Ridership)\nplt.show()\nprint(stattools.acf(train_df['1991-01-01':'1993-01-01'].Ridership, nlags=24, fft=True))\n\n\n\n\n\n\n\n\n[ 1.00000000e+00  6.35437842e-02 -1.49498748e-01 -3.02952532e-03\n  1.65998491e-01 -8.59840878e-02 -6.75578620e-01 -2.82741268e-02\n  4.54455641e-02  2.44855546e-02 -1.82791170e-01 -4.00188369e-03\n  3.55455309e-01 -4.33122713e-03 -2.73382412e-02 -5.48434679e-02\n  1.48262818e-01  5.09943126e-02 -1.28619883e-01  1.66932807e-02\n  3.79646645e-03  2.10553074e-04 -7.42058354e-02  2.38580903e-02\n  1.97525933e-02]"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 17 - Regression-based forecasting.html#figure-17.8",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 17 - Regression-based forecasting.html#figure-17.8",
    "title": "Chapter 17: Regression-based Forecasting",
    "section": "Figure 17.8",
    "text": "Figure 17.8\n\nresidual = train_df.Ridership - ridership_lm_trendseason.predict(train_df)\ntsaplots.plot_acf(residual)\nplt.xlim(-1, 13)\nplt.show()\nprint(stattools.acf(residual, nlags=40, fft=True))\n\n\n\n\n\n\n\n\n[ 1.          0.60405883  0.44983171  0.3373083   0.25329034  0.21565844\n  0.15901326  0.17676683  0.17553762  0.17140882  0.10739472  0.11976519\n  0.08512942  0.03621337 -0.04100296 -0.10220097 -0.08359282 -0.10871958\n -0.13125876 -0.19609773 -0.18755508 -0.17909437 -0.12700313 -0.06082369\n -0.17881149 -0.16108249 -0.19865283 -0.22465493 -0.19063245 -0.14932752\n -0.17833501 -0.18558963 -0.18364622 -0.18812853 -0.15001921 -0.19938667\n -0.24784616 -0.20621591 -0.19947185 -0.1780327  -0.09340723]"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 17 - Regression-based forecasting.html#table-17.7",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 17 - Regression-based forecasting.html#table-17.7",
    "title": "Chapter 17: Regression-based Forecasting",
    "section": "Table 17.7",
    "text": "Table 17.7\n\nformula = 'Ridership ~ trend + np.square(trend) + C(Month)'\ntrain_lm_trendseason = sm.ols(formula=formula, data=train_df).fit()\n\ntrain_res_arima = ARIMA(train_lm_trendseason.resid, order=(1, 0, 0), freq='MS').fit(trend='nc', disp=0)\nforecast, _, conf_int = train_res_arima.forecast(1)\n\nprint(train_res_arima.summary())\nprint(pd.DataFrame({'coef': train_res_arima.params, 'std err': train_res_arima.bse}))\n\nprint(forecast)\nprint('Forecast {0:.3f} [{1[0][0]:.3f}, {1[0][1]:.3f}]'.format(forecast, conf_int))\n\nprint(train_lm_trendseason.resid[-1])\n\n                              ARMA Model Results                              \n==============================================================================\nDep. Variable:                      y   No. Observations:                  123\nModel:                     ARMA(1, 0)   Log Likelihood                -663.542\nMethod:                       css-mle   S.D. of innovations             53.192\nDate:                Thu, 27 May 2021   AIC                           1331.084\nTime:                        20:03:41   BIC                           1336.709\nSample:                    01-01-1991   HQIC                          1333.369\n                         - 03-01-2001                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nar.L1.y        0.5998      0.071      8.416      0.000       0.460       0.739\n                                    Roots                                    \n=============================================================================\n                  Real          Imaginary           Modulus         Frequency\n-----------------------------------------------------------------------------\nAR.1            1.6673           +0.0000j            1.6673            0.0000\n-----------------------------------------------------------------------------\n             coef   std err\nar.L1.y  0.599781  0.071268\n7.261881856079463\nForecast 7.262 [-96.992, 111.516]\n12.107558346752512\n\n\n/opt/conda/lib/python3.9/site-packages/statsmodels/tsa/arima_model.py:472: FutureWarning: \nstatsmodels.tsa.arima_model.ARMA and statsmodels.tsa.arima_model.ARIMA have\nbeen deprecated in favor of statsmodels.tsa.arima.model.ARIMA (note the .\nbetween arima and model) and\nstatsmodels.tsa.SARIMAX. These will be removed after the 0.12 release.\n\nstatsmodels.tsa.arima.model.ARIMA makes use of the statespace framework and\nis both well tested and maintained.\n\nTo silence this warning and continue using ARMA and ARIMA until they are\nremoved, use:\n\nimport warnings\nwarnings.filterwarnings('ignore', 'statsmodels.tsa.arima_model.ARMA',\n                        FutureWarning)\nwarnings.filterwarnings('ignore', 'statsmodels.tsa.arima_model.ARIMA',\n                        FutureWarning)\n\n  warnings.warn(ARIMA_DEPRECATION_WARN, FutureWarning)\n\n\n\nprint(ridership_lm_trendseason.predict(valid_df).head(), valid_df.head())\n\nDate\n2001-04-01    2004.270893\n2001-05-01    2045.419400\n2001-06-01    2008.675207\n2001-07-01    2128.560114\n2001-08-01    2187.911321\ndtype: float64             Ridership  const  trend  Month\nDate                                      \n2001-04-01   2023.792    1.0  124.0      4\n2001-05-01   2047.008    1.0  125.0      5\n2001-06-01   2072.913    1.0  126.0      6\n2001-07-01   2126.717    1.0  127.0      7\n2001-08-01   2202.638    1.0  128.0      8\n\n\n\n# train_ridership_arima = ARIMA(train_df.Ridership, order=(10, 0, 0), freq='MS').fit()\n# forecast, _, conf_int = train_ridership_arima.forecast(1)\n# print(pd.DataFrame({'coef': train_ridership_arima.params, 'std err': train_ridership_arima.bse}))\n# print('Forecast {0[0]:.3f} [{1[0][0]:.3f}, {1[0][1]:.3f}]'.format(forecast, conf_int))\n\n# # print(train_lm_trendseason.resid[-1])\n# ax = train_df.Ridership.plot(figsize=(9,4))\n# train_ridership_arima.fittedvalues.plot(ax=ax)\n# singleGraphLayout(ax, [1300, 2550], train_df, valid_df)\n# valid_df.Ridership.plot(ax=ax)\n\n# forecast = pd.Series(train_ridership_arima.forecast(len(valid_df))[0], index=valid_df.index)\n# forecast.plot(ax=ax)\n# plt.show()"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 17 - Regression-based forecasting.html#figure-17.9",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 17 - Regression-based forecasting.html#figure-17.9",
    "title": "Chapter 17: Regression-based Forecasting",
    "section": "Figure 17.9",
    "text": "Figure 17.9\n\nax = train_lm_trendseason.resid.plot(figsize=(9,4))\ntrain_res_arima.fittedvalues.plot(ax=ax)\nsingleGraphLayout(ax, [-250, 250], train_df, valid_df)\nplt.show()\n\n\n\n\n\n\n\n\n\ntsaplots.plot_acf(train_res_arima.resid)\nplt.xlim(-1, 13)\nplt.show()"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 17 - Regression-based forecasting.html#table-17.8",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 17 - Regression-based forecasting.html#table-17.8",
    "title": "Chapter 17: Regression-based Forecasting",
    "section": "Table 17.8",
    "text": "Table 17.8\n\nsp500_df = dmba.load_data('SP500.csv')\n# convert date to first of each month\nsp500_df['Date'] = pd.to_datetime(sp500_df.Date, format='%d-%b-%y').dt.to_period('M')\nsp500_ts = pd.Series(sp500_df.Close.values, index=sp500_df.Date, name='sp500')\nsp500_arima = ARIMA(sp500_ts, order=(1, 0, 0)).fit(disp=0)\nprint(pd.DataFrame({'coef': sp500_arima.params, 'std err': sp500_arima.bse}))\n\n                   coef     std err\nconst        888.534652  221.785817\nar.L1.sp500    0.983373    0.014523\n\n\n/opt/conda/lib/python3.9/site-packages/statsmodels/tsa/arima_model.py:472: FutureWarning: \nstatsmodels.tsa.arima_model.ARMA and statsmodels.tsa.arima_model.ARIMA have\nbeen deprecated in favor of statsmodels.tsa.arima.model.ARIMA (note the .\nbetween arima and model) and\nstatsmodels.tsa.SARIMAX. These will be removed after the 0.12 release.\n\nstatsmodels.tsa.arima.model.ARIMA makes use of the statespace framework and\nis both well tested and maintained.\n\nTo silence this warning and continue using ARMA and ARIMA until they are\nremoved, use:\n\nimport warnings\nwarnings.filterwarnings('ignore', 'statsmodels.tsa.arima_model.ARMA',\n                        FutureWarning)\nwarnings.filterwarnings('ignore', 'statsmodels.tsa.arima_model.ARIMA',\n                        FutureWarning)\n\n  warnings.warn(ARIMA_DEPRECATION_WARN, FutureWarning)"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 14 - Association rules.html",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 14 - Association rules.html",
    "title": "Chapter 14: Association Rules and Collaborative Filtering",
    "section": "",
    "text": "2019 Galit Shmueli, Peter C. Bruce, Peter Gedeck\n\nCode included in\nData Mining for Business Analytics: Concepts, Techniques, and Applications in Python (First Edition) Galit Shmueli, Peter C. Bruce, Peter Gedeck, and Nitin R. Patel. 2019."
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 14 - Association rules.html#import-required-packages",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 14 - Association rules.html#import-required-packages",
    "title": "Chapter 14: Association Rules and Collaborative Filtering",
    "section": "Import required packages",
    "text": "Import required packages\n\nfrom pathlib import Path\n\nimport heapq\nfrom collections import defaultdict\n\nimport pandas as pd\nimport matplotlib.pylab as plt\nfrom mlxtend.frequent_patterns import apriori\nfrom mlxtend.frequent_patterns import association_rules\n\nfrom surprise import Dataset, Reader, KNNBasic\nfrom surprise.model_selection import train_test_split\n\nimport dmba\n\n%matplotlib inline\n\nno display found. Using non-interactive Agg backend"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 14 - Association rules.html#table-14.4",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 14 - Association rules.html#table-14.4",
    "title": "Chapter 14: Association Rules and Collaborative Filtering",
    "section": "Table 14.4",
    "text": "Table 14.4\n\n# Load and preprocess data set \nfp_df = dmba.load_data('Faceplate.csv')\nfp_df.set_index('Transaction', inplace=True)\nfp_df\n\n\n\n\n\n\n\n\nRed\nWhite\nBlue\nOrange\nGreen\nYellow\n\n\nTransaction\n\n\n\n\n\n\n\n\n\n\n1\n1\n1\n0\n0\n1\n0\n\n\n2\n0\n1\n0\n1\n0\n0\n\n\n3\n0\n1\n1\n0\n0\n0\n\n\n4\n1\n1\n0\n1\n0\n0\n\n\n5\n1\n0\n1\n0\n0\n0\n\n\n6\n0\n1\n1\n0\n0\n0\n\n\n7\n1\n0\n1\n0\n0\n0\n\n\n8\n1\n1\n1\n0\n1\n0\n\n\n9\n1\n1\n1\n0\n0\n0\n\n\n10\n0\n0\n0\n0\n0\n1\n\n\n\n\n\n\n\n\n# create frequent itemsets\nitemsets = apriori(fp_df, min_support=0.2, use_colnames=True)\n\n# and convert into rules\nrules = association_rules(itemsets, metric='confidence', min_threshold=0.5)\nrules.sort_values(by=['lift'], ascending=False).head(6)\n\nprint(rules.sort_values(by=['lift'], ascending=False)\n      .drop(columns=['antecedent support', 'consequent support', 'conviction'])\n      .head(6))\n\n       antecedents   consequents  support  confidence      lift  leverage\n12    (Red, White)       (Green)      0.2         0.5  2.500000      0.12\n15         (Green)  (Red, White)      0.2         1.0  2.500000      0.12\n4          (Green)         (Red)      0.2         1.0  1.666667      0.08\n14  (Green, White)         (Red)      0.2         1.0  1.666667      0.08\n7         (Orange)       (White)      0.2         1.0  1.428571      0.06\n8          (Green)       (White)      0.2         1.0  1.428571      0.06\n\n\n\n# filter to get rules with single consequents only\nrules[[len(c) == 1 for c in rules.consequents]].sort_values(by=['lift'], ascending=False).head(6)\n\n\n\n\n\n\n\n\nantecedents\nconsequents\nantecedent support\nconsequent support\nsupport\nconfidence\nlift\nleverage\nconviction\n\n\n\n\n12\n(Red, White)\n(Green)\n0.4\n0.2\n0.2\n0.5\n2.500000\n0.12\n1.6\n\n\n4\n(Green)\n(Red)\n0.2\n0.6\n0.2\n1.0\n1.666667\n0.08\ninf\n\n\n14\n(Green, White)\n(Red)\n0.2\n0.6\n0.2\n1.0\n1.666667\n0.08\ninf\n\n\n7\n(Orange)\n(White)\n0.2\n0.7\n0.2\n1.0\n1.428571\n0.06\ninf\n\n\n8\n(Green)\n(White)\n0.2\n0.7\n0.2\n1.0\n1.428571\n0.06\ninf\n\n\n13\n(Red, Green)\n(White)\n0.2\n0.7\n0.2\n1.0\n1.428571\n0.06\ninf\n\n\n\n\n\n\n\nThe apriori method accepts sparse data frames as well. If we convert the original data frame to sparse format, we can see that the memory requirements go down to 40%. The fill_value argument informs the to_sparse method here which fields to ignore in each transaction.\n\n# Convert data set into a sparse data frame\nsparse_df = fp_df.astype(pd.SparseDtype(int, fill_value=0))\nprint('Density {}'.format(sparse_df.sparse.density))\n\n# create frequent itemsets\nitemsets = apriori(sparse_df, min_support=0.2, use_colnames=True)\n\n# and convert into rules\nrules = association_rules(itemsets, metric='confidence', min_threshold=0.5)\nrules.sort_values(by=['lift'], ascending=False).head(6)\n\nDensity 0.4000000000000001\n\n\n\n\n\n\n\n\n\nantecedents\nconsequents\nantecedent support\nconsequent support\nsupport\nconfidence\nlift\nleverage\nconviction\n\n\n\n\n12\n(Red, White)\n(Green)\n0.4\n0.2\n0.2\n0.5\n2.500000\n0.12\n1.6\n\n\n15\n(Green)\n(Red, White)\n0.2\n0.4\n0.2\n1.0\n2.500000\n0.12\ninf\n\n\n4\n(Green)\n(Red)\n0.2\n0.6\n0.2\n1.0\n1.666667\n0.08\ninf\n\n\n14\n(Green, White)\n(Red)\n0.2\n0.6\n0.2\n1.0\n1.666667\n0.08\ninf\n\n\n7\n(Orange)\n(White)\n0.2\n0.7\n0.2\n1.0\n1.428571\n0.06\ninf\n\n\n8\n(Green)\n(White)\n0.2\n0.7\n0.2\n1.0\n1.428571\n0.06\ninf"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 14 - Association rules.html#data-required-for-table-14.5-and-14.6",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 14 - Association rules.html#data-required-for-table-14.5-and-14.6",
    "title": "Chapter 14: Association Rules and Collaborative Filtering",
    "section": "Data required for Table 14.5 and 14.6",
    "text": "Data required for Table 14.5 and 14.6\n\n# Prepare the dataset for table 14.6 based on table 14.5\nfrom itertools import chain\nrandomTransactions = [{8}, {3,4,8}, {8}, {3,9}, {9}, {1,8}, {6,9}, {3,5,7,9}, {8}, set(), \n                      {1,7,9}, {1,4,5,8,9}, {5,7,9}, {6,7,8}, {3,7,9}, {1,4,9}, {6,7,8}, {8}, set(), {9},\n                      {2,5,6,8}, {4,6,9}, {4,9}, {8,9}, {6,8}, {1,6,8}, {5,8}, {4,8,9}, {9}, {8},\n                      {1,5,8}, {3,6,9}, {7,9}, {7,8,9}, {3,4,6,8}, {1,4,8}, {4,7,8}, {8,9}, {4,5,7,9}, {2,8,9},\n                      {2,5,9}, {1,2,7,9}, {5,8}, {1,7,8}, {8}, {2,7,9}, {4,6,9}, {9}, {9}, {6,7,8}]\nprint(randomTransactions)\nuniqueItems = sorted(set(chain.from_iterable(randomTransactions)))\nrandomData = pd.DataFrame(0, index=range(len(randomTransactions)), columns=uniqueItems)\nfor row, transaction in enumerate(randomTransactions):\n    for item in transaction:\n        randomData.loc[row][item] = 1\nrandomData.head()\n\n[{8}, {8, 3, 4}, {8}, {9, 3}, {9}, {8, 1}, {9, 6}, {9, 3, 5, 7}, {8}, set(), {1, 9, 7}, {1, 4, 5, 8, 9}, {9, 5, 7}, {8, 6, 7}, {9, 3, 7}, {1, 4, 9}, {8, 6, 7}, {8}, set(), {9}, {8, 2, 5, 6}, {9, 4, 6}, {9, 4}, {8, 9}, {8, 6}, {8, 1, 6}, {8, 5}, {8, 9, 4}, {9}, {8}, {8, 1, 5}, {9, 3, 6}, {9, 7}, {8, 9, 7}, {8, 3, 4, 6}, {8, 1, 4}, {8, 4, 7}, {8, 9}, {9, 4, 5, 7}, {8, 9, 2}, {9, 2, 5}, {1, 2, 9, 7}, {8, 5}, {8, 1, 7}, {8}, {9, 2, 7}, {9, 4, 6}, {9}, {9}, {8, 6, 7}]\n\n\n\n\n\n\n\n\n\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n1\n0\n0\n1\n1\n0\n0\n0\n1\n0\n\n\n2\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n3\n0\n0\n1\n0\n0\n0\n0\n0\n1\n\n\n4\n0\n0\n0\n0\n0\n0\n0\n0\n1"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 14 - Association rules.html#table-14.6",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 14 - Association rules.html#table-14.6",
    "title": "Chapter 14: Association Rules and Collaborative Filtering",
    "section": "Table 14.6",
    "text": "Table 14.6\n\n# create frequent itemsets\nitemsets = apriori(randomData, min_support=2/len(randomData), use_colnames=True)\n# and convert into rules\nrules = association_rules(itemsets, metric='confidence', min_threshold=0.7)\nprint(rules.sort_values(by=['lift'], ascending=False)\n      .drop(columns=['antecedent support', 'consequent support', 'conviction'])\n      .head(6))\n\n  antecedents consequents  support  confidence      lift  leverage\n3      (8, 3)         (4)     0.04         1.0  4.545455    0.0312\n1      (1, 5)         (8)     0.04         1.0  1.851852    0.0184\n2      (2, 7)         (9)     0.04         1.0  1.851852    0.0184\n4      (3, 4)         (8)     0.04         1.0  1.851852    0.0184\n5      (3, 7)         (9)     0.04         1.0  1.851852    0.0184\n6      (4, 5)         (9)     0.04         1.0  1.851852    0.0184"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 14 - Association rules.html#table-14.8",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 14 - Association rules.html#table-14.8",
    "title": "Chapter 14: Association Rules and Collaborative Filtering",
    "section": "Table 14.8",
    "text": "Table 14.8\n\n# load dataset\nall_books_df = dmba.load_data('CharlesBookClub.csv')\n\n# create the binary incidence matrix\nignore = ['Seq#', 'ID#', 'Gender', 'M', 'R', 'F', 'FirstPurch', 'Related Purchase',\n          'Mcode', 'Rcode', 'Fcode', 'Yes_Florence', 'No_Florence']\ncount_books = all_books_df.drop(columns=ignore)\ncount_books[count_books &gt; 0] = 1\n\ncount_books.head()\n\n\n\n\n\n\n\n\nChildBks\nYouthBks\nCookBks\nDoItYBks\nRefBks\nArtBks\nGeogBks\nItalCook\nItalAtlas\nItalArt\nFlorence\n\n\n\n\n0\n0\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n1\n1\n1\n0\n1\n0\n1\n1\n0\n0\n0\n\n\n3\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n\n\n\n\n# determine item frequencies\nitemFrequency = count_books.sum(axis=0) / len(count_books)\n\n# and plot as histogram\nax = itemFrequency.plot.bar(color='blue')\nplt.ylabel('Item frequency (relative)')\nplt.show()\n\n\n\n\n\n\n\n\n\n# create frequent itemsets and rules\nitemsets = apriori(count_books, min_support=200/4000, use_colnames=True)\nrules = association_rules(itemsets, metric='confidence', min_threshold=0.5)\n\nprint('Number of rules', len(rules))\n\n# Display 25 rules with highest lift\nrules.sort_values(by=['lift'], ascending=False).head(25)\n\npd.set_option('precision', 5)\npd.set_option('display.width', 100)\nprint(rules.sort_values(by=['lift'], ascending=False).drop(columns=['antecedent support', 'consequent support', 'conviction']).head(25))\npd.set_option('precision', 6)\n\nNumber of rules 81\n                      antecedents          consequents  support  confidence     lift  leverage\n64             (YouthBks, RefBks)  (ChildBks, CookBks)  0.05525     0.68000  2.80992   0.03559\n73             (DoItYBks, RefBks)  (ChildBks, CookBks)  0.06125     0.66216  2.73621   0.03886\n60           (DoItYBks, YouthBks)  (ChildBks, CookBks)  0.06700     0.64891  2.68145   0.04201\n80              (GeogBks, RefBks)  (ChildBks, CookBks)  0.05025     0.61468  2.54000   0.03047\n69            (GeogBks, YouthBks)  (ChildBks, CookBks)  0.06325     0.60526  2.50109   0.03796\n77            (GeogBks, DoItYBks)  (ChildBks, CookBks)  0.06050     0.59901  2.47525   0.03606\n66   (GeogBks, ChildBks, CookBks)           (YouthBks)  0.06325     0.57763  2.42445   0.03716\n70    (ChildBks, CookBks, RefBks)           (DoItYBks)  0.06125     0.59179  2.32301   0.03488\n47            (GeogBks, DoItYBks)           (YouthBks)  0.05450     0.53960  2.26486   0.03044\n62    (ChildBks, CookBks, RefBks)           (YouthBks)  0.05525     0.53382  2.24057   0.03059\n58  (ChildBks, CookBks, DoItYBks)           (YouthBks)  0.06700     0.52446  2.20131   0.03656\n56  (ChildBks, YouthBks, CookBks)           (DoItYBks)  0.06700     0.55833  2.19169   0.03643\n33             (ChildBks, RefBks)           (DoItYBks)  0.07100     0.55361  2.17314   0.03833\n74   (GeogBks, ChildBks, CookBks)           (DoItYBks)  0.06050     0.55251  2.16884   0.03260\n19            (GeogBks, ChildBks)           (YouthBks)  0.07550     0.51624  2.16680   0.04066\n46             (GeogBks, CookBks)           (YouthBks)  0.08025     0.51360  2.15572   0.04302\n61   (ChildBks, YouthBks, RefBks)            (CookBks)  0.05525     0.89113  2.14471   0.02949\n16           (ChildBks, YouthBks)           (DoItYBks)  0.08025     0.54407  2.13569   0.04267\n51              (CookBks, RefBks)           (DoItYBks)  0.07450     0.53309  2.09262   0.03890\n28                       (RefBks)  (ChildBks, CookBks)  0.10350     0.50549  2.08882   0.05395\n72    (DoItYBks, CookBks, RefBks)           (ChildBks)  0.06125     0.82215  2.08667   0.03190\n15                     (YouthBks)  (ChildBks, CookBks)  0.12000     0.50367  2.08129   0.06234\n71   (ChildBks, DoItYBks, RefBks)            (CookBks)  0.06125     0.86268  2.07624   0.03175\n22            (ChildBks, CookBks)           (DoItYBks)  0.12775     0.52789  2.07220   0.06610\n25                     (DoItYBks)  (ChildBks, CookBks)  0.12775     0.50147  2.07220   0.06610\n\n\n\n# Filter rules by number of antecedents (maximum 2) and consequents (maximum 1)\nrules = rules[[len(c) &lt;= 2 for c in rules.antecedents]]\nrules = rules[[len(c) == 1 for c in rules.consequents]]\n\nrules.sort_values(by=['lift'], ascending=False).head(10)\n\n\n\n\n\n\n\n\nantecedents\nconsequents\nantecedent support\nconsequent support\nsupport\nconfidence\nlift\nleverage\nconviction\n\n\n\n\n47\n(GeogBks, DoItYBks)\n(YouthBks)\n0.10100\n0.23825\n0.05450\n0.539604\n2.264864\n0.030437\n1.654554\n\n\n33\n(ChildBks, RefBks)\n(DoItYBks)\n0.12825\n0.25475\n0.07100\n0.553606\n2.173135\n0.038328\n1.669490\n\n\n19\n(GeogBks, ChildBks)\n(YouthBks)\n0.14625\n0.23825\n0.07550\n0.516239\n2.166797\n0.040656\n1.574642\n\n\n46\n(GeogBks, CookBks)\n(YouthBks)\n0.15625\n0.23825\n0.08025\n0.513600\n2.155719\n0.043023\n1.566098\n\n\n16\n(ChildBks, YouthBks)\n(DoItYBks)\n0.14750\n0.25475\n0.08025\n0.544068\n2.135693\n0.042674\n1.634563\n\n\n51\n(CookBks, RefBks)\n(DoItYBks)\n0.13975\n0.25475\n0.07450\n0.533095\n2.092619\n0.038899\n1.596148\n\n\n22\n(ChildBks, CookBks)\n(DoItYBks)\n0.24200\n0.25475\n0.12775\n0.527893\n2.072198\n0.066101\n1.578560\n\n\n48\n(GeogBks, YouthBks)\n(DoItYBks)\n0.10450\n0.25475\n0.05450\n0.521531\n2.047227\n0.027879\n1.557573\n\n\n42\n(YouthBks, CookBks)\n(DoItYBks)\n0.16100\n0.25475\n0.08375\n0.520186\n2.041948\n0.042735\n1.553207\n\n\n43\n(YouthBks, RefBks)\n(CookBks)\n0.08125\n0.41550\n0.06825\n0.840000\n2.021661\n0.034491\n3.653125"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 14 - Association rules.html#section-14.2-collaborative-filtering",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 14 - Association rules.html#section-14.2-collaborative-filtering",
    "title": "Chapter 14: Association Rules and Collaborative Filtering",
    "section": "Section 14.2 Collaborative Filtering",
    "text": "Section 14.2 Collaborative Filtering\n\nratings = pd.DataFrame([\n    [30878, 1, 4], [30878, 5, 1], [30878, 18, 3], [30878, 28, 3], [30878, 30, 4], [30878, 44, 5], \n    [124105, 1, 4], \n    [822109, 1, 5], \n    [823519, 1, 3], [823519, 8, 1], [823519, 17, 4], [823519, 28, 4], [823519, 30, 5], \n    [885013, 1, 4], [885013, 5, 5], \n    [893988, 1, 3], [893988, 30, 4], [893988, 44, 4], \n    [1248029, 1, 3], [1248029, 28, 2], [1248029, 30, 4], [1248029, 48, 3], \n    [1503895, 1, 4], \n    [1842128, 1, 4], [1842128, 30, 3], \n    [2238063, 1, 3], \n], columns=['customerID', 'movieID', 'rating'])\n\nreader = Reader(rating_scale=(1, 5))\ndata = Dataset.load_from_df(ratings[['customerID', 'movieID', 'rating']], reader)\ntrainset = data.build_full_trainset()\nsim_options = {'name': 'cosine', 'user_based': False}  # compute cosine similarities between items\nalgo = KNNBasic(sim_options=sim_options)\nalgo.fit(trainset)\npred = algo.predict(str(823519), str(30), r_ui=4, verbose=True)\n\nComputing the cosine similarity matrix...\nDone computing similarity matrix.\nuser: 823519     item: 30         r_ui = 4.00   est = 3.54   {'was_impossible': True, 'reason': 'User and/or item is unknown.'}"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 14 - Association rules.html#table-14.11",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 14 - Association rules.html#table-14.11",
    "title": "Chapter 14: Association Rules and Collaborative Filtering",
    "section": "Table 14.11",
    "text": "Table 14.11\n\nimport random\n\nrandom.seed(0)\nnratings = 5000\nrandomData = pd.DataFrame({\n    'itemID': [random.randint(0,99) for _ in range(nratings)],\n    'userID': [random.randint(0,999) for _ in range(nratings)],\n    'rating': [random.randint(1,5) for _ in range(nratings)],\n})\n\ndef get_top_n(predictions, n=10):\n    # First map the predictions to each user.\n    byUser = defaultdict(list)\n    for p in predictions:\n        byUser[p.uid].append(p)\n    \n    # For each user, reduce predictions to top-n\n    for uid, userPredictions in byUser.items():\n        byUser[uid] = heapq.nlargest(n, userPredictions, key=lambda p: p.est)\n    return byUser\n\n# Convert thes data set into the format required by the surprise package\n# The columns must correspond to user id, item id and ratings (in that order)\nreader = Reader(rating_scale=(1, 5))\ndata = Dataset.load_from_df(randomData[['userID', 'itemID', 'rating']], reader)\n\n# Split into training and test set\ntrainset, testset = train_test_split(data, test_size=.25, random_state=1)\n\n## User-based filtering\n# compute cosine similarity between users \nsim_options = {'name': 'cosine', 'user_based': True}\nalgo = KNNBasic(sim_options=sim_options)\nalgo.fit(trainset)\n\n# Than predict ratings for all pairs (u, i) that are NOT in the training set.\npredictions = algo.test(testset)\n\ntop_n = get_top_n(predictions, n=4)\n\n# Print the recommended items for each user\nprint()\nprint('Top-4 recommended items for each user')\nfor uid, user_ratings in list(top_n.items())[:5]:\n    print('User {}'.format(uid))\n    for prediction in user_ratings:\n        print('  Item {0.iid} ({0.est:.2f})'.format(prediction), end='')\n    print()\nprint()\n\n    \n## Item-based filtering\n# compute cosine similarity between users \nsim_options = {'name': 'cosine', 'user_based': False}\nalgo = KNNBasic(sim_options=sim_options)\nalgo.fit(trainset)\n\n# Than predict ratings for all pairs (u, i) that are NOT in the training set.\npredictions = algo.test(testset)\ntop_n = get_top_n(predictions, n=4)\n\n# Print the recommended items for each user\nprint()\nprint('Top-4 recommended items for each user')\nfor uid, user_ratings in list(top_n.items())[:5]:\n    print('User {}'.format(uid))\n    for prediction in user_ratings:\n        print('  Item {0.iid} ({0.est:.2f})'.format(prediction), end='')\n    print()\n\nComputing the cosine similarity matrix...\nDone computing similarity matrix.\n\nTop-4 recommended items for each user\nUser 6\n  Item 6 (5.00)  Item 77 (2.50)  Item 60 (1.00)\nUser 222\n  Item 77 (3.50)  Item 75 (2.78)\nUser 424\n  Item 14 (3.50)  Item 45 (3.10)  Item 54 (2.34)\nUser 87\n  Item 27 (3.00)  Item 54 (3.00)  Item 82 (3.00)  Item 32 (1.00)\nUser 121\n  Item 98 (3.48)  Item 32 (2.83)\n\nComputing the cosine similarity matrix...\nDone computing similarity matrix.\n\nTop-4 recommended items for each user\nUser 6\n  Item 77 (3.00)  Item 60 (3.00)  Item 6 (3.00)\nUser 222\n  Item 77 (2.24)  Item 75 (2.00)\nUser 424\n  Item 54 (3.47)  Item 14 (3.44)  Item 45 (3.00)\nUser 87\n  Item 27 (3.00)  Item 32 (3.00)  Item 82 (3.00)  Item 54 (2.50)\nUser 121\n  Item 32 (3.06)  Item 98 (2.31)\n\n\n\n## Build a model using the full dataset\ntrainset = data.build_full_trainset()\nsim_options = {'name': 'cosine', 'user_based': False}\nalgo = KNNBasic(sim_options=sim_options)\nalgo.fit(trainset)\n\n# Predict rating for user 383 and item 7\nalgo.predict(383, 7)\n\nComputing the cosine similarity matrix...\nDone computing similarity matrix.\n\n\nPrediction(uid=383, iid=7, r_ui=None, est=2.3661840936304324, details={'actual_k': 4, 'was_impossible': False})"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 12 - Discriminant analysis.html",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 12 - Discriminant analysis.html",
    "title": "Chapter 12: Discriminant Analysis",
    "section": "",
    "text": "2019 Galit Shmueli, Peter C. Bruce, Peter Gedeck\n\nCode included in\nData Mining for Business Analytics: Concepts, Techniques, and Applications in Python (First Edition) Galit Shmueli, Peter C. Bruce, Peter Gedeck, and Nitin R. Patel. 2019."
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 12 - Discriminant analysis.html#import-required-packages",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 12 - Discriminant analysis.html#import-required-packages",
    "title": "Chapter 12: Discriminant Analysis",
    "section": "Import required packages",
    "text": "Import required packages\n\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nimport matplotlib.pylab as plt\n\nimport dmba\nfrom dmba import classificationSummary\n\n%matplotlib inline\n\nno display found. Using non-interactive Agg backend"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 12 - Discriminant analysis.html#table-12.1",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 12 - Discriminant analysis.html#table-12.1",
    "title": "Chapter 12: Discriminant Analysis",
    "section": "Table 12.1",
    "text": "Table 12.1\n\nmower_df = dmba.load_data('RidingMowers.csv')\n\nda_reg = LinearDiscriminantAnalysis()\nda_reg.fit(mower_df.drop(columns=['Ownership']), mower_df['Ownership'])\n\nc_income = da_reg.coef_[0, 0]\nc_lotSize = da_reg.coef_[0, 1]\nintercept = da_reg.intercept_[0]\nprint('Coefficients', da_reg.coef_)\nprint('Intercept', da_reg.intercept_)\n\nprint('Decision function')\nprint(' = {:.2f}*Income + {:.2f}*Lot_Size + {:.2f}'.format(c_income, c_lotSize, intercept))\n\nprint('\\nExample Income=$60K Lot_Size=18.4Kft2')\nprint('  {:.2f}*60 + {:.2f}*18.4 + {:.2f} = {:.2f}'.format(c_income, c_lotSize, intercept,\n                                                          da_reg.decision_function([[60, 18.4]])[0]))\nprint('  negative =&gt; nonowner')\n\nCoefficients [[0.1002303  0.78518471]]\nIntercept [-21.73876167]\nDecision function\n = 0.10*Income + 0.79*Lot_Size + -21.74\n\nExample Income=$60K Lot_Size=18.4Kft2\n  0.10*60 + 0.79*18.4 + -21.74 = -1.28\n  negative =&gt; nonowner\n\n\n\nmower_df\nda_reg.coef_\nda_reg.intercept_\n\narray([-21.73876167])"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 12 - Discriminant analysis.html#figure-12.1-and-figure-12.3",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 12 - Discriminant analysis.html#figure-12.1-and-figure-12.3",
    "title": "Chapter 12: Discriminant Analysis",
    "section": "Figure 12.1 and Figure 12.3",
    "text": "Figure 12.1 and Figure 12.3\n\nax = mower_df[mower_df.Ownership=='Owner'].plot.scatter(x='Income', y='Lot_Size', c='blue', label='owner')\nmower_df[mower_df.Ownership!='Owner'].plot.scatter(x='Income', y='Lot_Size', c='red', label='nonowner', ax=ax)\n\nax.plot(da_reg.means_[:,0], da_reg.means_[:,1], 'x', color='black', label='Class means', markersize=10)\nx_adhoc = np.array([48, 85])\ny_adhoc = np.array([24, 14])\nax.plot(x_adhoc, y_adhoc, '-', color='C0', label='ad hoc line')\nax.set_xlabel('Income ($000s)')\nax.set_ylabel('Lot Size (000s sqft)')\nax.set_xlim(20, 120)\nax.set_ylim(13, 25)\n\nbox = ax.get_position()\nax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\nplt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n\nplt.show()\n\n\n\n\n\n\n\n\n\nax = mower_df[mower_df.Ownership=='Owner'].plot.scatter(x='Income', y='Lot_Size', c='blue', label='owner')\nmower_df[mower_df.Ownership!='Owner'].plot.scatter(x='Income', y='Lot_Size', c='red', label='nonowner', ax=ax)\n\nax.plot(da_reg.means_[:,0], da_reg.means_[:,1], 'x', color='black', label='Class means', markersize=5)\nx_lda = np.array(ax.get_xlim())\ny_lda = (- intercept - c_income * x_lda) / c_lotSize\nax.plot(x_lda, y_lda, '--', color='C1', label='DA line')\nax.plot(x_adhoc, y_adhoc, '-', color='C0', label='ad hoc line')\nax.set_xlabel('Income ($000s)')\nax.set_ylabel('Lot Size (000s sqft)')\nax.set_xlim(20, 120)\nax.set_ylim(13, 25)\n\nbox = ax.get_position()\nax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\nplt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n\nplt.show()"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 12 - Discriminant analysis.html#table-12.2",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 12 - Discriminant analysis.html#table-12.2",
    "title": "Chapter 12: Discriminant Analysis",
    "section": "Table 12.2",
    "text": "Table 12.2\n\nda_reg = LinearDiscriminantAnalysis()\nda_reg.fit(mower_df.drop(columns=['Ownership']), mower_df['Ownership'])\n\nresult_df = mower_df.copy()\nresult_df.index = result_df.index + 1\nresult_df['Dec. Function'] = da_reg.decision_function(mower_df.drop(columns=['Ownership']))\nresult_df['Prediction'] = da_reg.predict(mower_df.drop(columns=['Ownership']))\nresult_df['p(Owner)'] = da_reg.predict_proba(mower_df.drop(columns=['Ownership']))[:, 1]\n\nprint(result_df)\n\n    Income  Lot_Size Ownership  Dec. Function Prediction  p(Owner)\n1     60.0      18.4     Owner      -1.277545   Nonowner  0.217968\n2     85.5      16.8     Owner       0.022032      Owner  0.505508\n3     64.8      21.6     Owner       1.716152      Owner  0.847632\n4     61.5      20.8     Owner       0.757244      Owner  0.680755\n5     87.0      23.6     Owner       5.511634      Owner  0.995977\n6    110.1      19.2     Owner       4.372141      Owner  0.987533\n7    108.0      17.6     Owner       2.905362      Owner  0.948111\n8     82.8      22.4     Owner       4.148445      Owner  0.984456\n9     69.0      20.0     Owner       0.880823      Owner  0.706993\n10    93.0      20.8     Owner       3.914499      Owner  0.980440\n11    51.0      22.0     Owner       0.647047      Owner  0.656345\n12    81.0      20.0     Owner       2.083587      Owner  0.889298\n13    75.0      19.6  Nonowner       1.168131      Owner  0.762807\n14    52.8      20.8  Nonowner      -0.114760   Nonowner  0.471342\n15    64.8      17.2  Nonowner      -1.738661   Nonowner  0.149483\n16    43.2      20.4  Nonowner      -1.391044   Nonowner  0.199241\n17    84.0      17.6  Nonowner       0.499835      Owner  0.622420\n18    49.2      17.6  Nonowner      -2.988180   Nonowner  0.047963\n19    59.4      16.0  Nonowner      -3.222126   Nonowner  0.038342\n20    66.0      18.4  Nonowner      -0.676163   Nonowner  0.337118\n21    47.4      16.4  Nonowner      -4.110816   Nonowner  0.016130\n22    33.0      18.8  Nonowner      -3.669689   Nonowner  0.024851\n23    51.0      14.0  Nonowner      -5.634430   Nonowner  0.003560\n24    63.0      14.8  Nonowner      -3.803519   Nonowner  0.021806"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 12 - Discriminant analysis.html#table-12.3",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 12 - Discriminant analysis.html#table-12.3",
    "title": "Chapter 12: Discriminant Analysis",
    "section": "Table 12.3",
    "text": "Table 12.3\n\naccidents_df = dmba.load_data('accidents.csv')\nlda_reg = LinearDiscriminantAnalysis()\nlda_reg.fit(accidents_df.drop(columns=['MAX_SEV']), accidents_df['MAX_SEV'])\n\nprint('Coefficients and intercept')\nfct = pd.DataFrame([lda_reg.intercept_], columns=lda_reg.classes_, index=['constant'])\nfct = fct.append(pd.DataFrame(lda_reg.coef_.transpose(), columns=lda_reg.classes_, \n                              index=list(accidents_df.columns)[:-1]))\nprint(fct)\nprint()\nclassificationSummary(accidents_df['MAX_SEV'], \n                      lda_reg.predict(accidents_df.drop(columns=['MAX_SEV'])),\n                      class_names=lda_reg.classes_)\naccidents_df.MAX_SEV.value_counts()\n\nCoefficients and intercept\n                    fatal  no-injury  non-fatal\nconstant        -1.972659  -0.891172  -0.610471\nRushHour        -0.996411   0.033430  -0.015774\nWRK_ZONE        -0.457188   0.220012  -0.204480\nWKDY            -1.471777   0.165707  -0.135404\nINT_HWY          0.755344  -0.075816   0.060599\nLGTCON_day       0.009515  -0.031421   0.030124\nLEVEL            0.976626  -0.082717   0.063598\nSPD_LIM          0.048033   0.004381  -0.005014\nSUR_COND_dry    -5.999809  -0.164874   0.257895\nTRAF_two_way     0.752985  -0.012844  -0.000048\nWEATHER_adverse -6.596690   0.079166   0.032564\n\nConfusion Matrix (Accuracy 0.5283)\n\n          Prediction\n   Actual     fatal no-injury non-fatal\n    fatal         1         1         3\nno-injury         6       114       172\nnon-fatal         6        95       202\n\n\nnon-fatal    303\nno-injury    292\nfatal          5\nName: MAX_SEV, dtype: int64\n\n\n\nlda_reg.intercept_\n\narray([-1.9726587 , -0.89117217, -0.61047129])"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 12 - Discriminant analysis.html#table-12.4",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 12 - Discriminant analysis.html#table-12.4",
    "title": "Chapter 12: Discriminant Analysis",
    "section": "Table 12.4",
    "text": "Table 12.4\n\nresult = pd.concat([\n    pd.DataFrame({'Classification': lda_reg.predict(accidents_df.drop(columns=['MAX_SEV'])),\n                  'Actual': accidents_df['MAX_SEV']}),\n    pd.DataFrame(lda_reg.decision_function(accidents_df.drop(columns=['MAX_SEV'])), \n                 columns=['Score {}'.format(cls) for cls in lda_reg.classes_]),\n    pd.DataFrame(lda_reg.predict_proba(accidents_df.drop(columns=['MAX_SEV'])), \n                 columns=['Propensity {}'.format(cls) for cls in lda_reg.classes_])\n], axis=1)\n\npd.set_option('precision',2)\npd.set_option('chop_threshold', .01)\nprint(result.head())\n\n  Classification     Actual  Score fatal  Score no-injury  Score non-fatal  \\\n0      no-injury  no-injury        -5.94            -0.46            -0.96   \n1      no-injury  non-fatal        -1.05            -0.46            -1.04   \n2      no-injury  no-injury        -7.88            -0.63            -0.77   \n3      no-injury  no-injury        -8.38            -0.54            -0.84   \n4      no-injury  non-fatal        -9.84            -0.50            -0.85   \n\n   Propensity fatal  Propensity no-injury  Propensity non-fatal  \n0          0.00e+00                  0.62                  0.38  \n1          2.63e-01                  0.47                  0.27  \n2          0.00e+00                  0.54                  0.46  \n3          0.00e+00                  0.57                  0.43  \n4          0.00e+00                  0.59                  0.41"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 12 - Discriminant analysis.html#prior-probabilities",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 12 - Discriminant analysis.html#prior-probabilities",
    "title": "Chapter 12: Discriminant Analysis",
    "section": "12.5 Prior probabilities",
    "text": "12.5 Prior probabilities\n\nda_reg_prior = LinearDiscriminantAnalysis(priors=[0.85,0.15])\nda_reg_prior.fit(mower_df.drop(columns=['Ownership']), mower_df['Ownership'])\n\nc_income = da_reg_prior.coef_[0, 0]\nc_lotSize = da_reg_prior.coef_[0, 1]\nintercept = da_reg_prior.intercept_[0]\n\nprint('Decision function')\nprint(' = {:.2f}*Income + {:.2f}*Lot_Size + {:.2f}'.format(c_income, c_lotSize, intercept))\n\n\ndata = [[75, 19.6]]\nprint('\\nExample Income=$75K Lot_Size=19.6Kft2')\nprint('  {} (no-priors) =&gt; {} (priors)'.format(da_reg.predict(data)[0], da_reg_prior.predict(data)[0]))\n\ndata = [[84, 17.6]]\nprint('\\nExample Income=$84K Lot_Size=17.6Kft2')\nprint('  {} (no-priors) =&gt; {} (priors)'.format(da_reg.predict(data)[0], da_reg_prior.predict(data)[0]))\n\nDecision function\n = 0.10*Income + 0.79*Lot_Size + -23.47\n\nExample Income=$75K Lot_Size=19.6Kft2\n  Owner (no-priors) =&gt; Nonowner (priors)\n\nExample Income=$84K Lot_Size=17.6Kft2\n  Owner (no-priors) =&gt; Nonowner (priors)\n\n\n\nax = mower_df[mower_df.Ownership=='Owner'].plot.scatter(x='Income', y='Lot_Size', c='blue', label='owner')\nmower_df[mower_df.Ownership!='Owner'].plot.scatter(x='Income', y='Lot_Size', c='red', label='nonowner', ax=ax)\n\nax.plot(da_reg.means_[:,0], da_reg.means_[:,1], 'x', color='black', label='Class means', markersize=5)\nx_lda_prior = np.array(ax.get_xlim())\ny_lda_prior = (- intercept - c_income * x_lda) / c_lotSize\nax.plot(x_lda, y_lda, '--', label='DA line')\nax.plot(x_lda_prior, y_lda_prior, '--', label='DA line (priors)')\nax.set_xlabel('Income ($000s)')\nax.set_ylabel('Lot Size (000s sqft)')\nax.set_xlim(20, 120)\nax.set_ylim(13, 25)\n\nbox = ax.get_position()\nax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\nplt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n\nplt.show()"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 12 - Discriminant analysis.html#figure-12.2",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 12 - Discriminant analysis.html#figure-12.2",
    "title": "Chapter 12: Discriminant Analysis",
    "section": "Figure 12.2",
    "text": "Figure 12.2\n\nfrom matplotlib.lines import Line2D\nfrom matplotlib.ticker import ScalarFormatter, FormatStrFormatter\n\nuniversal_df = dmba.load_data('UniversalBank.csv')\n\nfig, axes = plt.subplots(nrows=2, ncols=1, figsize=[5, 10])\nplt.subplots_adjust(hspace=0.25)\n# plt.tight_layout()\n\ndef createGraph(df, title, ax):\n    ccIdx = df[df['Personal Loan'] == 0].index\n    df.loc[ccIdx].plot(x='Income', y='CCAvg', kind='scatter',\n                              label='nonacceptor', color='C1',\n                              ylim = (0.05, 20), xlim = (5, 300),\n                              logx=True, logy=True, ax=ax)\n    df.drop(ccIdx).plot(x='Income', y='CCAvg', kind='scatter',\n                              label='acceptor', color='C0',\n                              ylim = (0.05, 20), xlim = (5, 300),\n                              logx=True, logy=True, ax=ax)\n    ax.set_xlabel('Annual Income ($000s)')\n    ax.set_ylabel('Monthly Credit Card Average Spending ($000s)')\n    ax.xaxis.set_major_formatter(ScalarFormatter())\n    ax.yaxis.set_major_formatter(FormatStrFormatter('%.1f'))\n    ax.set_title(title)\ncreateGraph(universal_df.sample(200, random_state=10), 'Sample of 200 customers', axes[0])\ncreateGraph(universal_df, 'All 5000 customers', axes[1])\nplt.show()\n\n\n\n\n\n\n\n\n\nuniversal_df.head()\n\n\n\n\n\n\n\n\nID\nAge\nExperience\nIncome\nZIP Code\nFamily\nCCAvg\nEducation\nMortgage\nPersonal Loan\nSecurities Account\nCD Account\nOnline\nCreditCard\n\n\n\n\n0\n1\n25\n1\n49\n91107\n4\n1.6\n1\n0\n0\n1\n0\n0\n0\n\n\n1\n2\n45\n19\n34\n90089\n3\n1.5\n1\n0\n0\n1\n0\n0\n0\n\n\n2\n3\n39\n15\n11\n94720\n1\n1.0\n1\n0\n0\n0\n0\n0\n0\n\n\n3\n4\n35\n9\n100\n94112\n1\n2.7\n2\n0\n0\n0\n0\n0\n0\n\n\n4\n5\n35\n8\n45\n91330\n4\n1.0\n2\n0\n0\n0\n0\n0\n1\n\n\n\n\n\n\n\n\nuniversal_df.shape\n\n(5000, 14)"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 09 - Classification and regression trees.html",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 09 - Classification and regression trees.html",
    "title": "Chapter 9: Classification and Regression Trees",
    "section": "",
    "text": "2019 Galit Shmueli, Peter C. Bruce, Peter Gedeck\n\nCode included in\nData Mining for Business Analytics: Concepts, Techniques, and Applications in Python (First Edition) Galit Shmueli, Peter C. Bruce, Peter Gedeck, and Nitin R. Patel. 2019."
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 09 - Classification and regression trees.html#import-required-packages",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 09 - Classification and regression trees.html#import-required-packages",
    "title": "Chapter 9: Classification and Regression Trees",
    "section": "Import required packages",
    "text": "Import required packages\n\n%matplotlib inline\n\nfrom pathlib import Path\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nimport matplotlib.pylab as plt\nfrom dmba import plotDecisionTree, classificationSummary, regressionSummary\n\nno display found. Using non-interactive Agg backend"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 09 - Classification and regression trees.html#figure-9.7",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 09 - Classification and regression trees.html#figure-9.7",
    "title": "Chapter 9: Classification and Regression Trees",
    "section": "Figure 9.7",
    "text": "Figure 9.7\n\nmower_df = pd.read_csv('RidingMowers.csv')\n\nclassTree = DecisionTreeClassifier(random_state=0, max_depth=1)\nclassTree.fit(mower_df.drop(columns=['Ownership']), mower_df['Ownership'])\n\nprint(\"Classes: {}\".format(', '.join(classTree.classes_)))\nplotDecisionTree(classTree, feature_names=mower_df.columns[:2], class_names=classTree.classes_)\n\nClasses: Nonowner, Owner\n\n\n\n\n\n\n\n\n\nThe order of the values vector in the boxes is the same as classTree.classes_."
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 09 - Classification and regression trees.html#figure-9.8",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 09 - Classification and regression trees.html#figure-9.8",
    "title": "Chapter 9: Classification and Regression Trees",
    "section": "Figure 9.8",
    "text": "Figure 9.8\nGrow tree fully\n\nclassTree = DecisionTreeClassifier(random_state=0)\nclassTree.fit(mower_df.drop(columns=['Ownership']), mower_df['Ownership'])\n\nprint(\"Classes: {}\".format(', '.join(classTree.classes_)))\nplotDecisionTree(classTree, feature_names=mower_df.columns[:2], class_names=classTree.classes_)\n\nClasses: Nonowner, Owner"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 09 - Classification and regression trees.html#figure-9.10",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 09 - Classification and regression trees.html#figure-9.10",
    "title": "Chapter 9: Classification and Regression Trees",
    "section": "Figure 9.10",
    "text": "Figure 9.10\n\nbank_df = pd.read_csv('UniversalBank.csv')\nbank_df = bank_df.drop(columns=['ID', 'ZIP Code'])\n\nX = bank_df.drop(columns=['Personal Loan'])\ny = bank_df['Personal Loan']\ntrain_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size=0.4, random_state=1)\n\nfullClassTree = DecisionTreeClassifier()\nfullClassTree.fit(train_X, train_y)\n\nplotDecisionTree(fullClassTree, feature_names=train_X.columns)"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 09 - Classification and regression trees.html#table-9.3",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 09 - Classification and regression trees.html#table-9.3",
    "title": "Chapter 9: Classification and Regression Trees",
    "section": "Table 9.3",
    "text": "Table 9.3\n\n\nclassificationSummary(train_y, fullClassTree.predict(train_X))\nclassificationSummary(valid_y, fullClassTree.predict(valid_X))\n\nConfusion Matrix (Accuracy 1.0000)\n\n       Prediction\nActual    0    1\n     0 2713    0\n     1    0  287\nConfusion Matrix (Accuracy 0.9785)\n\n       Prediction\nActual    0    1\n     0 1792   15\n     1   28  165"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 09 - Classification and regression trees.html#table-9.4",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 09 - Classification and regression trees.html#table-9.4",
    "title": "Chapter 9: Classification and Regression Trees",
    "section": "Table 9.4",
    "text": "Table 9.4\n\n# Five-fold cross-validation of the full decision tree classifier\ntreeClassifier = DecisionTreeClassifier()\n\nscores = cross_val_score(treeClassifier, train_X, train_y, cv=5)\nprint('Accuracy scores of each fold: ', [f'{acc:.3f}' for acc in scores])\nprint(f'Accuracy: {scores.mean():.3f} (+/- {scores.std() * 2:.3f})')\nprint(f'Accuracy: {scores.mean():.3f} (+/- {scores.std():.3f})')\n\nAccuracy scores of each fold:  ['0.988', '0.973', '0.995', '0.987', '0.993']\nAccuracy: 0.987 (+/- 0.015)\nAccuracy: 0.987 (+/- 0.008)"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 09 - Classification and regression trees.html#figure-9.12",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 09 - Classification and regression trees.html#figure-9.12",
    "title": "Chapter 9: Classification and Regression Trees",
    "section": "Figure 9.12",
    "text": "Figure 9.12\n\nsmallClassTree = DecisionTreeClassifier(max_depth=30, min_samples_split=20, min_impurity_decrease=0.01)\nsmallClassTree.fit(train_X, train_y)\n\nplotDecisionTree(smallClassTree, feature_names=train_X.columns)"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 09 - Classification and regression trees.html#table-9.5",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 09 - Classification and regression trees.html#table-9.5",
    "title": "Chapter 9: Classification and Regression Trees",
    "section": "Table 9.5",
    "text": "Table 9.5\n\nclassificationSummary(train_y, smallClassTree.predict(train_X))\nclassificationSummary(valid_y, smallClassTree.predict(valid_X))\n\nConfusion Matrix (Accuracy 0.9823)\n\n       Prediction\nActual    0    1\n     0 2711    2\n     1   51  236\nConfusion Matrix (Accuracy 0.9770)\n\n       Prediction\nActual    0    1\n     0 1804    3\n     1   43  150"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 09 - Classification and regression trees.html#grid-search",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 09 - Classification and regression trees.html#grid-search",
    "title": "Chapter 9: Classification and Regression Trees",
    "section": "Grid search",
    "text": "Grid search"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 09 - Classification and regression trees.html#table-9.6",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 09 - Classification and regression trees.html#table-9.6",
    "title": "Chapter 9: Classification and Regression Trees",
    "section": "Table 9.6",
    "text": "Table 9.6\n\n# Start with an initial guess for parameters\nparam_grid = {\n    'max_depth': [10, 20, 30, 40], \n    'min_samples_split': [20, 40, 60, 80, 100], \n    'min_impurity_decrease': [0, 0.0005, 0.001, 0.005, 0.01], \n}\ngridSearch = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5, n_jobs=-1)\ngridSearch.fit(train_X, train_y)\nprint('Initial score: ', gridSearch.best_score_)\nprint('Initial parameters: ', gridSearch.best_params_)\n\n# Adapt grid based on result from initial grid search\nparam_grid = {\n    'max_depth': list(range(2, 16)), \n    'min_samples_split': list(range(10, 22)), \n    'min_impurity_decrease': [0.0009, 0.001, 0.0011], \n}\ngridSearch = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5, n_jobs=-1)\ngridSearch.fit(train_X, train_y)\nprint('Improved score: ', gridSearch.best_score_)\nprint('Improved parameters: ', gridSearch.best_params_)\n\nbestClassTree = gridSearch.best_estimator_\n\nInitial score:  0.9876666666666667\nInitial parameters:  {'max_depth': 20, 'min_impurity_decrease': 0, 'min_samples_split': 20}\nImproved score:  0.9873333333333333\nImproved parameters:  {'max_depth': 4, 'min_impurity_decrease': 0.0011, 'min_samples_split': 13}\n\n\n\nplotDecisionTree(bestClassTree, feature_names=train_X.columns)\n\n\n\n\n\n\n\n\n\nclassificationSummary(train_y, bestClassTree.predict(train_X))\nclassificationSummary(valid_y, bestClassTree.predict(valid_X))\n\nConfusion Matrix (Accuracy 0.9867)\n\n       Prediction\nActual    0    1\n     0 2708    5\n     1   35  252\nConfusion Matrix (Accuracy 0.9815)\n\n       Prediction\nActual    0    1\n     0 1801    6\n     1   31  162"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 09 - Classification and regression trees.html#figure-9.16",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 09 - Classification and regression trees.html#figure-9.16",
    "title": "Chapter 9: Classification and Regression Trees",
    "section": "Figure 9.16",
    "text": "Figure 9.16\n\nfakeScatter = pd.DataFrame([\n    (30, 19, 1), (35, 5, 1), (40, 25, 1), (45, 13, 1), \n    (38, 10, 1), (36, 23, 1), (48, 8, 1), (44, 20, 1), (50, 12),\n    (51, 19, 0), (62, 30, 0), (42, 29, 0), (45, 25, 0), \n    (55, 10, 0), (52, 27, 0), (58, 6, 0), (53, 11, 0), \n], columns=['x', 'y', 'category'])\nax = fakeScatter.loc[fakeScatter.category==1,:].plot.scatter(x='x', y='y', color='C0')\nfakeScatter.loc[fakeScatter.category==0,:].plot.scatter(x='x', y='y', ax=ax, color='C1')\nplt.plot((40, 54), (30, 5), ':', color='grey')\nax.set_xlabel('Income ($000s)')\nax.set_ylabel('Education')\n\nplt.tight_layout()\nplt.show()\n\n\ndef basePlot(ax):\n    mower_df.loc[mower_df.Ownership=='Owner'].plot(x='Income', y='Lot_Size', style='o', \n                                                   markerfacecolor='C0', markeredgecolor='C0',\n                                                   ax=ax)\n    mower_df.loc[mower_df.Ownership=='Nonowner'].plot(x='Income', y='Lot_Size', style='o',\n                                                      markerfacecolor='none', markeredgecolor='C1',\n                                                      ax=ax)\n    ax.legend([\"Owner\", \"Nonowner\"]);\n    ax.set_xlim(20, 120)\n    ax.set_ylim(13, 25)\n    ax.set_xlabel('Income ($000s)')\n    ax.set_ylabel('Lot Size (000s sqft)')\n    return ax\n\nfig, ax = plt.subplots(figsize=(7, 4))\n\nax = basePlot(ax)\nplt.tight_layout()\nplt.show()\n\n\nfig, ax = plt.subplots(figsize=(7, 4))\n\nax = basePlot(ax)\nx0 = 59.7\nax.plot((x0, x0), (25, 13), color='grey')\nplt.tight_layout()\nplt.show()\n\n\nclassTree = DecisionTreeClassifier(random_state=0, max_depth=1)\nclassTree.fit(mower_df.drop(columns=['Ownership']), mower_df['Ownership'])\nplotDecisionTree(classTree, feature_names=mower_df.columns[:2], class_names=classTree.classes_)\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(7, 4))\n\nax = basePlot(ax)\nx0 = 59.7\ny1 = 21.4\nax.plot((x0, x0), (25, 13), color='grey')\nax.plot((20, x0), (y1, y1), color='grey')\nplt.tight_layout()\nplt.show()\n\n\nfig, ax = plt.subplots(figsize=(7, 4))\n\nax = basePlot(ax)\nx0 = 59.7\ny1 = 21.4\ny2 = 19.8\nax.plot((x0, x0), (25, 13), color='grey')\nax.plot((20, x0), (y1, y1), color='grey')\nax.plot((x0, 120), (y2, y2), color='grey')\nplt.tight_layout()\nplt.show()\n\n\nclassTree = DecisionTreeClassifier(random_state=0, max_depth=2)\nclassTree.fit(mower_df.drop(columns=['Ownership']), mower_df['Ownership'])\nplotDecisionTree(classTree, feature_names=mower_df.columns[:2], class_names=classTree.classes_)\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(7, 4))\n\nax = basePlot(ax)\nx0 = 59.7\ny1 = 21.4\ny2 = 19.8\nx3 = 84.75\nx4 = 61.5\nax.plot((x0, x0), (25, 13), color='grey')\nax.plot((20, x0), (y1, y1), color='grey')\nax.plot((x0, 120), (y2, y2), color='grey')\nax.plot((x3, x3), (13, y2), color='grey')\nax.plot((x4, x4), (13, y2), color='grey')\nplt.tight_layout()\nplt.show()\n\n\nclassTree = DecisionTreeClassifier(random_state=0)\nclassTree.fit(mower_df.drop(columns=['Ownership']), mower_df['Ownership'])\nplotDecisionTree(classTree, feature_names=mower_df.columns[:2], class_names=classTree.classes_)"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 09 - Classification and regression trees.html#information-about-full-decision-tree",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 09 - Classification and regression trees.html#information-about-full-decision-tree",
    "title": "Chapter 9: Classification and Regression Trees",
    "section": "Information about full decision tree",
    "text": "Information about full decision tree\n\ntree = fullClassTree\nprint('Number of nodes', tree.tree_.node_count)\n\nNumber of nodes 85\n\n\n\n\n\n\n\n\n\n\nestimator = tree\n# Using those arrays, we can parse the tree structure:\n\nn_nodes = estimator.tree_.node_count\nchildren_left = estimator.tree_.children_left\nchildren_right = estimator.tree_.children_right\nfeature = estimator.tree_.feature\nthreshold = estimator.tree_.threshold\nvalue = estimator.tree_.value\n\n\n# The tree structure can be traversed to compute various properties such\n# as the depth of each node and whether or not it is a leaf.\nnode_depth = np.zeros(shape=n_nodes, dtype=np.int64)\nis_leaves = np.zeros(shape=n_nodes, dtype=bool)\nstack = [(0, -1)]  # seed is the root node id and its parent depth\nwhile len(stack) &gt; 0:\n    node_id, parent_depth = stack.pop()\n    node_depth[node_id] = parent_depth + 1\n\n    # If we have a test node\n    if (children_left[node_id] != children_right[node_id]):\n        stack.append((children_left[node_id], parent_depth + 1))\n        stack.append((children_right[node_id], parent_depth + 1))\n    else:\n        is_leaves[node_id] = True\n\nfrom collections import Counter\nnodeClassCounter = Counter()\nterminal_leaves = 0\nfor i in range(n_nodes):\n    if is_leaves[i]:\n        terminal_leaves = terminal_leaves + 1\n        nodeClassCounter.update([np.argmax(value[i][0])])\nprint()\nprint('Number of terminal leaves', terminal_leaves)\nprint(nodeClassCounter)\n\n\nNumber of terminal leaves 43\nCounter({0: 24, 1: 19})"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 09 - Classification and regression trees.html#regression-trees-table-9.7-and-figure-9.14",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 09 - Classification and regression trees.html#regression-trees-table-9.7-and-figure-9.14",
    "title": "Chapter 9: Classification and Regression Trees",
    "section": "Regression Trees : Table 9.7 and Figure 9.14",
    "text": "Regression Trees : Table 9.7 and Figure 9.14\n\ntoyotaCorolla_df = pd.read_csv('ToyotaCorolla.csv').iloc[:1000,:]\ntoyotaCorolla_df = toyotaCorolla_df.rename(columns={'Age_08_04': 'Age', 'Quarterly_Tax': 'Tax'})\n\npredictors = ['Age', 'KM', 'Fuel_Type', 'HP', 'Met_Color', 'Automatic', 'CC', \n              'Doors', 'Tax', 'Weight']\noutcome = 'Price'\n\nX = pd.get_dummies(toyotaCorolla_df[predictors], drop_first=True)\ny = toyotaCorolla_df[outcome]\n\ntrain_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size=0.4, random_state=1)\n\n# user grid search to find optimized tree\nparam_grid = {\n    'max_depth': [5, 10, 15, 20, 25], \n    'min_impurity_decrease': [0, 0.001, 0.005, 0.01], \n    'min_samples_split': [10, 20, 30, 40, 50], \n}\ngridSearch = GridSearchCV(DecisionTreeRegressor(), param_grid, cv=5, n_jobs=-1)\ngridSearch.fit(train_X, train_y)\nprint('Initial parameters: ', gridSearch.best_params_)\n\nparam_grid = {\n    'max_depth': [3, 4, 5, 6, 7, 8, 9, 10, 11, 12], \n    'min_impurity_decrease': [0, 0.001, 0.002, 0.003, 0.005, 0.006, 0.007, 0.008], \n    'min_samples_split': [14, 15, 16, 18, 20, ], \n}\ngridSearch = GridSearchCV(DecisionTreeRegressor(), param_grid, cv=5, n_jobs=-1)\ngridSearch.fit(train_X, train_y)\nprint('Improved parameters: ', gridSearch.best_params_)\n\nregTree = gridSearch.best_estimator_\n\nInitial parameters:  {'max_depth': 5, 'min_impurity_decrease': 0, 'min_samples_split': 20}\nImproved parameters:  {'max_depth': 6, 'min_impurity_decrease': 0, 'min_samples_split': 16}\n\n\n\nregressionSummary(train_y, regTree.predict(train_X))\nregressionSummary(valid_y, regTree.predict(valid_X))\n\n\nRegression statistics\n\n                      Mean Error (ME) : 0.0000\n       Root Mean Squared Error (RMSE) : 1058.8202\n            Mean Absolute Error (MAE) : 767.7203\n          Mean Percentage Error (MPE) : -0.8074\nMean Absolute Percentage Error (MAPE) : 6.8325\n\nRegression statistics\n\n                      Mean Error (ME) : 60.5241\n       Root Mean Squared Error (RMSE) : 1554.9146\n            Mean Absolute Error (MAE) : 1026.3487\n          Mean Percentage Error (MPE) : -1.3082\nMean Absolute Percentage Error (MAPE) : 9.2311\n\n\n\nplotDecisionTree(regTree, feature_names=train_X.columns)\nplotDecisionTree(regTree, feature_names=train_X.columns, rotate=True)"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 09 - Classification and regression trees.html#figure-9.15-no-longer-used",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 09 - Classification and regression trees.html#figure-9.15-no-longer-used",
    "title": "Chapter 9: Classification and Regression Trees",
    "section": "Figure 9.15 (no longer used)",
    "text": "Figure 9.15 (no longer used)\n\nbank_df = pd.read_csv('UniversalBank.csv')\nbank_df = bank_df.drop(columns=['ID', 'ZIP Code'])\n\nX = bank_df.drop(columns=['Personal Loan'])\ny = bank_df['Personal Loan']\ntrain_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size=0.4, random_state=1)\n\nrf = RandomForestClassifier(n_estimators=500, random_state=1)\nrf.fit(train_X, train_y)\n\nRandomForestClassifier(n_estimators=500, random_state=1)\n\n\nVariable importance plot\n\nimportances = rf.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in rf.estimators_], axis=0)\n\ndf = pd.DataFrame({'feature': train_X.columns, 'importance': importances, 'std': std})\ndf = df.sort_values('importance')\nprint(df)\n\nax = df.plot(kind='barh', xerr='std', x='feature', legend=False)\nax.set_ylabel('')\n\nplt.tight_layout()\nplt.show()\n\n               feature  importance       std\n7   Securities Account    0.003964  0.004998\n9               Online    0.006394  0.005350\n10          CreditCard    0.007678  0.007053\n6             Mortgage    0.034243  0.023469\n1           Experience    0.035539  0.016061\n0                  Age    0.036258  0.015858\n8           CD Account    0.057917  0.043185\n3               Family    0.111375  0.053146\n4                CCAvg    0.172105  0.103011\n5            Education    0.200772  0.101002\n2               Income    0.333756  0.129227\n\n\nConfusion matrix and metrics\n\nclassificationSummary(valid_y, rf.predict(valid_X))\n\nConfusion Matrix (Accuracy 0.9820)\n\n       Prediction\nActual    0    1\n     0 1803    4\n     1   32  161"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 09 - Classification and regression trees.html#table-9.5-1",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 09 - Classification and regression trees.html#table-9.5-1",
    "title": "Chapter 9: Classification and Regression Trees",
    "section": "Table 9.5",
    "text": "Table 9.5\n\nboost = GradientBoostingClassifier()\nboost.fit(train_X, train_y)\nclassificationSummary(valid_y, boost.predict(valid_X))\n\nConfusion Matrix (Accuracy 0.9835)\n\n       Prediction\nActual    0    1\n     0 1799    8\n     1   25  168"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 18 - TS smoothing.html",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 18 - TS smoothing.html",
    "title": "Chapter 18: Smoothing Methods",
    "section": "",
    "text": "2019 Galit Shmueli, Peter C. Bruce, Peter Gedeck\n\nCode included in\nData Mining for Business Analytics: Concepts, Techniques, and Applications in Python (First Edition) Galit Shmueli, Peter C. Bruce, Peter Gedeck, and Nitin R. Patel. 2019."
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 18 - TS smoothing.html#import-required-packages",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 18 - TS smoothing.html#import-required-packages",
    "title": "Chapter 18: Smoothing Methods",
    "section": "Import required packages",
    "text": "Import required packages\n\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pylab as plt\nimport statsmodels.formula.api as sm\nfrom statsmodels.tsa import tsatools\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\n\nimport dmba\n\n%matplotlib inline\n\nno display found. Using non-interactive Agg backend\n\n\n\n# Create the graph\ndef singleGraphLayout(ax, ylim, train_df, valid_df):\n    ax.set_xlim('1990', '2004-6')\n    ax.set_ylim(*ylim)\n    ax.set_xlabel('Time')\n    one_month = pd.Timedelta('31 days')\n    xtrain = (min(train_df.index), max(train_df.index) - one_month)\n    xvalid = (min(valid_df.index) + one_month, max(valid_df.index) - one_month)\n    xtv = xtrain[1] + 0.5 * (xvalid[0] - xtrain[1])\n\n    ypos = 0.9 * ylim[1] + 0.1 * ylim[0]\n    ax.add_line(plt.Line2D(xtrain, (ypos, ypos), color='black', linewidth=0.5))\n    ax.add_line(plt.Line2D(xvalid, (ypos, ypos), color='black', linewidth=0.5))\n    ax.axvline(x=xtv, ymin=0, ymax=1, color='black', linewidth=0.5)\n\n    ypos = 0.925 * ylim[1] + 0.075 * ylim[0]\n    ax.text('1995', ypos, 'Training')\n    ax.text('2002-3', ypos, 'Validation')\n\ndef graphLayout(axes, train_df, valid_df):\n    singleGraphLayout(axes[0], [1300, 2550], train_df, valid_df)\n    singleGraphLayout(axes[1], [-550, 550], train_df, valid_df)\n    train_df.plot(y='Ridership', ax=axes[0], color='C0', linewidth=0.75)\n    valid_df.plot(y='Ridership', ax=axes[0], color='C0', linestyle='dashed', linewidth=0.75)\n    axes[1].axhline(y=0, xmin=0, xmax=1, color='black', linewidth=0.5)\n    axes[0].set_xlabel('')\n    axes[0].set_ylabel('Ridership (in 000s)')\n    axes[1].set_ylabel('Forecast Errors')\n    if axes[0].get_legend(): \n        axes[0].get_legend().remove()\n    # ensure that both axes have the same x-range\n    xlim = (min(axes[0].get_xlim()[0], axes[1].get_xlim()[0]), \n            max(axes[0].get_xlim()[1], axes[1].get_xlim()[1]))\n    axes[0].set_xlim(*xlim)\n    axes[1].set_xlim(*xlim)"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 18 - TS smoothing.html#figure-18.2",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 18 - TS smoothing.html#figure-18.2",
    "title": "Chapter 18: Smoothing Methods",
    "section": "Figure 18.2",
    "text": "Figure 18.2\n\n# Load data and convert to time series\nAmtrak_df = dmba.load_data('Amtrak.csv')\nAmtrak_df['Date'] = pd.to_datetime(Amtrak_df.Month, format='%d/%m/%Y')\nridership_ts = pd.Series(Amtrak_df.Ridership.values, index=Amtrak_df.Date, name='Ridership')\nridership_ts.index = pd.DatetimeIndex(ridership_ts.index, freq=ridership_ts.index.inferred_freq)\n\n# centered moving average with window size = 12\nma_centered = ridership_ts.rolling(12, center=True).mean()\n\n# trailing moving average with window size = 12\nma_trailing = ridership_ts.rolling(12).mean()\n\n# shift the average by one time unit\nma_centered = pd.Series(ma_centered[:-1].values, index=ma_centered.index[1:])\nma_trailing = pd.Series(ma_trailing[:-1].values, index=ma_trailing.index[1:])\n\nfig, ax = plt.subplots(figsize=(8, 7))\nax = ridership_ts.plot(ax=ax, color='black', linewidth=0.25)\nma_centered.plot(ax=ax, linewidth=2)\nma_trailing.plot(ax=ax, style='--', linewidth=2)\nax.set_xlabel('Time')\nax.set_ylabel('Ridership')\nax.legend(['Ridership', 'Centered Moving Average', 'Trailing Moving Average'])\n\nplt.show()"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 18 - TS smoothing.html#figure-18.3",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 18 - TS smoothing.html#figure-18.3",
    "title": "Chapter 18: Smoothing Methods",
    "section": "Figure 18.3",
    "text": "Figure 18.3\n\n# partition the data\nnValid = 36\nnTrain = len(ridership_ts) - nValid\n\ntrain_ts = ridership_ts[:nTrain]\nvalid_ts = ridership_ts[nTrain:]\n\n# moving average on training\nma_trailing = train_ts.rolling(12).mean()\nlast_ma = ma_trailing[-1]\n\n# create forecast based on last moving average in the training period\nma_trailing_pred = pd.Series(last_ma, index=valid_ts.index)\n\nfig, ax = plt.subplots(figsize=(9,4))\ntrain_ts.plot(ax=ax, color='black', linewidth=0.5)\nvalid_ts.plot(ax=ax, color='black', linewidth=0.25)\nma_trailing.plot(ax=ax, linewidth=2, color='C0')\nma_trailing_pred.plot(ax=ax, style='--', linewidth=2, color='C0')\nsingleGraphLayout(ax, [1300, 2550], train_ts, valid_ts)\n\nplt.show()\n\n\n\n\n\n\n\n\n\nfig, axes = plt.subplots(nrows=2, ncols=1, figsize=(9, 7.5))\n\nma_trailing.plot(ax=axes[0], linewidth=2, color='C1')\nma_trailing_pred.plot(ax=axes[0], linewidth=2, color='C1', linestyle='dashed')\n\nresidual = train_ts - ma_trailing\nresidual.plot(ax=axes[1], color='C1')\nresidual = valid_ts - ma_trailing_pred\nresidual.plot(ax=axes[1], color='C1', linestyle='dashed')\n\ngraphLayout(axes, train_ts, valid_ts)\n\nplt.show()"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 18 - TS smoothing.html#table-18.1",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 18 - TS smoothing.html#table-18.1",
    "title": "Chapter 18: Smoothing Methods",
    "section": "Table 18.1",
    "text": "Table 18.1\n\n# Build a model with seasonality, trend, and quadratic trend\nridership_df = tsatools.add_trend(ridership_ts, trend='ct')\nridership_df['Month'] = ridership_df.index.month\n\n# partition the data\ntrain_df = ridership_df[:nTrain]\nvalid_df = ridership_df[nTrain:]\n\nformula = 'Ridership ~ trend + np.square(trend) + C(Month)'\nridership_lm_trendseason = sm.ols(formula=formula, data=train_df).fit()\n\n# create single-point forecast\nridership_prediction = ridership_lm_trendseason.predict(valid_df.iloc[0, :])\n\n# apply MA to residuals\nma_trailing = ridership_lm_trendseason.resid.rolling(12).mean()\n\nprint('Prediction', ridership_prediction[0])\nprint('ma_trailing', ma_trailing[-1])\n\nPrediction 2004.2708927644999\nma_trailing 30.780684624059024"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 18 - TS smoothing.html#figure-18.4",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 18 - TS smoothing.html#figure-18.4",
    "title": "Chapter 18: Smoothing Methods",
    "section": "Figure 18.4",
    "text": "Figure 18.4\n\nax = train_df.Ridership.plot()\nvalid_df.Ridership.plot(ax=ax)\nridership_lm_trendseason.predict(valid_df).plot(ax=ax)\n\n\n\n\n\n\n\n\n\n# residuals_ts = ridership_lm_trendseason.resid\n\n# ax = ridership_lm_trendseason.resid.plot(color='black', linewidth=0.5)\n# ax.set_ylabel('Ridership')\n# ax.set_xlabel('Time')\n# ax.axhline(y=0, xmin=0, xmax=1, color='grey', linewidth=0.5)\n\n# # run exponential smoothing\n# # with smoothing level alpha = 0.2\n# expSmooth = ExponentialSmoothing(residuals_ts, freq='MS')\n# expSmoothFit = expSmooth.fit(smoothing_level=0.2)\n\n# expSmoothFit.fittedvalues.plot(ax=ax)\n# expSmoothFit.forecast(len(valid_ts)).plot(ax=ax, style='--', linewidth=2, color='C0')\n\n# plt.show()\n\n# print(expSmoothFit.forecast(1))\n\n\nresiduals_ts = ridership_lm_trendseason.resid\nresiduals_pred = valid_df.Ridership - ridership_lm_trendseason.predict(valid_df)\n\n\nfig, ax = plt.subplots(figsize=(9,4))\n\nridership_lm_trendseason.resid.plot(ax=ax, color='black', linewidth=0.5)\nresiduals_pred.plot(ax=ax, color='black', linewidth=0.5)\nax.set_ylabel('Ridership')\nax.set_xlabel('Time')\nax.axhline(y=0, xmin=0, xmax=1, color='grey', linewidth=0.5)\n\n# run exponential smoothing\n# with smoothing level alpha = 0.2\nexpSmooth = ExponentialSmoothing(residuals_ts, freq='MS')\nexpSmoothFit = expSmooth.fit(smoothing_level=0.2)\n\nexpSmoothFit.fittedvalues.plot(ax=ax)\nexpSmoothFit.forecast(len(valid_ts)).plot(ax=ax, style='--', linewidth=2, color='C0')\n\nsingleGraphLayout(ax, [-550, 550], train_df, valid_df)\n\nplt.show()\n\nprint(expSmoothFit.forecast(1))\n\n/opt/conda/lib/python3.9/site-packages/statsmodels/tsa/holtwinters/model.py:427: FutureWarning: After 0.13 initialization must be handled at model creation\n  warnings.warn(\n\n\n\n\n\n\n\n\n\n2001-04-01    14.142855\nFreq: MS, dtype: float64\n\n\n\n# residuals_ts = ridership_lm_trendseason.resid\n\n# fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(9, 7.5))\n\n# ridership_lm_trendseason.predict(train_df).plot(ax=axes[0], linewidth=2, color='C1')\n# ridership_lm_trendseason.predict(valid_df).plot(ax=axes[0], linewidth=1, color='C1')\n\n# ridership_lm_trendseason.resid.plot(ax=axes[1], color='C1')\n# residuals_pred = valid_df.Ridership - ridership_lm_trendseason.predict(valid_df)\n# residuals_pred.plot(ax=axes[1], color='C1', linestyle='dashed')\n\n\n# # run exponential smoothing\n# # with smoothing level alpha = 0.2\n# expSmooth = ExponentialSmoothing(residuals_ts, freq='MS')\n# expSmoothFit = expSmooth.fit(smoothing_level=0.2)\n# expSmoothForecast = expSmoothFit.forecast(len(valid_ts))\n\n# expSmoothFit.fittedvalues.plot(ax=axes[1], color='C2')\n# expSmoothForecast.plot(ax=axes[1], linewidth=2, color='C2', linestyle='dashed')\n\n# correctedRidership_lm_trendseason = ridership_lm_trendseason.predict(valid_df) - expSmoothForecast\n# correctedRidership_lm_trendseason.plot(ax=axes[0], linewidth=0.5, color='C2')\n\n\n# graphLayout(axes, train_ts, valid_ts)\n\n\n# plt.show()\n\n# print(expSmoothFit.forecast(1))"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 18 - TS smoothing.html#table-18.2",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 18 - TS smoothing.html#table-18.2",
    "title": "Chapter 18: Smoothing Methods",
    "section": "Table 18.2",
    "text": "Table 18.2\n\nexpSmoothFit.params\n\n{'smoothing_level': 0.5641935329401964,\n 'smoothing_trend': 0.0039077893405662185,\n 'smoothing_seasonal': 0.0005060994453987424,\n 'damping_trend': nan,\n 'initial_level': 1677.021852558779,\n 'initial_trend': 0.19821938768344577,\n 'initial_seasons': array([ 32.30923383, -10.24612048, 293.96013748, 294.91368035,\n        326.80746863, 281.36122053, 392.33191764, 443.45406142,\n        122.93665201, 247.42488569, 238.2382331 , 275.18748082]),\n 'use_boxcox': False,\n 'lamda': None,\n 'remove_bias': False}\n\n\n\nprint('AIC: ', expSmoothFit.aic)\nprint('AICc: ', expSmoothFit.aicc)\nprint('BIC: ', expSmoothFit.bic)\n\nAIC:  1022.3592535867122\nAICc:  1028.9361766636353\nBIC:  1067.354203272671"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 01 - Introduction.html",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 01 - Introduction.html",
    "title": "Chapter 1: Introduction",
    "section": "",
    "text": "2019 Galit Shmueli, Peter C. Bruce, Peter Gedeck\n\nCode included in\nData Mining for Business Analytics: Concepts, Techniques, and Applications in Python (First Edition) Galit Shmueli, Peter C. Bruce, Peter Gedeck, and Nitin R. Patel. 2019.\n%matplotlib inline\n\nfrom pathlib import Path\nimport pandas as pd\nimport matplotlib.pylab as plt\nimport dmba\n\nno display found. Using non-interactive Agg backend"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 01 - Introduction.html#figure-1.1",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 01 - Introduction.html#figure-1.1",
    "title": "Chapter 1: Introduction",
    "section": "Figure 1.1",
    "text": "Figure 1.1\n\nmower_df = dmba.load_data('RidingMowers.csv')\nmower_df.head()\n\n\n\n\n\n\n\n\nIncome\nLot_Size\nOwnership\n\n\n\n\n0\n60.0\n18.4\nOwner\n\n\n1\n85.5\n16.8\nOwner\n\n\n2\n64.8\n21.6\nOwner\n\n\n3\n61.5\n20.8\nOwner\n\n\n4\n87.0\n23.6\nOwner\n\n\n\n\n\n\n\n\ndef basePlot(ax):\n    mower_df.loc[mower_df.Ownership=='Owner'].plot(x='Income', y='Lot_Size', style='o', \n                                                   markerfacecolor='C0', markeredgecolor='C0',\n                                                   ax=ax)\n    mower_df.loc[mower_df.Ownership=='Nonowner'].plot(x='Income', y='Lot_Size', style='o',\n                                                      markerfacecolor='none', markeredgecolor='C1',\n                                                      ax=ax)\n    ax.legend([\"Owner\", \"Nonowner\"]);\n    ax.set_xlim(20, 120)\n    ax.set_ylim(13, 25)\n    ax.set_xlabel('Income ($000s)')\n    ax.set_ylabel('Lot Size (000s sqft)')\n    return ax\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(9, 3))\n\nax = basePlot(axes[0])\ny0, y1, y2 = 21.3, 19.8, 16.4\nx0, x1, x2 = 55, 58, 95\nax.plot((20, x0), (y0,y0), color='grey')\nax.plot((20, 120), (y1,y1), color='grey')\nax.plot((x1, x2), (y2,y2), color='grey')\nax.plot((x0, x0), (25,y1), color='grey')\nax.plot((x1, x1), (y1,13), color='grey')\nax.plot((x2, x2), (y1,13), color='grey')\n\nax = basePlot(axes[1])\nax.plot((40, 95), (25,13), color='grey')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 07 - kNN.html",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 07 - kNN.html",
    "title": "Chapter 7: k-Nearest Neighbors (kNN)",
    "section": "",
    "text": "2019 Galit Shmueli, Peter C. Bruce, Peter Gedeck\n\nCode included in\nData Mining for Business Analytics: Concepts, Techniques, and Applications in Python (First Edition) Galit Shmueli, Peter C. Bruce, Peter Gedeck, and Nitin R. Patel. 2019."
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 07 - kNN.html#import-required-packages",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 07 - kNN.html#import-required-packages",
    "title": "Chapter 7: k-Nearest Neighbors (kNN)",
    "section": "Import required packages",
    "text": "Import required packages\n\nfrom pathlib import Path\n\nimport pandas as pd\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.neighbors import NearestNeighbors, KNeighborsClassifier\nimport matplotlib.pylab as plt\n\nimport dmba\n\n%matplotlib inline\n\nno display found. Using non-interactive Agg backend"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 07 - kNN.html#table-7.1",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 07 - kNN.html#table-7.1",
    "title": "Chapter 7: k-Nearest Neighbors (kNN)",
    "section": "Table 7.1",
    "text": "Table 7.1\n\nmower_df = dmba.load_data('RidingMowers.csv')\nmower_df['Number'] = mower_df.index + 1\nmower_df.head(9)\n\n\n\n\n\n\n\n\nIncome\nLot_Size\nOwnership\nNumber\n\n\n\n\n0\n60.0\n18.4\nOwner\n1\n\n\n1\n85.5\n16.8\nOwner\n2\n\n\n2\n64.8\n21.6\nOwner\n3\n\n\n3\n61.5\n20.8\nOwner\n4\n\n\n4\n87.0\n23.6\nOwner\n5\n\n\n5\n110.1\n19.2\nOwner\n6\n\n\n6\n108.0\n17.6\nOwner\n7\n\n\n7\n82.8\n22.4\nOwner\n8\n\n\n8\n69.0\n20.0\nOwner\n9\n\n\n\n\n\n\n\n\ntrainData, validData = train_test_split(mower_df, test_size=0.4, random_state=26)\nprint(trainData.shape, validData.shape)\nnewHousehold = pd.DataFrame([{'Income': 60, 'Lot_Size': 20}])\nnewHousehold\n\n(14, 4) (10, 4)\n\n\n\n\n\n\n\n\n\nIncome\nLot_Size\n\n\n\n\n0\n60\n20\n\n\n\n\n\n\n\nScatter plot\n\nfig, ax = plt.subplots()\n\nsubset = trainData.loc[trainData['Ownership']=='Owner']\nax.scatter(subset.Income, subset.Lot_Size, marker='o', label='Owner', color='C1')\n\nsubset = trainData.loc[trainData['Ownership']=='Nonowner']\nax.scatter(subset.Income, subset.Lot_Size, marker='D', label='Nonowner', color='C0')\n\nax.scatter(newHousehold.Income, newHousehold.Lot_Size, marker='*', label='New household', color='black', s=150)\n\nplt.xlabel('Income')  # set x-axis label\nplt.ylabel('Lot_Size')  # set y-axis label\nfor _, row in trainData.iterrows():\n    ax.annotate(row.Number, (row.Income + 2, row.Lot_Size))\n    \nhandles, labels = ax.get_legend_handles_labels()\nax.set_xlim(40, 115)\nax.legend(handles, labels, loc=4)\n\nplt.show()\n\n\n\n\n\n\n\n\n\ndef plotDataset(ax, data, showLabel=True, **kwargs):\n    subset = data.loc[data['Ownership']=='Owner']\n    ax.scatter(subset.Income, subset.Lot_Size, marker='o', label='Owner' if showLabel else None, color='C1', **kwargs)\n\n    subset = data.loc[data['Ownership']=='Nonowner']\n    ax.scatter(subset.Income, subset.Lot_Size, marker='D', label='Nonowner' if showLabel else None, color='C0', **kwargs)\n\n    plt.xlabel('Income')  # set x-axis label\n    plt.ylabel('Lot_Size')  # set y-axis label\n    for _, row in data.iterrows():\n        ax.annotate(row.Number, (row.Income + 2, row.Lot_Size))\n\nfig, ax = plt.subplots()\n\nplotDataset(ax, trainData)\nplotDataset(ax, validData, showLabel=False, facecolors='none')\n\nax.scatter(newHousehold.Income, newHousehold.Lot_Size, marker='*', label='New household', color='black', s=150)\n\nplt.xlabel('Income')  # set x-axis label\nplt.ylabel('Lot_Size')  # set y-axis label\n    \nhandles, labels = ax.get_legend_handles_labels()\nax.set_xlim(40, 115)\nax.legend(handles, labels, loc=4)\n\nplt.show()"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 07 - kNN.html#table-7.2",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 07 - kNN.html#table-7.2",
    "title": "Chapter 7: k-Nearest Neighbors (kNN)",
    "section": "Table 7.2",
    "text": "Table 7.2\nInitialize normalized training, validation, and complete data frames. Use the training data to learn the transformation.\n\nscaler = preprocessing.StandardScaler()\nscaler.fit(trainData[['Income', 'Lot_Size']])  # Note the use of an array of column names\n\n# Transform the full dataset\nmowerNorm = pd.concat([pd.DataFrame(scaler.transform(mower_df[['Income', 'Lot_Size']]), \n                                    columns=['zIncome', 'zLot_Size']),\n                       mower_df[['Ownership', 'Number']]], axis=1)\ntrainNorm = mowerNorm.iloc[trainData.index]\nvalidNorm = mowerNorm.iloc[validData.index]\nnewHouseholdNorm = pd.DataFrame(scaler.transform(newHousehold), columns=['zIncome', 'zLot_Size'])\n\nUse k-nearest neighbour\n\nknn = NearestNeighbors(n_neighbors=3)\nknn.fit(trainNorm[['zIncome', 'zLot_Size']])\ndistances, indices = knn.kneighbors(newHouseholdNorm)\nprint(trainNorm.iloc[indices[0], :])  # indices is a list of lists, we are only interested in the first element\n\n     zIncome  zLot_Size Ownership  Number\n3  -0.409776   0.743358     Owner       4\n13 -0.804953   0.743358  Nonowner      14\n0  -0.477910  -0.174908     Owner       1"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 07 - kNN.html#table-7.3",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 07 - kNN.html#table-7.3",
    "title": "Chapter 7: k-Nearest Neighbors (kNN)",
    "section": "Table 7.3",
    "text": "Table 7.3\nInitialize a data frame with two columns: k and accuracy\n\ntrain_X = trainNorm[['zIncome', 'zLot_Size']]\ntrain_y = trainNorm['Ownership']\nvalid_X = validNorm[['zIncome', 'zLot_Size']]\nvalid_y = validNorm['Ownership']\n\n# Train a classifier for different values of k\nresults = []\nfor k in range(1, 15):\n    knn = KNeighborsClassifier(n_neighbors=k).fit(train_X, train_y)\n    results.append({\n        'k': k,\n        'accuracy': accuracy_score(valid_y, knn.predict(valid_X))\n    })\n\n# Convert results to a pandas data frame\nresults = pd.DataFrame(results)\nprint(results)\n\n     k  accuracy\n0    1       0.6\n1    2       0.7\n2    3       0.8\n3    4       0.9\n4    5       0.7\n5    6       0.9\n6    7       0.9\n7    8       0.9\n8    9       0.9\n9   10       0.8\n10  11       0.8\n11  12       0.9\n12  13       0.4\n13  14       0.4"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 07 - kNN.html#table-7.4",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 07 - kNN.html#table-7.4",
    "title": "Chapter 7: k-Nearest Neighbors (kNN)",
    "section": "Table 7.4",
    "text": "Table 7.4\n\n# Retrain with full dataset\nmower_X = mowerNorm[['zIncome', 'zLot_Size']]\nmower_y = mowerNorm['Ownership']\nknn = KNeighborsClassifier(n_neighbors=4).fit(mower_X, mower_y)\ndistances, indices = knn.kneighbors(newHouseholdNorm)\nprint(knn.predict(newHouseholdNorm))\nprint('Distances',distances)\nprint('Indices', indices)\nprint(mowerNorm.iloc[indices[0], :])\n\n['Owner']\nDistances [[0.31358009 0.40880312 0.44793643 0.61217726]]\nIndices [[ 3  8 13  0]]\n     zIncome  zLot_Size Ownership  Number\n3  -0.409776   0.743358     Owner       4\n8  -0.069107   0.437269     Owner       9\n13 -0.804953   0.743358  Nonowner      14\n0  -0.477910  -0.174908     Owner       1"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 19 - Social Network Analysis.html",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 19 - Social Network Analysis.html",
    "title": "Chapter 19: Social Network Analysis",
    "section": "",
    "text": "2019 Galit Shmueli, Peter C. Bruce, Peter Gedeck\n\nCode included in\nData Mining for Business Analytics: Concepts, Techniques, and Applications in Python (First Edition) Galit Shmueli, Peter C. Bruce, Peter Gedeck, and Nitin R. Patel. 2019."
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 19 - Social Network Analysis.html#import-required-packages",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 19 - Social Network Analysis.html#import-required-packages",
    "title": "Chapter 19: Social Network Analysis",
    "section": "Import required packages",
    "text": "Import required packages\n\nfrom pathlib import Path\n\nimport collections\nimport pandas as pd\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\nimport dmba\n\n%matplotlib inline\n\nno display found. Using non-interactive Agg backend"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 19 - Social Network Analysis.html#figure-19.1",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 19 - Social Network Analysis.html#figure-19.1",
    "title": "Chapter 19: Social Network Analysis",
    "section": "Figure 19.1",
    "text": "Figure 19.1\n\n# Build a dataframe that defines the edges and use to build the graph\ndf = pd.DataFrame([\n    (\"Dave\", \"Jenny\"), (\"Peter\", \"Jenny\"), (\"John\", \"Jenny\"),\n    (\"Dave\", \"Peter\"), (\"Dave\", \"John\"), (\"Peter\", \"Sam\"),\n    (\"Sam\", \"Albert\"), (\"Peter\", \"John\")\n], columns=['from', 'to'])\nG = nx.from_pandas_edgelist(df, 'from', 'to')\n \n# Plot it\nnx.draw(G, with_labels=True, node_color='lightblue', node_size=1600)\nplt.show()"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 19 - Social Network Analysis.html#figure-19.2",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 19 - Social Network Analysis.html#figure-19.2",
    "title": "Chapter 19: Social Network Analysis",
    "section": "Figure 19.2",
    "text": "Figure 19.2\n\n# generate and plot graph\n# use nx.DiGraph to create a directed graph\nG = nx.from_pandas_edgelist(df, 'from', 'to', create_using=nx.DiGraph())\n\nnx.draw(G, with_labels=True, node_color='lightblue', node_size=1600)\nplt.show()"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 19 - Social Network Analysis.html#figure-19.4-drug-laundry-network-in-san-antonio-tx",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 19 - Social Network Analysis.html#figure-19.4-drug-laundry-network-in-san-antonio-tx",
    "title": "Chapter 19: Social Network Analysis",
    "section": "Figure 19.4 DRUG LAUNDRY NETWORK IN SAN ANTONIO, TX",
    "text": "Figure 19.4 DRUG LAUNDRY NETWORK IN SAN ANTONIO, TX\n\ndrug_df = dmba.load_data('drug.csv')\n\nG = nx.from_pandas_edgelist(drug_df, 'Entity', 'Related Entity')\n\ncentrality = nx.eigenvector_centrality(G)\n\nnx.draw(G, with_labels=False, node_color='skyblue', node_size=[400*centrality[n] for n in G.nodes()])\nplt.show()"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 19 - Social Network Analysis.html#figure-19.5",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 19 - Social Network Analysis.html#figure-19.5",
    "title": "Chapter 19: Social Network Analysis",
    "section": "Figure 19.5",
    "text": "Figure 19.5\n\nG = nx.from_pandas_edgelist(df, 'from', 'to')\n\nplt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n\nplt.subplot(121)\nnx.draw_circular(G, with_labels=True, node_color='lightblue', node_size=1600)\nplt.subplot(122)\nnx.draw_kamada_kawai(G, with_labels=True, node_color='lightblue', node_size=1600)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 19 - Social Network Analysis.html#table-19.2-adjacency-matrix",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 19 - Social Network Analysis.html#table-19.2-adjacency-matrix",
    "title": "Chapter 19: Social Network Analysis",
    "section": "Table 19.2 Adjacency matrix",
    "text": "Table 19.2 Adjacency matrix\n\nG = nx.from_pandas_edgelist(df, 'from', 'to', create_using=nx.DiGraph())\nprint(nx.to_numpy_matrix(G))\n\n[[0. 1. 1. 1. 0. 0.]\n [0. 0. 0. 0. 0. 0.]\n [0. 1. 0. 1. 1. 0.]\n [0. 1. 0. 0. 0. 0.]\n [0. 0. 0. 0. 0. 1.]\n [0. 0. 0. 0. 0. 0.]]\n\n\n\nG = nx.from_pandas_edgelist(df, 'from', 'to')\nprint(G.degree())\nprint('Centrality: ')\nprint(nx.closeness_centrality(G))\n\nprint('Betweenness: ')\nprint(nx.betweenness_centrality(G, normalized=False))\nprint(nx.betweenness_centrality(G))\n\nprint('Eigenvector centrality: ')\nprint(nx.eigenvector_centrality(G, tol=1e-2))\nv = nx.eigenvector_centrality(G, tol=1e-2).values()\n\nnx.draw_kamada_kawai(G, with_labels=True, node_color='lightblue', node_size=1600)\n\n[('Dave', 3), ('Jenny', 3), ('Peter', 4), ('John', 3), ('Sam', 2), ('Albert', 1)]\nCentrality: \n{'Dave': 0.625, 'Jenny': 0.625, 'Peter': 0.8333333333333334, 'John': 0.625, 'Sam': 0.625, 'Albert': 0.4166666666666667}\nBetweenness: \n{'Dave': 0.0, 'Jenny': 0.0, 'Peter': 6.0, 'John': 0.0, 'Sam': 4.0, 'Albert': 0.0}\n{'Dave': 0.0, 'Jenny': 0.0, 'Peter': 0.6000000000000001, 'John': 0.0, 'Sam': 0.4, 'Albert': 0.0}\nEigenvector centrality: \n{'Dave': 0.47246979363344, 'Jenny': 0.47246979363344, 'Peter': 0.528716197637421, 'John': 0.47246979363344, 'Sam': 0.2105222549863287, 'Albert': 0.08035200571997278}\n\n\n\n\n\n\n\n\n\n\nprint(nx.betweenness_centrality(G))\nprint(nx.current_flow_betweenness_centrality(G))\n\n{'Dave': 0.0, 'Jenny': 0.0, 'Peter': 0.6000000000000001, 'John': 0.0, 'Sam': 0.4, 'Albert': 0.0}\n{'Dave': 0.175, 'Jenny': 0.175, 'Peter': 0.675, 'John': 0.175, 'Sam': 0.4, 'Albert': 0.0}\n\n\n\nprint(G.nodes)\n\nplt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n\nplt.subplot(121)\nG_ego = nx.ego_graph(G, 'Peter')\nnx.draw(G_ego, with_labels=True, node_color='lightblue', node_size=1600)\nplt.subplot(122)\nG_ego = nx.ego_graph(G, 'Peter', radius=2)\nnx.draw(G_ego, with_labels=True, node_color='lightblue', node_size=1600)\n\n['Dave', 'Jenny', 'Peter', 'John', 'Sam', 'Albert']"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 19 - Social Network Analysis.html#table-19.5",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 19 - Social Network Analysis.html#table-19.5",
    "title": "Chapter 19: Social Network Analysis",
    "section": "Table 19.5",
    "text": "Table 19.5\n\ndegreeCount = collections.Counter(d for node, d in G.degree())\ndegreeDistribution = [0] * (1 + max(degreeCount))\nfor degree, count in degreeCount.items():\n    degreeDistribution[degree] = count\ndegreeDistribution\n\n[0, 1, 1, 3, 1]\n\n\n\ndegreeCount = collections.Counter(d for node, d in G.degree())\ndegreeCount\n\nCounter({3: 3, 4: 1, 2: 1, 1: 1})\n\n\n\nnx.density(G)\n\n0.5333333333333333\n\n\n\n[d / sum(degreeDistribution) for d in degreeDistribution]  # normalized\n\n[0.0, 0.16666666666666666, 0.16666666666666666, 0.5, 0.16666666666666666]"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 19 - Social Network Analysis.html#table-19.2",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 19 - Social Network Analysis.html#table-19.2",
    "title": "Chapter 19: Social Network Analysis",
    "section": "Table 19.2",
    "text": "Table 19.2\n\ntoken = '&lt;access token&gt;'\nif token != '&lt;access token&gt;':  # skip execution if token is not available\n    import facebook\n    graph = facebook.GraphAPI(access_token=token)\n    results = graph.request('/dataminingbook/posts?fields=likes.summary(true),message,created_time&limit=10')\n    pd.DataFrame([\n        {\n            'created_time': d.get('created_time', ''),\n            'message': d.get('message', ''),\n            'likes': d.get('likes', {}).get('summary', {}).get('total_count', 0),\n        } for d in results['data']])"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter15/chapter15.html#short-video-introduction",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter15/chapter15.html#short-video-introduction",
    "title": "TITLE",
    "section": "SHORT VIDEO INTRODUCTION",
    "text": "SHORT VIDEO INTRODUCTION"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter15/chapter15.html#a-1",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter15/chapter15.html#a-1",
    "title": "TITLE",
    "section": "a",
    "text": "a\na"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter15/chapter15.html#more-to-read",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter15/chapter15.html#more-to-read",
    "title": "TITLE",
    "section": "More to Read",
    "text": "More to Read"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter15/chapter15.html#assignment",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter15/chapter15.html#assignment",
    "title": "TITLE",
    "section": "Assignment",
    "text": "Assignment\n\n\n\nWhat to do\n\n\n\nRequirement\n\nPDF format\nfile name should be include your student id and name\n\nstuID_name_title.pdf (e.g. 1111111_ChungilChae_SelfIntroduction.pdf)\n\n\nDue date\n\nby DATE 11:59PM\nNO LATE SUBMISSION ALLOWED!!!!"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter13/chapter13.html#short-video-introduction",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter13/chapter13.html#short-video-introduction",
    "title": "TITLE",
    "section": "SHORT VIDEO INTRODUCTION",
    "text": "SHORT VIDEO INTRODUCTION"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter13/chapter13.html#a-1",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter13/chapter13.html#a-1",
    "title": "TITLE",
    "section": "a",
    "text": "a\na"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter13/chapter13.html#more-to-read",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter13/chapter13.html#more-to-read",
    "title": "TITLE",
    "section": "More to Read",
    "text": "More to Read"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter13/chapter13.html#assignment",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter13/chapter13.html#assignment",
    "title": "TITLE",
    "section": "Assignment",
    "text": "Assignment\n\n\n\nWhat to do\n\n\n\nRequirement\n\nPDF format\nfile name should be include your student id and name\n\nstuID_name_title.pdf (e.g. 1111111_ChungilChae_SelfIntroduction.pdf)\n\n\nDue date\n\nby DATE 11:59PM\nNO LATE SUBMISSION ALLOWED!!!!"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter3/chapter3.html#short-video-introduction",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter3/chapter3.html#short-video-introduction",
    "title": "TITLE",
    "section": "SHORT VIDEO INTRODUCTION",
    "text": "SHORT VIDEO INTRODUCTION"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter3/chapter3.html#a-1",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter3/chapter3.html#a-1",
    "title": "TITLE",
    "section": "a",
    "text": "a\na"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter3/chapter3.html#more-to-read",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter3/chapter3.html#more-to-read",
    "title": "TITLE",
    "section": "More to Read",
    "text": "More to Read"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter3/chapter3.html#assignment",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter3/chapter3.html#assignment",
    "title": "TITLE",
    "section": "Assignment",
    "text": "Assignment\n\n\n\nWhat to do\n\n\n\nRequirement\n\nPDF format\nfile name should be include your student id and name\n\nstuID_name_title.pdf (e.g. 1111111_ChungilChae_SelfIntroduction.pdf)\n\n\nDue date\n\nby DATE 11:59PM\nNO LATE SUBMISSION ALLOWED!!!!"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter5/chapter5.html#short-video-introduction",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter5/chapter5.html#short-video-introduction",
    "title": "TITLE",
    "section": "SHORT VIDEO INTRODUCTION",
    "text": "SHORT VIDEO INTRODUCTION"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter5/chapter5.html#a-1",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter5/chapter5.html#a-1",
    "title": "TITLE",
    "section": "a",
    "text": "a\na"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter5/chapter5.html#more-to-read",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter5/chapter5.html#more-to-read",
    "title": "TITLE",
    "section": "More to Read",
    "text": "More to Read"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter5/chapter5.html#assignment",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter5/chapter5.html#assignment",
    "title": "TITLE",
    "section": "Assignment",
    "text": "Assignment\n\n\n\nWhat to do\n\n\n\nRequirement\n\nPDF format\nfile name should be include your student id and name\n\nstuID_name_title.pdf (e.g. 1111111_ChungilChae_SelfIntroduction.pdf)\n\n\nDue date\n\nby DATE 11:59PM\nNO LATE SUBMISSION ALLOWED!!!!"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter11/chapter11.html#short-video-introduction",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter11/chapter11.html#short-video-introduction",
    "title": "TITLE",
    "section": "SHORT VIDEO INTRODUCTION",
    "text": "SHORT VIDEO INTRODUCTION"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter11/chapter11.html#a-1",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter11/chapter11.html#a-1",
    "title": "TITLE",
    "section": "a",
    "text": "a\na"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter11/chapter11.html#more-to-read",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter11/chapter11.html#more-to-read",
    "title": "TITLE",
    "section": "More to Read",
    "text": "More to Read"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter11/chapter11.html#assignment",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter11/chapter11.html#assignment",
    "title": "TITLE",
    "section": "Assignment",
    "text": "Assignment\n\n\n\nWhat to do\n\n\n\nRequirement\n\nPDF format\nfile name should be include your student id and name\n\nstuID_name_title.pdf (e.g. 1111111_ChungilChae_SelfIntroduction.pdf)\n\n\nDue date\n\nby DATE 11:59PM\nNO LATE SUBMISSION ALLOWED!!!!"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter20/chapter20.html#short-video-introduction",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter20/chapter20.html#short-video-introduction",
    "title": "TITLE",
    "section": "SHORT VIDEO INTRODUCTION",
    "text": "SHORT VIDEO INTRODUCTION"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter20/chapter20.html#a-1",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter20/chapter20.html#a-1",
    "title": "TITLE",
    "section": "a",
    "text": "a\na"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter20/chapter20.html#more-to-read",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter20/chapter20.html#more-to-read",
    "title": "TITLE",
    "section": "More to Read",
    "text": "More to Read"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter20/chapter20.html#assignment",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter20/chapter20.html#assignment",
    "title": "TITLE",
    "section": "Assignment",
    "text": "Assignment\n\n\n\nWhat to do\n\n\n\nRequirement\n\nPDF format\nfile name should be include your student id and name\n\nstuID_name_title.pdf (e.g. 1111111_ChungilChae_SelfIntroduction.pdf)\n\n\nDue date\n\nby DATE 11:59PM\nNO LATE SUBMISSION ALLOWED!!!!"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter19/chapter19.html#short-video-introduction",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter19/chapter19.html#short-video-introduction",
    "title": "TITLE",
    "section": "SHORT VIDEO INTRODUCTION",
    "text": "SHORT VIDEO INTRODUCTION"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter19/chapter19.html#a-1",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter19/chapter19.html#a-1",
    "title": "TITLE",
    "section": "a",
    "text": "a\na"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter19/chapter19.html#more-to-read",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter19/chapter19.html#more-to-read",
    "title": "TITLE",
    "section": "More to Read",
    "text": "More to Read"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter19/chapter19.html#assignment",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter19/chapter19.html#assignment",
    "title": "TITLE",
    "section": "Assignment",
    "text": "Assignment\n\n\n\nWhat to do\n\n\n\nRequirement\n\nPDF format\nfile name should be include your student id and name\n\nstuID_name_title.pdf (e.g. 1111111_ChungilChae_SelfIntroduction.pdf)\n\n\nDue date\n\nby DATE 11:59PM\nNO LATE SUBMISSION ALLOWED!!!!"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter17/chapter17.html#short-video-introduction",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter17/chapter17.html#short-video-introduction",
    "title": "TITLE",
    "section": "SHORT VIDEO INTRODUCTION",
    "text": "SHORT VIDEO INTRODUCTION"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter17/chapter17.html#a-1",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter17/chapter17.html#a-1",
    "title": "TITLE",
    "section": "a",
    "text": "a\na"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter17/chapter17.html#more-to-read",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter17/chapter17.html#more-to-read",
    "title": "TITLE",
    "section": "More to Read",
    "text": "More to Read"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter17/chapter17.html#assignment",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter17/chapter17.html#assignment",
    "title": "TITLE",
    "section": "Assignment",
    "text": "Assignment\n\n\n\nWhat to do\n\n\n\nRequirement\n\nPDF format\nfile name should be include your student id and name\n\nstuID_name_title.pdf (e.g. 1111111_ChungilChae_SelfIntroduction.pdf)\n\n\nDue date\n\nby DATE 11:59PM\nNO LATE SUBMISSION ALLOWED!!!!"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#short-video-introduction",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#short-video-introduction",
    "title": "Introduction and House Keeping",
    "section": "SHORT VIDEO INTRODUCTION",
    "text": "SHORT VIDEO INTRODUCTION"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#professor",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#professor",
    "title": "Introduction and House Keeping",
    "section": "Professor",
    "text": "Professor\nChad (Dr. Chungil Chae)\n\n\n\n\n\nChad (Chungil Chae)\nCBPM B223 | cchae@kean.edu\nAssistant Professor at CBPM, WKU since 2020 Fall\nCall ma Chad, but in formal situation and space, Dr.Chae or Prof.Chae\nTeaching business analytics major courses\n\nMGS 3001: Python for Business\nMGS 3101: Foundation of Business Analytics\nMGS 3701: Data Mining\nMGS 4701: Application of Business Analytics"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#teaching-assistant",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#teaching-assistant",
    "title": "Introduction and House Keeping",
    "section": "Teaching Assistant",
    "text": "Teaching Assistant\n\n\n\n\n\n\n**** ()\n(wku.edu.cn?)\nmajor in Finance"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#overview",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#overview",
    "title": "Introduction and House Keeping",
    "section": "Overview",
    "text": "Overview"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#part-contents",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#part-contents",
    "title": "Introduction and House Keeping",
    "section": "Part Contents",
    "text": "Part Contents\n\nPart I (Chapters 1–2) gives a general overview of data mining and its components.\nPart II (Chapters 3–4) focuses on the early stages of data exploration and dimension reduction.\nPart III (Chapter 5) discusses performance evaluation. Although it contains only one chapter, we discuss a variety of topics, from predictive performance metrics to misclassification costs. The principles covered in this part are crucial for the proper evaluation and comparison of supervised learning methods.\nPart IV includes eight chapters (Chapters 6–13), covering a variety of popular supervised learning methods (for classification and/or prediction). Within this part, the topics are generally organized according to the level of sophistication of the algorithms, their popularity, and ease of understanding. The final chapter introduces ensembles and combinations of methods."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#class-information",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#class-information",
    "title": "Introduction and House Keeping",
    "section": "Class Information",
    "text": "Class Information\n\nMGS3701: Data Mining\nClass time: T, TH 4:00 pm - 5:15 pm\nClass room: CBPM A202"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#in-class",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#in-class",
    "title": "Introduction and House Keeping",
    "section": "In CLass",
    "text": "In CLass\n\nYou are expected to read chapter and course material before class\nBased on your class participation, you will get extra score\nComputer and other digital device is allowed ONLY students uses it for class related purpose.\nIn case instuctor find unauthorized useage of digital device, you will be asked to leave class."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#attendence-and-absent",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#attendence-and-absent",
    "title": "Introduction and House Keeping",
    "section": "Attendence and Absent",
    "text": "Attendence and Absent\n\nDON”T SENT ME EMAIL or ANY MESSAGE about YOUR ABSENT in ADVANCE\nMore than three times of absents automatically will be marked as F\nAttendence will be managed in student performance application\nWhen instructor or TA check your attendence and if you are not in class, no matter what reason, your attendence will be marked as absent.\nHowever, if you have proper and official evidence that WKU allow for absent, bring it to your instructor for revise your absent mark to attendece."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#integration",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#integration",
    "title": "Introduction and House Keeping",
    "section": "Integration",
    "text": "Integration\n\nPlagiarism is not tolerated\n\nRight after find plagiarism, it will be reported to Office of Vice Chancellor for Academic Affairs directly\nStudent will be kicked out from class immediately\nRead Academic Integrity Policy"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#generative-ai-use",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#generative-ai-use",
    "title": "Introduction and House Keeping",
    "section": "Generative AI Use",
    "text": "Generative AI Use\nStudents are permitted to use AI tools, including, but not limited to, ChatGhT, in this course to generate ideas and brainstorm.\n\nThink of generative AI as an always-available brainstorming partner. However, you should note that the material generated by these programs may be inaccurate, incomplete, or otherwise problematic. Beware that use may also stifle your independent thinking and creativity.\nAcademic work involves developing essential skills such as critical thinking, problem-solving, and effective communication, which cannot be fully developed by relying solely on Artificial Intelligence (AI)."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter6/chapter6.html#short-video-introduction",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter6/chapter6.html#short-video-introduction",
    "title": "TITLE",
    "section": "SHORT VIDEO INTRODUCTION",
    "text": "SHORT VIDEO INTRODUCTION"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter6/chapter6.html#a-1",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter6/chapter6.html#a-1",
    "title": "TITLE",
    "section": "a",
    "text": "a\na"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter6/chapter6.html#more-to-read",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter6/chapter6.html#more-to-read",
    "title": "TITLE",
    "section": "More to Read",
    "text": "More to Read"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter6/chapter6.html#assignment",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter6/chapter6.html#assignment",
    "title": "TITLE",
    "section": "Assignment",
    "text": "Assignment\n\n\n\nWhat to do\n\n\n\nRequirement\n\nPDF format\nfile name should be include your student id and name\n\nstuID_name_title.pdf (e.g. 1111111_ChungilChae_SelfIntroduction.pdf)\n\n\nDue date\n\nby DATE 11:59PM\nNO LATE SUBMISSION ALLOWED!!!!"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter8/chapter8.html#short-video-introduction",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter8/chapter8.html#short-video-introduction",
    "title": "TITLE",
    "section": "SHORT VIDEO INTRODUCTION",
    "text": "SHORT VIDEO INTRODUCTION"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter8/chapter8.html#a-1",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter8/chapter8.html#a-1",
    "title": "TITLE",
    "section": "a",
    "text": "a\na"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter8/chapter8.html#more-to-read",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter8/chapter8.html#more-to-read",
    "title": "TITLE",
    "section": "More to Read",
    "text": "More to Read"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter8/chapter8.html#assignment",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter8/chapter8.html#assignment",
    "title": "TITLE",
    "section": "Assignment",
    "text": "Assignment\n\n\n\nWhat to do\n\n\n\nRequirement\n\nPDF format\nfile name should be include your student id and name\n\nstuID_name_title.pdf (e.g. 1111111_ChungilChae_SelfIntroduction.pdf)\n\n\nDue date\n\nby DATE 11:59PM\nNO LATE SUBMISSION ALLOWED!!!!"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/Lecture_Note/chapter1/chapter1_note.html",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/Lecture_Note/chapter1/chapter1_note.html",
    "title": "[Lecture Note] Chapter1: Introduction",
    "section": "",
    "text": "What Is Business Analytics?\nWhat Is Data Mining?\nData Mining and Related Terms\nBig Data\nData Science\nWhy Are There So Many Different Methods?\nTerminology and Notation\nRoad Maps to This Book (Shmueli et al., 2017)"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/Lecture_Note/chapter1/chapter1_note.html#business-analytics",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/Lecture_Note/chapter1/chapter1_note.html#business-analytics",
    "title": "[Lecture Note] Chapter1: Introduction",
    "section": "Business Analytics",
    "text": "Business Analytics\n\nBusiness Analytics (BA) involves using quantitative data to aid decision-making, with its applications and interpretations varying across different organizations.\n\nFor example, a British tabloid once utilized it to test which images on their website, like cats or dogs, garnered more views.\nIn contrast, the Washington Post uses analytics to target specific influential audiences, such as defense contractors, by tracking reader behaviors like time of day and subscription details.\nBA encompasses simple data analysis techniques such as counting and basic arithmetic.\n\nHowever, it also includes more advanced practices known as Business Intelligence (BI),which focuses on data visualization and dynamic reporting to help understand past and current events.\n\nBI tools have evolved from static reports to interactive dashboards that provide real-time data interaction, enhancing managerial decision-making.\n\nFurthermore, Business Analytics has expanded to include sophisticated statistical and data mining methods aimed at exploring data relationships, making predictions, and forecasting future trends.\n\nTechniques like regression models describe relationships and predict outcomes.\nThe field of BA is also supported by methods like A-B testing used in pricing strategies, demonstrating its practical implications in various business contexts.\nHowever, successful deployment of BA requires a clear understanding of the business context and the functionality of analytics tools to avoid misapplication.\n\nOverall, Business Analytics has evolved from basic data reporting (BI) to a comprehensive toolkit that includes advanced analytics, emphasizing the necessity for strategic application aligned with business objectives."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/Lecture_Note/chapter1/chapter1_note.html#who-uses-predictive-analytics",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/Lecture_Note/chapter1/chapter1_note.html#who-uses-predictive-analytics",
    "title": "[Lecture Note] Chapter1: Introduction",
    "section": "Who Uses Predictive Analytics?",
    "text": "Who Uses Predictive Analytics?\n\nThe integration of predictive analytics into various sectors has significantly enhanced organizational capabilities due to the growing availability of data. Key examples include:\n\n\nCredit Scoring:\n\nCredit scoring utilizes predictive modeling to assess an individual’s likelihood of repaying debts. Rather than being an arbitrary measure, it derives from historical data analysis to forecast future repayment behaviors. This established method helps financial institutions determine creditworthiness efficiently.\n\nFuture Purchases:\n\nAn instance of the application of predictive models in marketing is demonstrated by Target’s method to infer whether a customer is likely pregnant based on their shopping patterns. This insight allows Target to send tailored promotions to potential mothers at the early stages of pregnancy, optimizing marketing efforts and potentially increasing sales during a crucial buying period.\n\nTax Evasion:\n\nThe U.S. Internal Revenue Service (IRS) has leveraged predictive analytics to enhance its enforcement strategies. By employing predictive models, the IRS is reportedly 25 times more successful in identifying cases of tax evasion. This focused approach allows them to concentrate resources on auditing individuals who are statistically more likely to have evaded taxes, making their processes more effective and efficient."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/Lecture_Note/chapter1/chapter1_note.html#data-mining-is",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/Lecture_Note/chapter1/chapter1_note.html#data-mining-is",
    "title": "[Lecture Note] Chapter1: Introduction",
    "section": "Data mining is",
    "text": "Data mining is\n\nIn the context of this book, data mining extends beyond simple counting, descriptive statistics, and rule-based methods, venturing into more sophisticated realms of business analytics.\nWhile data visualization serves as an introductory tool in advanced analytics, the primary focus of the book is on deeper, more complex data analytics tools.\nThese include both statistical and machine-learning techniques designed to automate and enhance decision-making processes, with a strong emphasis on prediction at an individual level.\nFor instance, rather than merely analyzing broad relationships like the connection between advertising and sales, the book explores targeted questions such as which specific advertisement or recommendation should be presented to a particular online shopper in real-time.\nAdditionally, it covers methods for clustering customers into distinct groups or “personas” tailored for different marketing strategies, and details how new prospects can be assigned to these personas for more effective engagement.\nThe rise of Big Data has further propelled data mining into prominence.\nThese methods are particularly adept at handling vast datasets and are key to unlocking valuable insights from the data deluge, thanks to their powerful analytical capabilities and automation potential.\nThis shift highlights the advanced level of analytics discussed in the book, which prioritizes actionable insights and customization in business strategies."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/Lecture_Note/chapter1/chapter1_note.html#terms",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/Lecture_Note/chapter1/chapter1_note.html#terms",
    "title": "[Lecture Note] Chapter1: Introduction",
    "section": "Terms",
    "text": "Terms\n\nDue to the hybrid origins and interdisciplinary nature of data mining, the terminology used by its practitioners often varies depending on their background in fields like machine learning (artificial intelligence) or statistics. Here’s a list of commonly used terms in data mining, along with descriptions of how they might be referred to in different fields:\n\n\nAlgorithm : A specific procedure for implementing a data mining technique, such as a classification tree or discriminant analysis. Attribute : Also referred to as Predictor.\nCase : Also known as Observation.\nConfidence : In the context of association rules, this is the conditional probability that an event will occur given another event. In statistics, it also refers to the concept of confidence intervals, which deal with the variability expected from one sample to another.\nDependent Variable : Known in machine learning as the Response.\nEstimation : Also referred to as Prediction.\nFeature : Another term for Predictor.\nHoldout Data : A subset of data not used in training a model but reserved for testing its performance. Also called the validation set or test set.\nInput Variable : Also known as Predictor.\nModel : An algorithm applied to a dataset, complete with its parameter settings.\nObservation : The unit of analysis, also called instance, sample, example, case, record, pattern, or row.\nOutcome Variable : Another term for Response.\nOutput Variable : Also known as Response.\nP (A | B) : The probability of event A given event B has occurred.\nPrediction : The act of predicting the value of a continuous variable; also called estimation.\nPredictor : An input variable (denoted by X) used in a predictive model. Also called feature, input variable, independent variable, or field.\nProfile : A set of measurements on an observation.\nRecord : Synonymous with Observation.\nResponse : The variable being predicted in supervised learning, also known as the dependent variable, output variable, target variable, or outcome variable.\nSample : Used in statistics to denote a collection of observations, whereas in machine learning it typically refers to a single observation.\nScore : A predicted value or class. Scoring involves using a model developed with training data to predict values in new data.\nSuccess Class : In binary outcomes, this refers to the class of interest (e.g., purchasers in a purchase/no purchase outcome).\nSupervised Learning : A type of machine learning where the model is trained on data where the outcome is known in order to learn to predict the outcome on new data.\nTarget : Synonymous with Response. Test Data : Data reserved for testing the final model’s performance, used only at the end stages of model building and selection.\nTraining Data : Data used initially to fit and train a model. Unsupervised Learning : Learning patterns from data that do not involve predicting a specific output value.\nValidation Data : Data used to check how well a model fits, to tweak models, and to select the best among tried models."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/Lecture_Note/chapter1/chapter1_note.html#order-of-topics",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/Lecture_Note/chapter1/chapter1_note.html#order-of-topics",
    "title": "[Lecture Note] Chapter1: Introduction",
    "section": "Order of Topics",
    "text": "Order of Topics\nThe book is structured into eight parts, each focusing on distinct aspects and applications of data mining:\n\nPart I (Chapters 1–2): This section provides a broad introduction to data mining, outlining its key components and the fundamental concepts underpinning the field. It serves as the foundational groundwork for the more detailed explorations that follow.\nPart II (Chapters 3–4): Here, the focus shifts to the preliminary stages of data analysis, specifically on data exploration and dimension reduction. These chapters help readers understand how to streamline complex datasets into more manageable and interpretable forms.\nPart III (Chapter 5): Although it consists of only one chapter, this part dives deep into performance evaluation, covering everything from predictive performance metrics to the costs associated with misclassification. The principles discussed here are critical for accurately evaluating and comparing different supervised learning methodologies.\nPart IV (Chapters 6–13): This substantial segment discusses various popular supervised learning methods used for classification and prediction. The chapters are organized by the complexity of the algorithms, their popularity, and their accessibility. The concluding chapter in this part introduces the concept of ensembles and method combinations, which can enhance prediction accuracy.\nPart V (Chapters 14–15): Focused on unsupervised learning, this part examines methods for mining relationships through association rules and collaborative filtering, as well as cluster analysis. These techniques are vital for discovering patterns and groupings in data without predefined labels.\nPart VI (Chapters 16–18): These chapters are devoted to forecasting time series data. The initial chapter addresses general issues related to handling and interpreting time series data, followed by chapters on regression-based forecasting and smoothing methods. These approaches are essential for making predictions about future events based on historical data.\nPart VII (Chapters 19–20): This section explores specialized applications of data mining in social network analysis and text mining. These chapters demonstrate how data mining techniques can be adapted to analyze data from specific structures like social networks and textual content.\nPart VIII: The final part of the book presents a collection of case studies that illustrate the practical application of the techniques discussed in earlier chapters.\nWhile the chapters within the book are designed to stand alone, enabling readers to focus on topics of particular interest without requiring sequential reading, it is recommended that Parts I–III be read first to establish a solid understanding of the basics before delving into the more advanced topics in Parts IV–VIII.\nAdditionally, Chapter 16 should ideally be read before proceeding with other chapters in Part VI to ensure a proper understanding of time series analysis fundamentals. This structured approach allows readers to build their knowledge progressively and effectively."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/Lecture_Note/chapter1/chapter1_note.html#more-to-read",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/Lecture_Note/chapter1/chapter1_note.html#more-to-read",
    "title": "[Lecture Note] Chapter1: Introduction",
    "section": "More to Read",
    "text": "More to Read"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/Lecture_Note/chapter1/chapter1_note.html#assignment",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/Lecture_Note/chapter1/chapter1_note.html#assignment",
    "title": "[Lecture Note] Chapter1: Introduction",
    "section": "Assignment",
    "text": "Assignment\n\nWhat to do\nRequirement\n\nPDF format\nfile name should be include your student id and name\n\nstuID_name_title.pdf (e.g. 1111111_ChungilChae_SelfIntroduction.pdf)\n\n\nDue date\n\nby DATE 11:59PM\nNO LATE SUBMISSION ALLOWED!!!!"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-13-ProbSolutions-Ensemble.html",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-13-ProbSolutions-Ensemble.html",
    "title": "Chapter 13: Combining Methods: Ensembles and Uplift Modeling",
    "section": "",
    "text": "2019-2020 Galit Shmueli, Peter C. Bruce, Peter Gedeck\n\nData Mining for Business Analytics: Concepts, Techniques, and Applications in Python (First Edition) Galit Shmueli, Peter C. Bruce, Peter Gedeck, and Nitin R. Patel. 2019.\nDate: 2020-03-08\nPython Version: 3.8.2 Jupyter Notebook Version: 5.6.1\nPackages: - dmba: 0.0.12 - matplotlib: 3.2.0 - pandas: 1.0.1 - scikit-learn: 0.22.2\nThe assistance from Mr. Kuber Deokar and Ms. Anuja Kulkarni in preparing these solutions is gratefully acknowledged.\n# Import required packages for this chapter\nfrom pathlib import Path\n\nimport pandas as pd\nfrom sklearn import preprocessing\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n\nimport matplotlib.pylab as plt\n\nfrom dmba import classificationSummary\nfrom dmba import liftChart\n\n%matplotlib inline\n# Working directory:\n#\n# We assume that data are kept in the same directory as the notebook. If you keep your \n# data in a different folder, replace the argument of the `Path`\nDATA = Path('.').resolve().parent / 'data'\nFIGURES = Path('.').resolve().parent / 'figures' / 'chapter_13'\nFIGURES.mkdir(exist_ok=True, parents=True)\n# and then load data using \n#\n# pd.read_csv(DATA / ‘filename.csv’)"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-13-ProbSolutions-Ensemble.html#data-preprocessing",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-13-ProbSolutions-Ensemble.html#data-preprocessing",
    "title": "Chapter 13: Combining Methods: Ensembles and Uplift Modeling",
    "section": "Data preprocessing",
    "text": "Data preprocessing\nPreprocess the data as follows:\n\nBin the following variables so they can be used in Naive Bayes: Age (5 bins), Experience (10 bins), Income (5 bins), CC Average (6 bins), and Mortgage (10 bins).\nEducation and Family can be used as is, without binning.\nZip code can be ignored.\nUse one-hot-encoding to convert the categorical data into indicator variables\nPartition the data: 60% training, 40% validation.\n\n\nbank_df = pd.read_csv(DATA / 'UniversalBank.csv')\nbank_df.head()\n\n\n\n\n\n\n\n\nID\nAge\nExperience\nIncome\nZIP Code\nFamily\nCCAvg\nEducation\nMortgage\nPersonal Loan\nSecurities Account\nCD Account\nOnline\nCreditCard\n\n\n\n\n0\n1\n25\n1\n49\n91107\n4\n1.6\n1\n0\n0\n1\n0\n0\n0\n\n\n1\n2\n45\n19\n34\n90089\n3\n1.5\n1\n0\n0\n1\n0\n0\n0\n\n\n2\n3\n39\n15\n11\n94720\n1\n1.0\n1\n0\n0\n0\n0\n0\n0\n\n\n3\n4\n35\n9\n100\n94112\n1\n2.7\n2\n0\n0\n0\n0\n0\n0\n\n\n4\n5\n35\n8\n45\n91330\n4\n1.0\n2\n0\n0\n0\n0\n0\n1\n\n\n\n\n\n\n\n\n# Drop ID and zip code columns\nbank_df.drop(columns=['ID', 'ZIP Code'], inplace=True)\n\n# The remaining columns (Age, Experience, Income, Mortgage and CCAvg) will be binned\nbank_df['Age'] = pd.cut(bank_df['Age'], 5, labels=range(1, 6)).astype('category')\nbank_df['Experience'] = pd.cut(bank_df['Experience'], 10, labels=range(1, 11)).astype('category')\nbank_df['Income'] = pd.cut(bank_df['Income'], 5, labels=range(1, 6)).astype('category')\nbank_df['CCAvg'] = pd.cut(bank_df['CCAvg'], 6, labels=range(1, 7)).astype('category')\nbank_df['Mortgage'] = pd.cut(bank_df['Mortgage'], 10, labels=range(1, 11)).astype('category')\n\n# Use one-hot-encoding for the \nX = bank_df.drop(columns=['Personal Loan'])\nX = pd.get_dummies(X)\n\ny = bank_df['Personal Loan']\n\n\ntrain_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size=0.4, random_state=1)\nprint('Training set:', train_X.shape, 'Validation set:', valid_X.shape)\n\nTraining set: (3000, 42) Validation set: (2000, 42)"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-13-ProbSolutions-Ensemble.html#solution-13.1.a",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-13-ProbSolutions-Ensemble.html#solution-13.1.a",
    "title": "Chapter 13: Combining Methods: Ensembles and Uplift Modeling",
    "section": "Solution 13.1.a",
    "text": "Solution 13.1.a\nFit models to the data for 1. logistic regression, 2. \\(k\\)-nearest neighbors with \\(k=3\\), 3. classification trees, and 4. Naive Bayes.\nUse Personal Loan as the outcome variable. Report the validation confusion matrix for each of the models.\n\n# Logistic regression\nlogit_reg = LogisticRegression(penalty=\"l2\", C=1e42, solver='liblinear')\nlogit_reg.fit(train_X, train_y)\nclassificationSummary(valid_y, logit_reg.predict(valid_X))\n\n# k-nearest neighbors\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(train_X, train_y)\nclassificationSummary(valid_y, knn.predict(valid_X))\n\nConfusion Matrix (Accuracy 0.9490)\n\n       Prediction\nActual    0    1\n     0 1773   34\n     1   68  125\nConfusion Matrix (Accuracy 0.9360)\n\n       Prediction\nActual    0    1\n     0 1798    9\n     1  119   74\n\n\n\n# classification tree\n# user grid search to find optimized tree\nparam_grid = {\n    'max_depth': [5, 10, 15, 20, 25], \n    'min_impurity_decrease': [0, 0.001, 0.005, 0.01], \n    'min_samples_split': [10, 20, 30, 40, 50], \n}\ngridSearch = GridSearchCV(DecisionTreeClassifier(random_state=1), param_grid, cv=5, n_jobs=-1)\ngridSearch.fit(train_X, train_y)\nprint('Initial parameters: ', gridSearch.best_params_)\n\nparam_grid = {\n    'max_depth': [5, 6, 7, 8, 9, 10, 11, 12], \n    'min_impurity_decrease': [0, 0.0005, 0.001, 0.0015], \n    'min_samples_split': [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], \n}\ngridSearch = GridSearchCV(DecisionTreeClassifier(random_state=1), param_grid, cv=5, n_jobs=-1)\ngridSearch.fit(train_X, train_y)\nprint('Improved parameters: ', gridSearch.best_params_)\n\nclassTree = gridSearch.best_estimator_\n\nclassificationSummary(valid_y, classTree.predict(valid_X))\n\nInitial parameters:  {'max_depth': 10, 'min_impurity_decrease': 0.001, 'min_samples_split': 10}\nImproved parameters:  {'max_depth': 7, 'min_impurity_decrease': 0, 'min_samples_split': 15}\nConfusion Matrix (Accuracy 0.9670)\n\n       Prediction\nActual    0    1\n     0 1789   18\n     1   48  145\n\n\n\n# Naive-Bayes\nnb = MultinomialNB(alpha=0.01)\nnb.fit(train_X, train_y)\nclassificationSummary(valid_y, nb.predict(valid_X))\n\nConfusion Matrix (Accuracy 0.8860)\n\n       Prediction\nActual    0    1\n     0 1657  150\n     1   78  115"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-13-ProbSolutions-Ensemble.html#solution-13.1.b",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-13-ProbSolutions-Ensemble.html#solution-13.1.b",
    "title": "Chapter 13: Combining Methods: Ensembles and Uplift Modeling",
    "section": "Solution 13.1.b",
    "text": "Solution 13.1.b\nCreate a data frame with the actual outcome, predicted outcome, and each of the models. Report the first 10 rows of this data frame.\n\nresult = pd.DataFrame({\n    'actual': valid_y,\n    'Logistic regression': logit_reg.predict(valid_X),\n    'k-nearest neighbor': knn.predict(valid_X),\n    'classification tree': classTree.predict(valid_X),\n    'naive-bayes': nb.predict(valid_X),\n})\nresult.head(10)\n\n\n\n\n\n\n\n\nactual\nLogistic regression\nk-nearest neighbor\nclassification tree\nnaive-bayes\n\n\n\n\n2764\n0\n0\n0\n0\n0\n\n\n4767\n0\n0\n0\n0\n0\n\n\n3814\n0\n0\n0\n0\n0\n\n\n3499\n0\n0\n0\n0\n0\n\n\n2735\n0\n0\n0\n0\n0\n\n\n3922\n0\n0\n0\n0\n0\n\n\n2701\n0\n0\n0\n0\n0\n\n\n1179\n0\n0\n0\n0\n0\n\n\n932\n0\n1\n0\n1\n1\n\n\n792\n0\n1\n0\n1\n1"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-13-ProbSolutions-Ensemble.html#solution-13.1.c",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-13-ProbSolutions-Ensemble.html#solution-13.1.c",
    "title": "Chapter 13: Combining Methods: Ensembles and Uplift Modeling",
    "section": "Solution 13.1.c",
    "text": "Solution 13.1.c\nAdd two columns to this data frame for (1) a majority vote of predicted outcomes, and (2) the average of the predicted probabilities. Using the classifications generated by these two methods derive a confusion matrix for each method and report the overall accuracy.\n\n# (1) majority vote\npredColumns = ['Logistic regression', 'k-nearest neighbor', 'classification tree', 'naive-bayes']\nresult['majority'] = [1 if p &gt; 0.5 else 0 for p in result[predColumns].mean(axis=1)]\n\n# (2) average probability\n# first create a data frame with the predicted probabilities for Personal Loan of 1\nprobabilities = pd.DataFrame({\n    'actual': valid_y,\n    'Logistic regression': logit_reg.predict_proba(valid_X)[:, 1],\n    'k-nearest neighbor': knn.predict_proba(valid_X)[:, 1],\n    'classification tree': classTree.predict_proba(valid_X)[:, 1],\n    'naive-bayes': nb.predict_proba(valid_X)[:, 1],\n})\nresult['average'] = probabilities[predColumns].mean(axis=1)\nresult['average_pred'] = [1 if p &gt; 0.5 else 0 for p in result['average']]\nresult.head(10)\n\n\n\n\n\n\n\n\nactual\nLogistic regression\nk-nearest neighbor\nclassification tree\nnaive-bayes\nmajority\naverage\naverage_pred\n\n\n\n\n2764\n0\n0\n0\n0\n0\n0\n1.572781e-03\n0\n\n\n4767\n0\n0\n0\n0\n0\n0\n2.846366e-07\n0\n\n\n3814\n0\n0\n0\n0\n0\n0\n6.143648e-07\n0\n\n\n3499\n0\n0\n0\n0\n0\n0\n3.452705e-02\n0\n\n\n2735\n0\n0\n0\n0\n0\n0\n3.897706e-03\n0\n\n\n3922\n0\n0\n0\n0\n0\n0\n1.440022e-06\n0\n\n\n2701\n0\n0\n0\n0\n0\n0\n8.749563e-04\n0\n\n\n1179\n0\n0\n0\n0\n0\n0\n1.418331e-01\n0\n\n\n932\n0\n1\n0\n1\n1\n1\n5.011567e-01\n1\n\n\n792\n0\n1\n0\n1\n1\n1\n6.453092e-01\n1\n\n\n\n\n\n\n\n\nprint('Majority vote')\nclassificationSummary(result['actual'], result['majority'])\n\nprint('Average probability')\nclassificationSummary(result['actual'], result['average_pred'])\n\nMajority vote\nConfusion Matrix (Accuracy 0.9495)\n\n       Prediction\nActual    0    1\n     0 1797   10\n     1   91  102\nAverage probability\nConfusion Matrix (Accuracy 0.9595)\n\n       Prediction\nActual    0    1\n     0 1796   11\n     1   70  123"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-13-ProbSolutions-Ensemble.html#solution-13.1.d",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-13-ProbSolutions-Ensemble.html#solution-13.1.d",
    "title": "Chapter 13: Combining Methods: Ensembles and Uplift Modeling",
    "section": "Solution 13.1.d",
    "text": "Solution 13.1.d\nCompare the error rates for the four individual methods and the two ensemble methods.\n\nmodels = ['Logistic regression', 'k-nearest neighbor', 'classification tree', 'naive-bayes', \n          'majority', 'average_pred']\nerrorRates = []\nfor model in models:\n    errorRates.append({'model': model, 'error rate': accuracy_score(result['actual'], result[model])})\npd.DataFrame(errorRates)\n\n\n\n\n\n\n\n\nmodel\nerror rate\n\n\n\n\n0\nLogistic regression\n0.9490\n\n\n1\nk-nearest neighbor\n0.9360\n\n\n2\nclassification tree\n0.9670\n\n\n3\nnaive-bayes\n0.8860\n\n\n4\nmajority\n0.9495\n\n\n5\naverage_pred\n0.9595\n\n\n\n\n\n\n\nThe accuracy values show that the decision tree model outperforms the other three methods. The two ensemble models have slightly lower performance compared to the classification tree."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-13-ProbSolutions-Ensemble.html#solution-13.2.a",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-13-ProbSolutions-Ensemble.html#solution-13.2.a",
    "title": "Chapter 13: Combining Methods: Ensembles and Uplift Modeling",
    "section": "Solution 13.2.a",
    "text": "Solution 13.2.a\nRun a classification tree, using the default settings of DecisionTreeClassifier. Looking at the validation set, what is the overall accuracy? What is the lift on the first decile?\n\nclassTree = DecisionTreeClassifier(random_state=1)\nclassTree.fit(train_X, train_y)\nclassificationSummary(valid_y, classTree.predict(valid_X))\n\nConfusion Matrix (Accuracy 0.8758)\n\n       Prediction\nActual   0   1\n     0 315  38\n     1  60 376\n\n\n\n# Create the information for the lift chart\nproba = classTree.predict_proba(valid_X)\nresult = pd.DataFrame({'actual': valid_y, \n                       'p(0)': [p[0] for p in proba],\n                       'p(1)': [p[1] for p in proba],\n                       'predicted': classTree.predict(valid_X) })\n\ndf = result.sort_values(by=['p(1)'], ascending=False)\nliftChart(df['p(1)'], title=False)\nplt.show()"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-13-ProbSolutions-Ensemble.html#solution-13.2.b",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-13-ProbSolutions-Ensemble.html#solution-13.2.b",
    "title": "Chapter 13: Combining Methods: Ensembles and Uplift Modeling",
    "section": "Solution 13.2.b",
    "text": "Solution 13.2.b\nRun a boosted tree with the same predictors (use AdaBoostClassifier with DecisionTreeClassifier as the base estimator). For the validation set, what is the overall accuracy? What is the lift on the first decile?\n\nboost = AdaBoostClassifier(n_estimators=100, base_estimator=classTree, random_state=1)\nboost.fit(train_X, train_y)\n\nclassificationSummary(valid_y, boost.predict(valid_X))\n\nConfusion Matrix (Accuracy 0.8619)\n\n       Prediction\nActual   0   1\n     0 312  41\n     1  68 368\n\n\n\n# Create the information for the lift chart\nproba = boost.predict_proba(valid_X)\nresult = pd.DataFrame({'actual': valid_y, \n                       'p(0)': [p[0] for p in proba],\n                       'p(1)': [p[1] for p in proba],\n                       'predicted': boost.predict(valid_X) })\n\ndf = result.sort_values(by=['p(1)'], ascending=False)\nliftChart(df['p(1)'], title=False)\nplt.show()"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-13-ProbSolutions-Ensemble.html#solution-13.2.c",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-13-ProbSolutions-Ensemble.html#solution-13.2.c",
    "title": "Chapter 13: Combining Methods: Ensembles and Uplift Modeling",
    "section": "Solution 13.2.c",
    "text": "Solution 13.2.c\nRun a bagged tree with the same predictors (use BaggingClassifier). For the validation set, what is the overall accuracy? What is the lift on the first decile?\n\nbagging = BaggingClassifier(classTree, max_samples=0.5, max_features=0.5, random_state=1)\nbagging.fit(train_X, train_y)\n\nclassificationSummary(valid_y, bagging.predict(valid_X))\n\nConfusion Matrix (Accuracy 0.8074)\n\n       Prediction\nActual   0   1\n     0 278  75\n     1  77 359\n\n\n\n# Create the information for the lift chart\nproba = bagging.predict_proba(valid_X)\nresult = pd.DataFrame({'actual': valid_y, \n                       'p(0)': [p[0] for p in proba],\n                       'p(1)': [p[1] for p in proba],\n                       'predicted': bagging.predict(valid_X) })\n\ndf = result.sort_values(by=['p(1)'], ascending=False)\nliftChart(df['p(1)'], title=False)\nplt.show()"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-13-ProbSolutions-Ensemble.html#solution-13.2.d",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-13-ProbSolutions-Ensemble.html#solution-13.2.d",
    "title": "Chapter 13: Combining Methods: Ensembles and Uplift Modeling",
    "section": "Solution 13.2.d",
    "text": "Solution 13.2.d\nRun a random forest (use RandomForestClassifier). Compare the bagged tree to the random forest in terms of validation accuracy and lift on first decile. How are the two methods conceptually different?\n\nrfModel = RandomForestClassifier(random_state=1, n_estimators=100)\nrfModel.fit(train_X, train_y)\n\nclassificationSummary(valid_y, rfModel.predict(valid_X))\n\nConfusion Matrix (Accuracy 0.8644)\n\n       Prediction\nActual   0   1\n     0 320  33\n     1  74 362\n\n\n\n# Create the information for the lift chart\nproba = rfModel.predict_proba(valid_X)\nresult = pd.DataFrame({'actual': valid_y, \n                       'p(0)': [p[0] for p in proba],\n                       'p(1)': [p[1] for p in proba],\n                       'predicted': rfModel.predict(valid_X) })\n\ndf = result.sort_values(by=['p(1)'], ascending=False)\nliftChart(df['p(1)'], title=False)\nplt.show()"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-13-ProbSolutions-Ensemble.html#data-preprocessing-1",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-13-ProbSolutions-Ensemble.html#data-preprocessing-1",
    "title": "Chapter 13: Combining Methods: Ensembles and Uplift Modeling",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\nTransform variable day of week info a categorical variable. Bin the scheduled departure time into eight bins (in Python use function pd.cut() from the pandas package). Partition the data into training (60%) and validation (40%).\nRun a boosted classification tree for delay. With the exception of setting and , use default setting for the DecisionTreeClassifier and the AdaBoostClassifier.\nCompared with the single tree, how does the boosted tree behave in terms of overall accuracy? Compared with the single tree, how does the boosted tree behave in terms of accuracy in identifying delayed flights? Explain why this model might have the best performance over the other models you fit.\n\ndata = pd.read_csv(DATA / 'FlightDelays.csv')\ndata = data.drop(columns=['FL_DATE', 'FL_NUM', 'TAIL_NUM', 'DEP_TIME', 'DAY_OF_MONTH'])\n\n# transform variables and create bins\ndata.DAY_WEEK = data.DAY_WEEK.astype('category')\ndata.CRS_DEP_TIME = pd.cut(data.CRS_DEP_TIME, bins=8)\ndata.DEST = data.DEST.astype('category')\ndata.ORIGIN = data.ORIGIN.astype('category')\n\ndata = pd.get_dummies(data, drop_first=True, columns=['DAY_WEEK', 'CRS_DEP_TIME', 'DEST', 'ORIGIN', 'CARRIER'])\ndata.head()\n\n\n\n\n\n\n\n\nDISTANCE\nWeather\nFlight Status\nDAY_WEEK_2\nDAY_WEEK_3\nDAY_WEEK_4\nDAY_WEEK_5\nDAY_WEEK_6\nDAY_WEEK_7\nCRS_DEP_TIME_(791.25, 982.5]\n...\nDEST_LGA\nORIGIN_DCA\nORIGIN_IAD\nCARRIER_DH\nCARRIER_DL\nCARRIER_MQ\nCARRIER_OH\nCARRIER_RU\nCARRIER_UA\nCARRIER_US\n\n\n\n\n0\n184\n0\nontime\n0\n0\n1\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\n1\n213\n0\nontime\n0\n0\n1\n0\n0\n0\n0\n...\n0\n1\n0\n1\n0\n0\n0\n0\n0\n0\n\n\n2\n229\n0\nontime\n0\n0\n1\n0\n0\n0\n0\n...\n1\n0\n1\n1\n0\n0\n0\n0\n0\n0\n\n\n3\n229\n0\nontime\n0\n0\n1\n0\n0\n0\n0\n...\n1\n0\n1\n1\n0\n0\n0\n0\n0\n0\n\n\n4\n229\n0\nontime\n0\n0\n1\n0\n0\n0\n0\n...\n1\n0\n1\n1\n0\n0\n0\n0\n0\n0\n\n\n\n\n5 rows × 27 columns\n\n\n\nPartition the data into training (60%) and validation (40%).\n\ny = data['Flight Status']\nX = data.drop(columns=['Flight Status'])\n\n# partition data\ntrain_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size=0.4, random_state=1)\n\nRun a boosted classification tree for delay. With the exception of setting n_estimators=500 and random_state=1, use default setting for the DecisionTreeClassifier and the AdaBoostClassifier.\n\ndefaultOptions = {'random_state': 1}\n\ndefaultTree = DecisionTreeClassifier(**defaultOptions)\ndefaultTree.fit(train_X, train_y)\nclassificationSummary(valid_y, defaultTree.predict(valid_X), class_names=['delayed', 'ontime'])\n\nConfusion Matrix (Accuracy 0.7310)\n\n        Prediction\n Actual delayed  ontime\ndelayed      62     105\n ontime     132     582\n\n\n\nboost = AdaBoostClassifier(DecisionTreeClassifier(**defaultOptions), random_state=1, n_estimators=500)\n\nboost.fit(train_X, train_y)\nclassificationSummary(valid_y, boost.predict(valid_X), class_names=['delayed', 'ontime'])\n\nConfusion Matrix (Accuracy 0.7401)\n\n        Prediction\n Actual delayed  ontime\ndelayed      37     130\n ontime      99     615"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-13-ProbSolutions-Ensemble.html#solution-13.3.a",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-13-ProbSolutions-Ensemble.html#solution-13.3.a",
    "title": "Chapter 13: Combining Methods: Ensembles and Uplift Modeling",
    "section": "Solution 13.3.a",
    "text": "Solution 13.3.a\nCompared with the single tree, how does the boosted tree behave in terms of overall accuracy?\nThe accuracy of the single tree is 0.73 and it is slightly higher (0.74) for the boosted tree."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-13-ProbSolutions-Ensemble.html#solution-13.3.b",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-13-ProbSolutions-Ensemble.html#solution-13.3.b",
    "title": "Chapter 13: Combining Methods: Ensembles and Uplift Modeling",
    "section": "Solution 13.3.b",
    "text": "Solution 13.3.b\nCompared with the single tree, how does the boosted tree behave in terms of accuracy in identifying delayed flights?\n\nprint('single tree: ', 62 / (62 + 105))\nprint('boosted tree: ', 37 / (37 + 130))\n\nsingle tree:  0.3712574850299401\nboosted tree:  0.2215568862275449\n\n\nThe single tree predicts 37% of the delayed flights correct. The boosted tree only 22%."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-13-ProbSolutions-Ensemble.html#solution-13.3.c",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-13-ProbSolutions-Ensemble.html#solution-13.3.c",
    "title": "Chapter 13: Combining Methods: Ensembles and Uplift Modeling",
    "section": "Solution 13.3.c",
    "text": "Solution 13.3.c\nExplain why this model might have the best performance over the other models you fit.\nBoosting is an iterative procedure that develops successive trees with different weights applied to the records in each iteration. It often yields superior performance over single trees or bagged trees by concentrating (via the weights) on the records which were misclassified in earlier iterations.\nIn our case, the overall accuracy of the single tree is practically identical to the boosted tree, however the accuracy in identifying delayed flights is higher."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-13-ProbSolutions-Ensemble.html#additional-material-13.3",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-13-ProbSolutions-Ensemble.html#additional-material-13.3",
    "title": "Chapter 13: Combining Methods: Ensembles and Uplift Modeling",
    "section": "Additional material 13.3",
    "text": "Additional material 13.3\nIn the above solution, we didn’t control the complexity of the model. We can use the arguments max_depth and min_samples_split to limit the size of the trees.\n\ndefaultOptions = {'random_state': 1, 'max_depth': 6, 'min_samples_split': 3, }\n\ndefaultTree = DecisionTreeClassifier(**defaultOptions)\ndefaultTree.fit(train_X, train_y)\nclassificationSummary(valid_y, defaultTree.predict(valid_X), class_names=['delayed', 'ontime'])\n\nboost = AdaBoostClassifier(DecisionTreeClassifier(**defaultOptions), random_state=1, n_estimators=500)\nboost.fit(train_X, train_y)\nclassificationSummary(valid_y, boost.predict(valid_X), class_names=['delayed', 'ontime'])\n\nConfusion Matrix (Accuracy 0.8229)\n\n        Prediction\n Actual delayed  ontime\ndelayed      33     134\n ontime      22     692\nConfusion Matrix (Accuracy 0.7514)\n\n        Prediction\n Actual delayed  ontime\ndelayed      39     128\n ontime      91     623\n\n\n\nprint('single tree: ', 33 / (33 + 134))\nprint('boosted tree: ', 39 / (39 + 128))\n\nsingle tree:  0.19760479041916168\nboosted tree:  0.23353293413173654\n\n\nThe overall accuracy of the single tree is now 0.83 while it is practically unchanged for the boosted trees (0.75). The accuracy for predicting delayed flights on the other hand drops significantly for the single tree (20%), while it improves slightly for the boosted tree (23%)."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-13-ProbSolutions-Ensemble.html#solution-13.4.a",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-13-ProbSolutions-Ensemble.html#solution-13.4.a",
    "title": "Chapter 13: Combining Methods: Ensembles and Uplift Modeling",
    "section": "Solution 13.4.a",
    "text": "Solution 13.4.a\nWhat is the purchase propensity ### 13.4.a.i among those who received the promotion? &gt; The purchase propensity among those who received the promotion = (Number of purchases made) / (Total Number of Records)\nThe Promotion_ord column has a 1 for members that received a promotion. The Purchase column has a 1 for members that purchased a product after the promotion. By multiplying the two columns we therefore get a 1 for all members that received a promotion and made a purchase and a 0 otherwise.\n\n# Number of purchases made for members that got a promotion\nprint(\"Number of purchases made\", sum(data['Purchase'] * data['Promotion_ord']))\n\n# Number of members that got a promotion\nprint(\"Total Number of Records\", sum(data['Promotion_ord']))\n\n# Purchase propensity\nprint(\"80 / 4976 = \", sum(data['Purchase'] * data['Promotion_ord']) / sum(data['Promotion_ord']))\n\nNumber of purchases made 80\nTotal Number of Records 4976\n80 / 4976 =  0.01607717041800643\n\n\n\n13.4.a.ii\namong those who did not receive the promotion?\n\n# Number of purchases made for members that got a promotion\nprint(\"Number of purchases made\", sum(data['Purchase'] * (1 - data['Promotion_ord'])))\n\n# Number of members that got a promotion\nprint(\"Total Number of Records\", sum(1 - data['Promotion_ord']))\n\n# Purchase propensity\nprint(\"32 / 5024 = \", sum(data['Purchase'] * (1 - data['Promotion_ord'])) / sum(1 - data['Promotion_ord']))\n\nNumber of purchases made 32\nTotal Number of Records 5024\n32 / 5024 =  0.006369426751592357"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-13-ProbSolutions-Ensemble.html#solution-13.4.b",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-13-ProbSolutions-Ensemble.html#solution-13.4.b",
    "title": "Chapter 13: Combining Methods: Ensembles and Uplift Modeling",
    "section": "Solution 13.4.b",
    "text": "Solution 13.4.b\nPartition the data into training (60%) and validation (40%)\n\ndata.Hair_Color = data.Hair_Color.astype('category')\ndata.U_S_Region = data.U_S_Region.astype('category')\ndata = pd.get_dummies(data, drop_first=True)\ndata.head()\n# KNeighborsClassifier(n_neighbors=row.k).fit(trainNorm[['zIncome', 'zLot_Size']], trainNorm['Ownership'])\n\n\n\n\n\n\n\n\nPurchase\nAge\nValidation\nPromotion_ord\nGender_ord\nResidence_ord\nHair_Color_Blond\nHair_Color_Brown\nHair_Color_Red\nU_S_Region_Northwest\nU_S_Region_Southeast\nU_S_Region_Southwest\n\n\n\n\n0\n0\n25\n1\n1\n0\n1\n0\n0\n0\n0\n0\n1\n\n\n1\n0\n30\n1\n0\n0\n1\n0\n0\n0\n1\n0\n0\n\n\n2\n0\n45\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\n3\n0\n35\n0\n0\n0\n1\n1\n0\n0\n0\n0\n1\n\n\n4\n0\n33\n0\n1\n0\n1\n0\n1\n0\n0\n0\n1\n\n\n\n\n\n\n\n\ny = data['Purchase']\nX = data.drop(columns=['Purchase'])\n\n# Standardize the dataset\nscaler = preprocessing.StandardScaler()\nX_norm = scaler.fit_transform(X * 1.0)\n\ndata_norm = pd.concat([pd.DataFrame(X_norm, columns=data.columns[1:]),\n                       data['Purchase']], axis=1)\ntrain, valid = train_test_split(data_norm, test_size=0.4, random_state=1)\n\n\nSolution 13.4.b.i\nUplift using a Random Forest.\n\nrfModel = RandomForestClassifier(n_estimators=100)\nrfModel.fit(train.drop(columns=['Purchase']), train.Purchase)\n\npred = rfModel.predict(valid.drop(columns=['Purchase']))\nclassificationSummary(valid.Purchase, pred)\n\nConfusion Matrix (Accuracy 0.9840)\n\n       Prediction\nActual    0    1\n     0 3936   12\n     1   52    0\n\n\n\nuplift_df = valid.drop(columns=['Purchase']).copy()  # Need to create a copy to allow modifying data\n\nuplift_df.Promotion_ord = 1\npredTreatment = rfModel.predict_proba(uplift_df)\nuplift_df.Promotion_ord = 0\npredControl = rfModel.predict_proba(uplift_df)\n\nupliftResult_rf = pd.DataFrame({\n    'probMessage': predTreatment[:,1],\n    'probNoMessage': predControl[:,1],\n    'uplift': predTreatment[:,1] - predControl[:,1],\n    }, index=uplift_df.index)\nupliftResult_rf = upliftResult_rf.sort_values(by=['uplift'], ascending=False)\nupliftResult_rf.reset_index().plot(x=None, y='uplift')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nSolution 13.4.b.ii\nUplift using \\(k\\)-NN.\n\nknnModel = KNeighborsClassifier(n_neighbors=11)\nknnModel.fit(train.drop(columns=['Purchase']), train.Purchase)\n\npred = knnModel.predict(valid.drop(columns=['Purchase']))\nclassificationSummary(valid.Purchase, pred)\n\nConfusion Matrix (Accuracy 0.9870)\n\n       Prediction\nActual    0    1\n     0 3948    0\n     1   52    0\n\n\n\nuplift_df = valid.drop(columns=['Purchase']).copy()  # Need to create a copy to allow modifying data\n\nuplift_df.Promotion_ord = 1\npredTreatment = knnModel.predict_proba(uplift_df)\nuplift_df.Promotion_ord = 0\npredControl = knnModel.predict_proba(uplift_df)\n\nupliftResult_knn = pd.DataFrame({\n    'probMessage': predTreatment[:,1],\n    'probNoMessage': predControl[:,1],\n    'uplift': predTreatment[:,1] - predControl[:,1],\n    }, index=uplift_df.index)\nupliftResult_knn = upliftResult_knn.sort_values(by=['uplift'], ascending=False)\nupliftResult_knn.reset_index().plot(x=None, y='uplift')\nplt.show()"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-13-ProbSolutions-Ensemble.html#solution-13.4.c",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-13-ProbSolutions-Ensemble.html#solution-13.4.c",
    "title": "Chapter 13: Combining Methods: Ensembles and Uplift Modeling",
    "section": "Solution 13.4.c",
    "text": "Solution 13.4.c\nReport the two models’ recommendations for the first three members.\n\nupliftResult_rf.head(3)\n\n\n\n\n\n\n\n\nprobMessage\nprobNoMessage\nuplift\n\n\n\n\n6751\n0.723333\n0.01\n0.713333\n\n\n201\n0.723333\n0.01\n0.713333\n\n\n3380\n0.730000\n0.04\n0.690000\n\n\n\n\n\n\n\n\nupliftResult_knn.head(3)\n\n\n\n\n\n\n\n\nprobMessage\nprobNoMessage\nuplift\n\n\n\n\n7317\n0.181818\n0.0\n0.181818\n\n\n7451\n0.181818\n0.0\n0.181818\n\n\n9520\n0.181818\n0.0\n0.181818"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-16-ProbSolutions-TS.html",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-16-ProbSolutions-TS.html",
    "title": "Chapter 16: Handling Time Series",
    "section": "",
    "text": "2019-2020 Galit Shmueli, Peter C. Bruce, Peter Gedeck\n\nData Mining for Business Analytics: Concepts, Techniques, and Applications in Python (First Edition) Galit Shmueli, Peter C. Bruce, Peter Gedeck, and Nitin R. Patel. 2019.\nDate: 2020-03-08\nPython Version: 3.8.2 Jupyter Notebook Version: 5.6.1\nPackages: - matplotlib: 3.2.0 - pandas: 1.0.1 - statsmodels: 0.11.1\nThe assistance from Mr. Kuber Deokar and Ms. Anuja Kulkarni in preparing these solutions is gratefully acknowledged.\n# Import required packages for this chapter\nfrom pathlib import Path\nimport warnings\n\nimport pandas as pd\nimport statsmodels.formula.api as sm\nfrom statsmodels.tsa import tsatools\n\nimport matplotlib.pylab as plt\nimport matplotlib.gridspec as gridspec\n\n%matplotlib inline\n# Working directory:\n#\n# We assume that data are kept in the same directory as the notebook. If you keep your \n# data in a different folder, replace the argument of the `Path`\nDATA = Path('.')\n# and then load data using \n#\n# pd.read_csv(DATA / ‘filename.csv’)"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-16-ProbSolutions-TS.html#solution-16.1.a",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-16-ProbSolutions-TS.html#solution-16.1.a",
    "title": "Chapter 16: Handling Time Series",
    "section": "Solution 16.1.a",
    "text": "Solution 16.1.a\nIs the goal of this study descriptive or predictive?\nThe goal of the study is exploratory"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-16-ProbSolutions-TS.html#solution-16.1.b",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-16-ProbSolutions-TS.html#solution-16.1.b",
    "title": "Chapter 16: Handling Time Series",
    "section": "Solution 16.1.b",
    "text": "Solution 16.1.b\nPlot each of the three pre-event time series (Air, Rail, Car).\n\ndf = pd.read_csv(DATA / 'Sept11Travel.csv')\n\n# convert the date information to a datetime object\ndf['Date'] = pd.to_datetime(df.Month, format='%b-%y')\n\nair_ts = pd.Series(df['Air RPM (000s)'].values, index=df.Date, name='Air')\nrail_ts = pd.Series(df['Rail PM'].values, index=df.Date, name='Rail')\ncar_ts = pd.Series(df['VMT (billions)'].values, index=df.Date, name='Car')\n\npre_air_ts = air_ts[:'2001-08-31']\npost_air_ts = air_ts['2001-08-31':]\npre_rail_ts = rail_ts[:'2001-08-31']\npost_rail_ts = rail_ts['2001-08-31':]\npre_car_ts = car_ts[:'2001-08-31']\npost_car_ts = car_ts['2001-08-31':]\n\ndef createGraphs():\n    fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(10,9))\n    pre_air_ts.plot(ax=axes[0], title='Air travel')\n    # post_air_ts.plot(ax=axes[0])\n    axes[0].set_ylabel('Air RPM (000s)')\n\n    pre_rail_ts.plot(ax=axes[1], title='Rail travel')\n    # post_rail_ts.plot(ax=axes[1])\n    axes[1].set_ylabel('Rail PM')\n\n    pre_car_ts.plot(ax=axes[2], title='Car travel')\n    # post_car_ts.plot(ax=axes[2])\n    axes[2].set_ylabel('VMT (billions)')\n\n    for ax in axes:\n        ax.axvline(x='2001-09-11', ymin=0, ymax=1, linewidth=0.5)\n    return axes\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    axes = createGraphs()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nSolution 16.1.b.i\nWhat time series components appear from the plot?\nLevel, seasonality, trend, and noise are present in all 3 series. In the air series, there is a big drop in September.\n\n\nSolution 16.1.b.ii\nWhat type of trend appears? Change the scale of the series, add trendlines and suppress seasonality to better visualize the trend pattern.\n\n# Use logarithmic scale for y-axis\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    createGraphs()\n\nplt.yscale('log')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\ndef addTrend(ts, outcome, ax):\n    df = tsatools.add_trend(ts, trend='ctt')\n    # fit a linear regression model to the time series\n    model = sm.ols(formula=f'{outcome} ~ trend + trend_squared', data=df).fit()\n    model.predict(df).plot(ax=ax)\n    \nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    axes = createGraphs()\n    addTrend(pre_air_ts, 'Air', axes[0])\n    addTrend(pre_rail_ts, 'Rail', axes[1])\n    addTrend(pre_car_ts, 'Car', axes[2])\n\nplt.tight_layout()\n\n\n\n\n\n\n\n\nBoth Air and Vehicle show increasing linear trends, visible even without suppressing seasonality. Rail shows a more complex trend, initially steady, then declining, then rising. Separate linear trends might also be fit to each of the 3 periods. Shifting to a logarithmic scale does not add information."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-16-ProbSolutions-TS.html#solution-16.2.a",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-16-ProbSolutions-TS.html#solution-16.2.a",
    "title": "Chapter 16: Handling Time Series",
    "section": "Solution 16.2.a",
    "text": "Solution 16.2.a\nWhich model appears more useful for explaining the different components of this time series? Why?\nA - the RMSE is much lower than for B, based on the training data (which are the bulk of the data)."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-16-ProbSolutions-TS.html#solution-16.2.b",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-16-ProbSolutions-TS.html#solution-16.2.b",
    "title": "Chapter 16: Handling Time Series",
    "section": "Solution 16.2.b",
    "text": "Solution 16.2.b\nWhich model appears to be more useful for forecasting purposes? Why?\nB - the validation error, an estimate of predictive accuracy, is lower for model B than for model A."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-16-ProbSolutions-TS.html#solution-16.3.a",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-16-ProbSolutions-TS.html#solution-16.3.a",
    "title": "Chapter 16: Handling Time Series",
    "section": "Solution 16.3.a",
    "text": "Solution 16.3.a\nCreate a well-formatted time plot of the data.\n\ndf = pd.read_csv(DATA / 'DepartmentStoreSales.csv')\ndf_ts = pd.Series(df['Sales'].values, index=df.Quarter)\nax = df_ts.plot(title='Time plot of quarterly sales')\nax.set_ylabel('Sales')\nplt.show()"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-16-ProbSolutions-TS.html#solution-16.3.b",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-16-ProbSolutions-TS.html#solution-16.3.b",
    "title": "Chapter 16: Handling Time Series",
    "section": "Solution 16.3.b",
    "text": "Solution 16.3.b\nWhich of the four components (level, trend, seasonality, noise) seem to be present in this series?\nAll four components seem to be present (the noise component is relatively small)."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-16-ProbSolutions-TS.html#solution-16.4.a",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-16-ProbSolutions-TS.html#solution-16.4.a",
    "title": "Chapter 16: Handling Time Series",
    "section": "Solution 16.4.a",
    "text": "Solution 16.4.a\nCreate a well-formatted time plot of the data.\n\ndf = pd.read_csv(DATA / 'ApplianceShipments.csv')\n\n# convert Quarter information into month-year\nfor q, m in ('Q1', '03'), ('Q2', '06'), ('Q3', '09'), ('Q4', '12'):\n    df['Quarter'] = df['Quarter'].str.replace(q, m)\ndf['Quarter'] = pd.to_datetime(df.Quarter, format='%m-%Y')\n\ndf_ts = pd.Series(df['Shipments'].values, index=df.Quarter, name='shipments')\nax = df_ts.plot(title='Time plot of quarterly shipments (in million $) of US household appliances')\nax.set_ylabel('Shipments')\nplt.show()"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-16-ProbSolutions-TS.html#solution-16.4.b",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-16-ProbSolutions-TS.html#solution-16.4.b",
    "title": "Chapter 16: Handling Time Series",
    "section": "Solution 16.4.b",
    "text": "Solution 16.4.b\nWhich of the four components (level, trend, seasonality, noise) seem to be present in this series?\nLevel, seasonality and noise are present (and possibly a slight upward trend)."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-16-ProbSolutions-TS.html#solution-16.5.a",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-16-ProbSolutions-TS.html#solution-16.5.a",
    "title": "Chapter 16: Handling Time Series",
    "section": "Solution 16.5.a",
    "text": "Solution 16.5.a\nReproduce the time plot.\n\ndf = pd.read_csv(DATA / 'CanadianWorkHours.csv')\ndf_ts = pd.Series(df['Hours'].values, index=pd.to_datetime(df.Year, format='%Y'), name='shipments')\nax = df_ts.plot(title='Time plot of Canadian Manufacturing Workers Workhours')\nax.set_ylabel('Hours per Week')\n\nplt.show()"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-16-ProbSolutions-TS.html#solution-16.5.b",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-16-ProbSolutions-TS.html#solution-16.5.b",
    "title": "Chapter 16: Handling Time Series",
    "section": "Solution 16.5.b",
    "text": "Solution 16.5.b\nWhich of the four components (level, trend, seasonality, noise) seem to be present in this series?\n\nLevel (no dominant level)\nTrend (yes - down, then up)\nSeasonality (no)\nNoise (yes)"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-16-ProbSolutions-TS.html#solution-16.6.a",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-16-ProbSolutions-TS.html#solution-16.6.a",
    "title": "Chapter 16: Handling Time Series",
    "section": "Solution 16.6.a",
    "text": "Solution 16.6.a\nCreate a well-formatted time plot of the data.\n\ndf = pd.read_csv(DATA / 'SouvenirSales.csv')\ndf_ts = pd.Series(df['Sales'].values, index=pd.to_datetime(df.Date, format='%b-%y'), name='souvenir')\nax = df_ts.plot(title='Time plot of monthly sales for a souvenir shop')\nax.set_ylabel('Sales')\nplt.show()"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-16-ProbSolutions-TS.html#solution-16.6.b",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-16-ProbSolutions-TS.html#solution-16.6.b",
    "title": "Chapter 16: Handling Time Series",
    "section": "Solution 16.6.b",
    "text": "Solution 16.6.b\nChange the scale on the x-axis, or on the y-axis, or on both to log-scale in order to achieve a linear relationship. Select the time plot that seems most linear.\n\ntrain_ts = df_ts[:'2001-01-01']\nvalid_ts = df_ts['2001-01-01':]\n\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(10,3))\n\nax = train_ts.plot(ax=axes[0], title='x-log')\nvalid_ts.plot(ax=ax)\nax.set_xscale('log')\nax.set_ylabel('Sales')\n\nax = train_ts.plot(ax=axes[1], title='y-log')\nvalid_ts.plot(ax=ax)\nax.set_yscale('log')\nax.set_ylabel('Sales')\n\nax = train_ts.plot(ax=axes[2], title='x-log, y-log')\nvalid_ts.plot(ax=ax)\nax.set_xscale('log')\nax.set_yscale('log')\nax.set_ylabel('Sales')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nPlotting log on the y-axis produces the most linear plot."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-16-ProbSolutions-TS.html#solution-16.6.c",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-16-ProbSolutions-TS.html#solution-16.6.c",
    "title": "Chapter 16: Handling Time Series",
    "section": "Solution 16.6.c",
    "text": "Solution 16.6.c\nComparing the two time plots, what can be said about the type of trend in the data?\nSince the log plot comes out linear, we can conclude that the actual trend in the data is exponential."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-16-ProbSolutions-TS.html#solution-16.6.d",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-16-ProbSolutions-TS.html#solution-16.6.d",
    "title": "Chapter 16: Handling Time Series",
    "section": "Solution 16.6.d",
    "text": "Solution 16.6.d\nWhy were the data partitioned? Partition the data into the training and validation set as explained above.\nThe validation partition provides a benchmark against which to test predictions, which is important because the goal of this effort is to forecast future sales.\n\n# Generate the naive and seasonal naive forecast\nnaive_pred = pd.Series(train_ts[-1], index=valid_ts.index)\nlast_season = train_ts[-12:]\nseasonal_pred = pd.Series(pd.concat([last_season]*5)[:len(valid_ts)].values, index=valid_ts.index)\n\nax = train_ts.plot(title='Time plot log of monthly sales for a souvenir shop')\nvalid_ts.plot(ax=ax)\nax.set_yscale('log')\nax.set_ylabel('Sales')\n\nnaive_pred.plot(ax=ax, color='green')\nseasonal_pred.plot(ax=ax, color='orange')\nplt.show()"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-16-ProbSolutions-TS.html#solution-16.7.a",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-16-ProbSolutions-TS.html#solution-16.7.a",
    "title": "Chapter 16: Handling Time Series",
    "section": "Solution 16.7.a",
    "text": "Solution 16.7.a\nCreate a well-formatted time plot of the data.\n\ndf = pd.read_csv(DATA / 'ShampooSales.csv')\ndf_ts = pd.Series(df['Shampoo Sales'].values, index=pd.to_datetime(df.Month, format='%b-%y'), name='souvenir')\nax = df_ts.plot(title='Time plot of monthly sales of a certain shampoo\\n over a 3 year period')\nax.set_xlabel('Date')\nax.set_ylabel('Sales')\nplt.show()\n\n\n\n\n\n\n\n\n\ndf = pd.read_csv(DATA / 'ShampooSales.csv')\ndf_ts = pd.Series(df['Shampoo Sales'].values, index=pd.to_datetime(df.Month, format='%b-%y'), name='souvenir')\nax = df_ts.plot(title='Time plot of log of monthly sales of a certain shampoo\\n over a 3 year period')\nax.set_xlabel('Date')\nax.set_ylabel('Sales')\nax.set_yscale('log')\nplt.show()"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-16-ProbSolutions-TS.html#solution-16.7.b",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-16-ProbSolutions-TS.html#solution-16.7.b",
    "title": "Chapter 16: Handling Time Series",
    "section": "Solution 16.7.b",
    "text": "Solution 16.7.b\nWhich of the four components (level, trend, seasonality, noise) seem to be present in this series?\nLevel, trend, noise"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-16-ProbSolutions-TS.html#solution-16.7.c",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-16-ProbSolutions-TS.html#solution-16.7.c",
    "title": "Chapter 16: Handling Time Series",
    "section": "Solution 16.7.c",
    "text": "Solution 16.7.c\nDo you expect to see seasonality in sales of shampoo? Why?\nNo - the need to wash hair is not cyclical."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-16-ProbSolutions-TS.html#solution-16.7.d",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-16-ProbSolutions-TS.html#solution-16.7.d",
    "title": "Chapter 16: Handling Time Series",
    "section": "Solution 16.7.d",
    "text": "Solution 16.7.d\nIf the goal is forecasting sales in future months, which of the following steps should be taken?\n\nPartition the data into training and validation sets\n\nYes\n\nTweak the model parameters to obtain good fit to the validation data\n\nYes (but not to an extent that would result in overfitting)\n\nLook at MAPE and RMSE values for the training set\n\nYes, but this assessment is secondary to the assessment for the validation set.\n\nLook at MAPE and RMSE values for the validation set\n\nYes"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-11-ProbSolutions-NN.html",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-11-ProbSolutions-NN.html",
    "title": "Chapter 11: Neural nets (NN)",
    "section": "",
    "text": "2019-2020 Galit Shmueli, Peter C. Bruce, Peter Gedeck\n\nData Mining for Business Analytics: Concepts, Techniques, and Applications in Python (First Edition) Galit Shmueli, Peter C. Bruce, Peter Gedeck, and Nitin R. Patel. 2019.\nDate: 2020-03-08\nPython Version: 3.8.2 Jupyter Notebook Version: 5.6.1\nPackages: - dmba: 0.0.12 - matplotlib: 3.2.0 - numpy: 1.18.1 - pandas: 1.0.1 - scikit-learn: 0.22.2\nThe assistance from Mr. Kuber Deokar and Ms. Anuja Kulkarni in preparing these solutions is gratefully acknowledged.\n# Import required packages for this chapter\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.neural_network import MLPClassifier, MLPRegressor\n\nimport matplotlib.pylab as plt\n\nfrom dmba import regressionSummary\n\n%matplotlib inline\n# Working directory:\n#\n# We assume that data are kept in the same directory as the notebook. If you keep your \n# data in a different folder, replace the argument of the `Path`\nDATA = Path('.')\n# and then load data using \n#\n# pd.read_csv(DATA / ‘filename.csv’)"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-11-ProbSolutions-NN.html#data-preprocessing",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-11-ProbSolutions-NN.html#data-preprocessing",
    "title": "Chapter 11: Neural nets (NN)",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\n\n# load the data\ncar_df = pd.read_csv(DATA / 'ToyotaCorolla.csv')\n# select the suggested variables\nselected_var = ['Price', 'Age_08_04', 'KM', 'Fuel_Type', 'HP', 'Automatic', 'Doors', 'Quarterly_Tax', \n                'Mfr_Guarantee', 'Guarantee_Period', 'Airco', 'Automatic_airco', 'CD_Player', \n                'Powered_Windows', 'Sport_Model', 'Tow_Bar']\ncar_df = car_df[selected_var]\n\n\n# convert the categorical data into dummy variables\ncategorical_var = ['Fuel_Type']\ncar_df = pd.get_dummies(car_df, columns=['Fuel_Type'], drop_first=True)\n\n# separate out predictors and response variables\nX_df = car_df.drop(columns=['Price'])\nY_df = car_df[ ['Price'] ]\n\n\n# normalize the data \nscaleInput = MinMaxScaler()\nscaleOutput = MinMaxScaler()\nX = scaleInput.fit_transform(X_df)\ny = scaleOutput.fit_transform(Y_df)\n\n\n# partition data\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.4, random_state=12345)\nX_train.shape, X_valid.shape, y_train.shape, y_valid.shape\n\n((861, 16), (575, 16), (861, 1), (575, 1))\n\n\n9.3.a.i. What happens to the RMS error for the training data as the number of layers and nodes increases?\n\nSingle layer with 2 nodes\n\n# train neural network with 2 hidden nodes\ncar_nnet = MLPRegressor(hidden_layer_sizes=(2), activation='logistic', solver='lbfgs', random_state=1)\ncar_nnet.fit(X_train, y_train.ravel())\n\nMLPRegressor(activation='logistic', alpha=0.0001, batch_size='auto', beta_1=0.9,\n             beta_2=0.999, early_stopping=False, epsilon=1e-08,\n             hidden_layer_sizes=2, learning_rate='constant',\n             learning_rate_init=0.001, max_fun=15000, max_iter=200,\n             momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n             power_t=0.5, random_state=1, shuffle=True, solver='lbfgs',\n             tol=0.0001, validation_fraction=0.1, verbose=False,\n             warm_start=False)\n\n\n\n# RMSE for training set\nprint('Training data (2)')\ny_actual = scaleOutput.inverse_transform(y_train).ravel()\ny_pred = scaleOutput.inverse_transform([car_nnet.predict(X_train)]).ravel()\nregressionSummary(y_pred, y_actual)\n\n# RMSE for validation set\nprint('\\nValidation data (2)')\ny_actual = scaleOutput.inverse_transform(y_valid).ravel()\ny_pred = scaleOutput.inverse_transform([car_nnet.predict(X_valid)]).ravel()\nregressionSummary(y_pred, y_actual)\n\n# plot predicted vs actual values\nax = pd.DataFrame({'actual': y_actual, 'predicted': y_pred}).plot.scatter(x='actual', y='predicted', alpha=0.5)\nplt.plot([5000, 30000], [5000, 30000], color='C1')\nplt.show()\n\nTraining data (2)\n\nRegression statistics\n\n                      Mean Error (ME) : 1.4720\n       Root Mean Squared Error (RMSE) : 1067.2968\n            Mean Absolute Error (MAE) : 798.5035\n          Mean Percentage Error (MPE) : 0.0232\nMean Absolute Percentage Error (MAPE) : 7.7100\n\nValidation data (2)\n\nRegression statistics\n\n                      Mean Error (ME) : -76.9962\n       Root Mean Squared Error (RMSE) : 1134.6333\n            Mean Absolute Error (MAE) : 851.7910\n          Mean Percentage Error (MPE) : -0.9009\nMean Absolute Percentage Error (MAPE) : 8.3696\n\n\n\n\n\n\n\n\n\n\n\nSingle layer with 5 nodes\n\n# train neural network with 5 hidden nodes\ncar_nnet = MLPRegressor(hidden_layer_sizes=(5), activation='logistic', solver='lbfgs', random_state=1)\ncar_nnet.fit(X_train, y_train.ravel())\n\n# RMSE for training set\nprint('Training data (5)')\ny_actual = scaleOutput.inverse_transform(y_train).ravel()\ny_pred = scaleOutput.inverse_transform([car_nnet.predict(X_train)]).ravel()\nregressionSummary(y_pred, y_actual)\n\n# RMSE for validation set\nprint('\\nValidation data (5)')\ny_actual = scaleOutput.inverse_transform(y_valid).ravel()\ny_pred = scaleOutput.inverse_transform([car_nnet.predict(X_valid)]).ravel()\nregressionSummary(y_pred, y_actual)\n\n# plot predicted vs actual values\nax = pd.DataFrame({'actual': y_actual, 'predicted': y_pred}).plot.scatter(x='actual', y='predicted', alpha=0.5)\nplt.plot([5000, 30000], [5000, 30000], color='C1')\nplt.show()\n\nTraining data (5)\n\nRegression statistics\n\n                      Mean Error (ME) : 0.4100\n       Root Mean Squared Error (RMSE) : 1035.9503\n            Mean Absolute Error (MAE) : 785.5496\n          Mean Percentage Error (MPE) : 0.0534\nMean Absolute Percentage Error (MAPE) : 7.6386\n\nValidation data (5)\n\nRegression statistics\n\n                      Mean Error (ME) : -86.4744\n       Root Mean Squared Error (RMSE) : 1143.4845\n            Mean Absolute Error (MAE) : 859.6481\n          Mean Percentage Error (MPE) : -1.0071\nMean Absolute Percentage Error (MAPE) : 8.5108\n\n\n\n\n\n\n\n\n\n\n\nTwo layer with 5 nodes each\n\n# train neural network with 2 layes with 5 hidden nodes each\ncar_nnet = MLPRegressor(hidden_layer_sizes=(5, 5), activation='logistic', solver='lbfgs', random_state=1)\ncar_nnet.fit(X_train, y_train.ravel())\n\n# RMSE for training set\nprint('Training data (5, 5)')\ny_actual = scaleOutput.inverse_transform(y_train).ravel()\ny_pred = scaleOutput.inverse_transform([car_nnet.predict(X_train)]).ravel()\nregressionSummary(y_pred, y_actual)\n\n# RMSE for validation set\nprint('\\nValidation data (5, 5)')\ny_actual = scaleOutput.inverse_transform(y_valid).ravel()\ny_pred = scaleOutput.inverse_transform([car_nnet.predict(X_valid)]).ravel()\nregressionSummary(y_pred, y_actual)\n\n# plot predicted vs actual values\nax = pd.DataFrame({'actual': y_actual, 'predicted': y_pred}).plot.scatter(x='actual', y='predicted', alpha=0.5)\nplt.plot([5000, 30000], [5000, 30000], color='C1')\nplt.show()\n\nTraining data (5, 5)\n\nRegression statistics\n\n                      Mean Error (ME) : -1.9756\n       Root Mean Squared Error (RMSE) : 1084.3964\n            Mean Absolute Error (MAE) : 805.6998\n          Mean Percentage Error (MPE) : 0.0288\nMean Absolute Percentage Error (MAPE) : 7.7659\n\nValidation data (5, 5)\n\nRegression statistics\n\n                      Mean Error (ME) : -84.4923\n       Root Mean Squared Error (RMSE) : 1146.2892\n            Mean Absolute Error (MAE) : 867.0769\n          Mean Percentage Error (MPE) : -0.9083\nMean Absolute Percentage Error (MAPE) : 8.5001\n\n\n\n\n\n\n\n\n\n\nWhat happens to the RMS error for the validation data?\nComment on the appropriate number of layers and nodes for this application"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-11-ProbSolutions-NN.html#summary",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-11-ProbSolutions-NN.html#summary",
    "title": "Chapter 11: Neural nets (NN)",
    "section": "Summary",
    "text": "Summary\n\nNetwork (2)\n\nTraining (RMSE) : 1067.2968\nValidation (RMSE) : 1134.6333\n\nNetwork (5)\n\nTraining (RMSE) : 1053.5151\nValidation (RMSE) : 1129.5478\n\nNetwork (5, 5)\n\nTraining (RMSE) : 1069.4853\nValidation (RMSE) : 1151.3034\n\n\nOverall, the neural network architecture has only little effect on the regression performance. It seems that the network with one hidden layer of 5 nodes has the lowest training RMSE. The RMSE for the validation sets behave in the opposite way.\nBased on these results, we would pick the neural network with one hidden layer of 2 nodes."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-11-ProbSolutions-NN.html#data-preprocessing-1",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-11-ProbSolutions-NN.html#data-preprocessing-1",
    "title": "Chapter 11: Neural nets (NN)",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\n\n# drop missing values from the data\nairline_df = pd.DataFrame.dropna(airline_df)\n# check variable types\nairline_df.dtypes\n\nID#                  float64\nTopflight            float64\nBalance              float64\nQual_miles           float64\ncc1_miles?           float64\ncc2_miles?           float64\ncc3_miles?           float64\nBonus_miles          float64\nBonus_trans          float64\nFlight_miles_12mo    float64\nFlight_trans_12      float64\nOnline_12            float64\nEmail                float64\nClub_member          float64\nAny_cc_miles_12mo    float64\nPhone_sale           float64\ndtype: object\n\n\n\n# Remove unnecessary variables\ndel airline_df['ID#']\n\nairline_df.head()\n\n\n\n\n\n\n\n\nTopflight\nBalance\nQual_miles\ncc1_miles?\ncc2_miles?\ncc3_miles?\nBonus_miles\nBonus_trans\nFlight_miles_12mo\nFlight_trans_12\nOnline_12\nEmail\nClub_member\nAny_cc_miles_12mo\nPhone_sale\n\n\n\n\n0\n0.0\n28143.0\n0.0\n0.0\n1.0\n0.0\n174.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n\n\n1\n0.0\n19244.0\n0.0\n0.0\n0.0\n0.0\n215.0\n2.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n0.0\n41354.0\n0.0\n1.0\n0.0\n0.0\n4123.0\n4.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n\n\n3\n0.0\n14776.0\n0.0\n0.0\n0.0\n0.0\n500.0\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n\n\n4\n1.0\n97752.0\n0.0\n1.0\n0.0\n0.0\n43300.0\n26.0\n2077.0\n4.0\n0.0\n1.0\n0.0\n1.0\n0.0\n\n\n\n\n\n\n\n\n# prettify the variable names by removing characters and spaces in their names\nairline_df.columns = [c.replace('?', '') for c in airline_df.columns]\n\n\n# some categorical variables are read as float64, convert them to categorical variables\n# airline_df['Topflight'] = airline_df['Topflight'].astype('category')\n# airline_df['cc1_miles'] = airline_df['cc1_miles'].astype('category')\n# airline_df['cc2_miles'] = airline_df['cc2_miles'].astype('category')\n# airline_df['cc3_miles'] = airline_df['cc3_miles'].astype('category')\n# airline_df['Email'] = airline_df['Email'].astype('category')\nairline_df.dtypes\n\nTopflight            float64\nBalance              float64\nQual_miles           float64\ncc1_miles            float64\ncc2_miles            float64\ncc3_miles            float64\nBonus_miles          float64\nBonus_trans          float64\nFlight_miles_12mo    float64\nFlight_trans_12      float64\nOnline_12            float64\nEmail                float64\nClub_member          float64\nAny_cc_miles_12mo    float64\nPhone_sale           float64\ndtype: object\n\n\n\n# separate out the predictors and response variables\nX_df = airline_df.drop(columns=['Phone_sale'])\nY_df = airline_df[ ['Phone_sale'] ]\n\n\n# normalize the data \nscaleInput = MinMaxScaler()\nscaleOutput = MinMaxScaler()\nX = scaleInput.fit_transform(X_df)\ny = scaleOutput.fit_transform(Y_df)\n\n\n# partition data\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.4, random_state=12345)\nX_train.shape, X_valid.shape, y_train.shape, y_valid.shape\n\n((2991, 14), (1994, 14), (2991, 1), (1994, 1))\n\n\n\nSingle hidden layer with 5 nodes\n\n# train neural network with 5 hidden nodes\nairline_nnet = MLPClassifier(hidden_layer_sizes=(5), activation='logistic', solver='lbfgs', \n                             max_iter=500, random_state=1)\nairline_nnet.fit(X_train, y_train.ravel())\n\n# RMSE for training set\nprint('Training data (5)')\ny_actual = scaleOutput.inverse_transform(y_train).ravel()\ny_pred = scaleOutput.inverse_transform([airline_nnet.predict(X_train)]).ravel()\nregressionSummary(y_pred, y_actual)\n\n# RMSE for validation set\nprint('\\nValidation data (5)')\ny_actual = scaleOutput.inverse_transform(y_valid).ravel()\ny_pred = scaleOutput.inverse_transform([airline_nnet.predict(X_valid)]).ravel()\nregressionSummary(y_pred, y_actual)\n\n# plot predicted vs actual values\nax = pd.DataFrame({'actual': y_actual, 'predicted': y_pred}).plot.scatter(x='actual', y='predicted', alpha=0.5)\nplt.plot([0, 1], [0, 1], color='C1')\nplt.show()\n\nTraining data (5)\n\nRegression statistics\n\n               Mean Error (ME) : -0.1177\nRoot Mean Squared Error (RMSE) : 0.3517\n     Mean Absolute Error (MAE) : 0.1237\n\nValidation data (5)\n\nRegression statistics\n\n               Mean Error (ME) : -0.1214\nRoot Mean Squared Error (RMSE) : 0.3734\n     Mean Absolute Error (MAE) : 0.1394\n\n\n\n\n\n\n\n\n\n\n\nDecile-wise lift chart training set\nPETER - I was not able to do lift charts. I think we will need to ask PeterG.\n\nfrom dmba import classificationSummary, gainsChart, liftChart\n\nclassificationSummary(y_train, airline_nnet.predict(X_train))\n\nnnet_proba = airline_nnet.predict_proba(X_train)\nresult = pd.DataFrame({'actual': y_train.ravel(), \n                       'p(0)': [p[0] for p in nnet_proba],\n                       'p(1)': [p[1] for p in nnet_proba],\n                      })\n\ndf = result.sort_values(by=['p(1)'], ascending=False)\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\ngainsChart(df.actual, ax=axes[0])\nliftChart(df['p(1)'], title=False, ax=axes[1])\nplt.tight_layout()\nplt.show()\n\nConfusion Matrix (Accuracy 0.8763)\n\n       Prediction\nActual    0    1\n     0 2595    9\n     1  361   26\n\n\n\n\n\n\n\n\n\n\n\nDecile-wise lift chart validation set\nPETER - I was not able to do lift charts. I think we will need to ask PeterG.\n11.4.b. Comment on the diﬀerence between the training and validation lift charts.\n\nclassificationSummary(y_valid, airline_nnet.predict(X_valid))\n\nnnet_proba = airline_nnet.predict_proba(X_valid)\nresult = pd.DataFrame({'actual': y_valid.ravel(), \n                       'p(0)': [p[0] for p in nnet_proba],\n                       'p(1)': [p[1] for p in nnet_proba],\n                      })\n\ndf = result.sort_values(by=['p(1)'], ascending=False)\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\ngainsChart(df.actual, ax=axes[0])\nliftChart(df['p(1)'], title=False, ax=axes[1])\nplt.tight_layout()\nplt.show()\n\nConfusion Matrix (Accuracy 0.8606)\n\n       Prediction\nActual    0    1\n     0 1708   18\n     1  260    8\n\n\n\n\n\n\n\n\n\n11.4.c. Run a second neural net model on the data, this time setting the number of hidden nodes to 1. Comment now on the diﬀerence between this model and the model you ran earlier, and how overftting might have aﬀected results.\n\n\nSingle hidden layer with 1 node\n\n# train neural network with 1 hidden node\nairline_nnet = MLPClassifier(hidden_layer_sizes=(1), activation='logistic', solver='lbfgs', random_state=1)\nairline_nnet.fit(X_train, y_train.ravel())\n\n# RMSE for training set\nprint('Training data (1)')\ny_actual = scaleOutput.inverse_transform(y_train).ravel()\ny_pred = scaleOutput.inverse_transform([airline_nnet.predict(X_train)]).ravel()\nregressionSummary(y_pred, y_actual)\n\n# RMSE for validation set\nprint('\\nValidation data (1)')\ny_actual = scaleOutput.inverse_transform(y_valid).ravel()\ny_pred = scaleOutput.inverse_transform([airline_nnet.predict(X_valid)]).ravel()\nregressionSummary(y_pred, y_actual)\n\n# plot predicted vs actual values\nax = pd.DataFrame({'actual': y_actual, 'predicted': y_pred}).plot.scatter(x='actual', y='predicted', alpha=0.5)\nplt.plot([1, 2], [1, 2], color='C1')\nplt.show()\n\nTraining data (1)\n\nRegression statistics\n\n               Mean Error (ME) : -0.1294\nRoot Mean Squared Error (RMSE) : 0.3597\n     Mean Absolute Error (MAE) : 0.1294\n\nValidation data (1)\n\nRegression statistics\n\n               Mean Error (ME) : -0.1344\nRoot Mean Squared Error (RMSE) : 0.3666\n     Mean Absolute Error (MAE) : 0.1344\n\n\n\n\n\n\n\n\n\n11.4.d. What sort of information, if any, is provided about the eﬀects of the various variables?\nAnswer:\nThe output from a neural network does not contain information on the effects of each of the predictors. Unlike linear or logistic regression where a coefficient is attached to each predictor, or even classification and regression trees, where the location of a variable indicates its importance, in neural networks there is no such information. In that sense it is often called a “black box”."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-02-ProbSolutions-Overview.html",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-02-ProbSolutions-Overview.html",
    "title": "Chapter 2: Overview of Data Mining Process",
    "section": "",
    "text": "2019-2020 Galit Shmueli, Peter C. Bruce, Peter Gedeck\n\nData Mining for Business Analytics: Concepts, Techniques, and Applications in Python (First Edition) Galit Shmueli, Peter C. Bruce, Peter Gedeck, and Nitin R. Patel. 2019.\nDate: 2020-03-08\nPython Version: 3.8.2 Jupyter Notebook Version: 5.6.1\nPackages: - pandas: 1.0.1 - scikit-learn: 0.22.2\nThe assistance from Mr. Kuber Deokar and Ms. Anuja Kulkarni in preparing these solutions is gratefully acknowledged.\n\n\n# import required functionality for this chapter\nfrom pathlib import Path\n\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\n\n# Working directory:\n#\n# We assume that data are kept in the same directory as the notebook. If you keep your \n# data in a different folder, replace the argument of the `Path`\nDATA = Path('.').resolve().parent / 'data'\nFIGURES = Path('.').resolve().parent / 'figures' / 'chapter_02'\nFIGURES.mkdir(exist_ok=True, parents=True)\n# and then load data using \n#\n# pd.read_csv(DATA / ‘filename.csv’)\n\n\nProblem 2.1 Supervised or Unsupervised Task\nAssuming that data mining techniques are to be used in the following cases, identify whether the task required is supervised or unsupervised learning.\n2.1.a. Deciding whether to issue a loan to an applicant based on demographic and financial data (with reference to a database of similar data on prior customers).\nAnswer: This is supervised learning, because the database includes information on whether the loan was approved or not.\n2.1.b. In an online bookstore, making recommendations to customers concerning additional items to buy based on the buying patterns in prior transactions.\nAnswer: This is unsupervised learning, because there is no apparent outcome (e.g., whether the recommendation was adopted or not).\n2.1.c. Identifying a network data packet as dangerous (virus, hacker attack) based on comparison to other packets whose threat status is known.\nAnswer: This is supervised learning, because for the other packets the status is known.\n2.1.d. Identifying segments of similar customers.\nAnswer: This is unsupervised learning because there is no known outcome (though once you use unsupervised learning to identify segments, you could use supervised learning to classify new customers into those segments).\n2.1.e. Predicting whether a company will go bankrupt based on comparing its financial data to those of similar bankrupt and nonbankrupt firms.\nAnswer: This is supervised learning, because the status of the similar firms is known.\n2.1.f. Estimating the repair time required for an aircraft based on a trouble ticket.\nAnswer: This is supervised learning, because there is likely to be knowledge of actual (historic) repair times of similar repairs.\n2.1.g. Automated sorting of mail by zip code scanning.\nAnswer: This is supervised learning, as there is likely to be knowledge about whether the sorting was correct in previous mail sorting.\n2.1.h. Printing of custom discount coupons at the conclusion of a grocery store checkout based on what you just bought and what others have bought previously.\nAnswer: This is unsupervised learning, if we assume that we do not know what will be purchased in the future.\n\n\nProblem 2.2\nDescribe the difference in roles assumed by the validation partition and the test partition.\nAnswer: The validation partition is used to assess the performance of each supervised learning model so that we can compare models and pick the best one. In some algorithms (e.g., classification and regression trees, k-nearest neighbors) the validation partition may be used in automated fashion to tune and improve the model. This means that the validation data are actually used to help build the model.\nThe test data partition is used for assessing the performance of the final chosen model on new data. The test data are not used to compare models, or to further tweak the model or improve its fit. (If the test data were used for these purposes, they would play a role in building or selecting the best model, and would no longer provide an unbiased assessment of the chosen model’s performance with completely new data.)\n\n\nProblem 2.3\nConsider the sample from a database of credit applicants in Table 2.16. Comment on the likelihood that it was sampled randomly, and whether it is likely to be a useful sample.\n\n\n\nTable2.16.PNG\n\n\nAnswer: This sample is not selected randomly as we can see from “observation #”, that there is pattern in the observations chosen for the sample. In particular, every 8th observation from the database was selected for the sample. When we select data with such a pre-decided methodology it might introduce a bias in the selected data set. This is true when the order of the observations in the dataset has some meaning (e.g., chronological order).\n\n\nProblem 2.4\nConsider the sample from a bank database shown in Table 2.17; it was selected randomly from a larger database to be the training set. Personal Loan indicates whether a solicitation for a personal loan was accepted and is the response variable. A campaign is planned for a similar solicitation in the future and the bank is looking for a model that will identify likely responders. Examine the data carefully and indicate what your next step would be.\n\n\n\nTable2.17.PNG\n\n\nAnswer: Since there are only 18 records and 9 predictor variables in the sample, the next step before building a model is to take a larger sample. 18 records is too few to support a model that considers 9 predictors. How big a sample? The availability of data, the cost and effort involved in data handling, and software capabilities are the main constraining factors. Also, it is useful to check the number of available responses, and the response ratio, in the larger database. If the response ratio is very low, it would be worthwhile to oversample the cases where response is positive (or, in other words, undersample the non-response cases). Two other issues:\n\nZip code should probably be aggregated at a higher level than 5 digits, which would likely produce an unmanageable number of predictor variables. For example, it could be aggregated at the level of the first 3 digits.\nIt also seems that the key information in “Mortgage” is whether the person has a mortgage, and not so much the level of the mortgage, so some consideration could be given to converting this to a binary variable.\n\nSee also the 2016 blog post by Tom Fawcett: Learning from Imbalanced Classes\n\n\nProblem 2.5\nUsing the concept of overfitting, explain why when a model is fit to training data, zero error with those data is not necessarily good.\nAnswer: Overfitting occurs when the model captures not only the generalizable pattern in the data, but also the error. When we split the data into training and validation sets, we assume that the same pattern (if there is a pattern) exists in both, and that they differ only in the error that they contain. An absurd and false model may fit perfectly (on training data set) if the model has enough complexity. Therefore we may get zero error for such a model using the training dataset. Such a model, however, is not likely to give useful results on the validation data set.\n\n\nProblem 2.6\nIn fitting a model to classify prospects as purchasers or nonpurchasers, a certain company drew the training data from internal data that include demographic and purchase information. Future data to be classified will be lists purchased from other sources, with demographic (but not purchase) data included. It was found that “refund issued” was a useful predictor in the training data. Why is this not an appropriate variable to include in the model?\nAnswer: The variable “refund issued” is unknown prior to the actual purchase, and therefore is not useful in a predictive model of future purchase behavior. In fact, “refund issued” can only be present for actual purchases but never for non-purchases. This explains why it was found to be closely related to purchase/non-purchase.\n\n\nProblem 2.7\nA dataset has 1000 records and 50 variables with 5% of the values missing, spread randomly throughout the records and variables. An analyst decides to remove records with missing values. About how many records would you expect to be removed?\nAnswer: For a record to have all values present, it must avoid having a missing value (P = 0.95) for each of 50 records. The chance that a given record will escape having a missing value for two variables is 0.95 * 0.95 = 0.903. The chance that a given record would escape having a missing value for all 50 records is (0.95)^50 = 0.076945. This implies that 1-0.076944 = 0.9231 (92.31%) of all records will have missing values and would be deleted.\n\n\nProblem 2.8\nNormalize the data in Table 2.18, showing calculations.\n\n\n\nTable2.18.PNG\n\n\nAnswer: Normalization of a measurement is obtained by subtracting the (column) average from each measurement and dividing the difference by the (column) standard deviation.\nFor variable Age (years): Mean =44.66667 and Standard deviation (std) = 14.97554\nFor variable Income ($): Mean=98.66667 and Standard deviation (std) = 62.86706\nFor normalizing age for observation # 1 (Here age = 25): After subtracting the average and dividing by standard deviation, the normalized age = -1.438596.\nFor normalizing income for observation # 1 (Here Income = 49000): After subtracting the average income and dividing by standard deviation, the normalized income = -0.865431.\nLet’s normalize the data using sklearn’s preprocessing\n\n# import the required functionality for this problem\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\n\n# create a data frame\ndf = pd.DataFrame({\n    'Age': [25, 56, 65, 32, 41, 49],\n    'Income': [49000, 156000, 99000, 192000, 39000, 57000]\n})\nprint(df)\n\n   Age  Income\n0   25   49000\n1   56  156000\n2   65   99000\n3   32  192000\n4   41   39000\n5   49   57000\n\n\n\n# normalize the data\nscaler = StandardScaler()\ndf_norm = pd.DataFrame(scaler.fit_transform(df), index=df.index, columns=df.columns)\nprint(df_norm)\n\n        Age    Income\n0 -1.438597 -0.865431\n1  0.829022  0.999021\n2  1.487363  0.005808\n3 -0.926554  1.626314\n4 -0.268213 -1.039679\n5  0.316979 -0.726033\n\n\n\n\nProblem 2.9\nStatistical distance between records can be measured in several ways. Consider Euclidean distance, measured as the square root of the sum of the squared differences. For the first two records in Table 2.17, it is\n\\[\\begin{equation*}\n\\sqrt{(25-56)^2 + (49000-156000)^2}\n\\end{equation*}\\]\nCan normalizing the data change which two records are farthest from each other in terms of Euclidean distance?\nAnswer: Yes, it can. By normalizing we equate the scales of the different variables, and therefore the Euclidean distance can dramatically change. In the example age is in single years while income is in thousands of dollars. The Euclidean distance on the raw data is therefore almost completely determined by income and unaffected by age. In contrast, after normalizing age and income will be on the same scale. Age will then have a much larger impact on the Euclidean distance. To see this in practice, examine the table below that compare the Euclidean distance before and after normalizing the data.\nWe now see that records 4 and 5 (distance = 153000) are farthest from each other before normalizing, whereas records 1 and 3 are farthest from each other after normalizing.\n\n# euclidean distances for original data\nfrom sklearn.metrics import pairwise\nd = pairwise.pairwise_distances(df, metric='euclidean')\npd.DataFrame(d, columns=df.index, index=df.index)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n\n\n\n\n0\n0.000000\n107000.004491\n50000.016000\n143000.000171\n10000.012800\n8000.036000\n\n\n1\n107000.004491\n0.000000\n57000.000711\n36000.008000\n117000.000962\n99000.000247\n\n\n2\n50000.016000\n57000.000711\n0.000000\n93000.005855\n60000.004800\n42000.003048\n\n\n3\n143000.000171\n36000.008000\n93000.005855\n0.000000\n153000.000265\n135000.001070\n\n\n4\n10000.012800\n117000.000962\n60000.004800\n153000.000265\n0.000000\n18000.001778\n\n\n5\n8000.036000\n99000.000247\n42000.003048\n135000.001070\n18000.001778\n0.000000\n\n\n\n\n\n\n\n\n# euclidean distances for normalized data\nd_norm = pairwise.pairwise_distances(df_norm, metric='euclidean')\npd.DataFrame(d_norm, columns=df_norm.index, index=df_norm.index)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n\n\n\n\n0\n0.000000\n2.935690\n3.052916\n2.543812\n1.183284e+00\n1.761101\n\n\n1\n2.935690\n0.000000\n1.191589\n1.864280\n2.315215e+00\n1.799444\n\n\n2\n3.052916\n1.191589\n0.000000\n2.907409\n2.043303e+00\n1.380358\n\n\n3\n2.543812\n1.864280\n2.907409\n0.000000\n2.746075e+00\n2.660809\n\n\n4\n1.183284\n2.315215\n2.043303\n2.746075\n2.107342e-08\n0.663945\n\n\n5\n1.761101\n1.799444\n1.380358\n2.660809\n6.639453e-01\n0.000000\n\n\n\n\n\n\n\n\n\nProblem 2.10\nTwo models are applied to a dataset that has been partitioned. Model A is considerably more accurate than model B on the training data, but slightly less accurate than model B on the validation data. Which model are you more likely to consider for final deployment?\nAnswer: We prefer the model with the lowest error on the validation data. Model A might be overfitting the training data. We would therefore select model B for deployment on new data.\n\n\nProblem 2.11\nThe dataset ToyotaCorolla.csv contains data on used cars on sale during the late summer of 2004 in the Netherlands. It has 1436 records containing details on 38 attributes, including Price, Age, Kilometers, HP, and other specifications.\nWe plan to analyze the data using various data mining techniques described in future chapters. Prepare the data for use as follows:\n2.11.a. The dataset has two categorical attributes, Fuel Type and Color. Describe how you would convert these to binary variables. Confirm this using pandas methods to transform categorical data into dummies. What would you do with the variable Model?\nAnswer: The categorical fuel_type variable has three categories: petrol, diesel and CNG. To convert these variables into dummy variables, we use only two variables (here we use Petrol and Diesel, but a different pair can also be chosen). The binary variable Petrol gets the value 1 if Fuel Type=Petrol and otherwise it gets the value 0. The binary variable Diesel gets the value 1 if Fuel Type=Diesel and otherwise it gets the value 0. CNG is the “reference category.” If Fuel type is CNG, both binary variables take the value 0.\nSimilarly, the variable Color is converted to dummy variables. It resulted in 10 dummy variables with “Color_Beige” as the reference category.\nMaking dummies of all the categories in Model would make for a lot of predictor variables. Some preliminary exploration should be done first, to see if there are a small number of models that account for most cases. The analysis could be confined initially to those to see how important Model is as a predictor.\n\n# load data\ntoyota_df = pd.read_csv(DATA / 'ToyotaCorolla.csv')\n# data dimension\nprint(\"\\n(#Rows, #Columns):\", toyota_df.shape)\nprint(\"\\n\")\n# review first few records\nprint(\"First few records:\\n\")\nprint(toyota_df.head())\n# create dummy variables for categorical variables, ignore the variable Model\ntoyota_df = pd.get_dummies(toyota_df.iloc[:,3:39], prefix_sep='_', drop_first=True)\n# print column/variable names\nprint(\"\\nVariables:\", toyota_df.columns)\n# review first few records in dummy variables\nprint(\"\\nFirst few records in dummy variables:\")\nprint(toyota_df.loc[:, 'Fuel_Type_Diesel':'Color_Yellow'].head(5))\n\n\n(#Rows, #Columns): (1436, 39)\n\n\nFirst few records:\n\n   Id                                          Model  Price  Age_08_04  \\\n0   1  TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors  13500         23   \n1   2  TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors  13750         23   \n2   3  TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors  13950         24   \n3   4  TOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors  14950         26   \n4   5    TOYOTA Corolla 2.0 D4D HATCHB SOL 2/3-Doors  13750         30   \n\n   Mfg_Month  Mfg_Year     KM Fuel_Type  HP  Met_Color  ... Powered_Windows  \\\n0         10      2002  46986    Diesel  90          1  ...               1   \n1         10      2002  72937    Diesel  90          1  ...               0   \n2          9      2002  41711    Diesel  90          1  ...               0   \n3          7      2002  48000    Diesel  90          0  ...               0   \n4          3      2002  38500    Diesel  90          0  ...               1   \n\n   Power_Steering  Radio  Mistlamps  Sport_Model  Backseat_Divider  \\\n0               1      0          0            0                 1   \n1               1      0          0            0                 1   \n2               1      0          0            0                 1   \n3               1      0          0            0                 1   \n4               1      0          1            0                 1   \n\n   Metallic_Rim  Radio_cassette  Parking_Assistant  Tow_Bar  \n0             0               0                  0        0  \n1             0               0                  0        0  \n2             0               0                  0        0  \n3             0               0                  0        0  \n4             0               0                  0        0  \n\n[5 rows x 39 columns]\n\nVariables: Index(['Age_08_04', 'Mfg_Month', 'Mfg_Year', 'KM', 'HP', 'Met_Color',\n       'Automatic', 'CC', 'Doors', 'Cylinders', 'Gears', 'Quarterly_Tax',\n       'Weight', 'Mfr_Guarantee', 'BOVAG_Guarantee', 'Guarantee_Period', 'ABS',\n       'Airbag_1', 'Airbag_2', 'Airco', 'Automatic_airco', 'Boardcomputer',\n       'CD_Player', 'Central_Lock', 'Powered_Windows', 'Power_Steering',\n       'Radio', 'Mistlamps', 'Sport_Model', 'Backseat_Divider', 'Metallic_Rim',\n       'Radio_cassette', 'Parking_Assistant', 'Tow_Bar', 'Fuel_Type_Diesel',\n       'Fuel_Type_Petrol', 'Color_Black', 'Color_Blue', 'Color_Green',\n       'Color_Grey', 'Color_Red', 'Color_Silver', 'Color_Violet',\n       'Color_White', 'Color_Yellow'],\n      dtype='object')\n\nFirst few records in dummy variables:\n   Fuel_Type_Diesel  Fuel_Type_Petrol  Color_Black  Color_Blue  Color_Green  \\\n0                 1                 0            0           1            0   \n1                 1                 0            0           0            0   \n2                 1                 0            0           1            0   \n3                 1                 0            1           0            0   \n4                 1                 0            1           0            0   \n\n   Color_Grey  Color_Red  Color_Silver  Color_Violet  Color_White  \\\n0           0          0             0             0            0   \n1           0          0             1             0            0   \n2           0          0             0             0            0   \n3           0          0             0             0            0   \n4           0          0             0             0            0   \n\n   Color_Yellow  \n0             0  \n1             0  \n2             0  \n3             0  \n4             0  \n\n\n2.11.b. Prepare the dataset (as factored into dummies) for data mining techniques of supervised learning by creating partitions in Python. Select all the variables and use default values for the random seed and partitioning percentages for training (50%), validation (30%), and test (20%) sets. Describe the roles that these partitions will play in modeling.\nAnswer\n\n# partition the data into training(50%), validation (30%) and test(20%) sets\n# random_state is set to a defined value to get the same partitions when re-running the code\n# training (50%), validation (30%), and test (20%)\nfrom sklearn.model_selection import train_test_split\n\ntrainData, temp = train_test_split(toyota_df, test_size=0.5, random_state=1)\nvalidData, testData = train_test_split(temp, test_size=0.4, random_state=1)\nprint('Training : ', trainData.shape)\nprint('Validation : ', validData.shape)\nprint('Test : ', testData.shape)\n\nTraining :  (718, 45)\nValidation :  (430, 45)\nTest :  (288, 45)\n\n\nTraining dataset\nThe training dataset is used to train or build models. For example, in a linear regression, the training dataset is used to fit the linear regression model, i.e. to compute the regression coefficients. This is usually the largest partition.\nValidation dataset\nOnce a model is built on training data, we assess the accuracy of the model on unseen data. For this, the model should be used on a dataset that was not used in the training process. In the validation data we know the actual value of the response variable, and can therefore examine the difference between the actual value and the predicted value to determine the error in prediction. Based on this performance, sometimes the validation dataset is used to tweak the model, or to choose between multiple fitted models.\nTest dataset\nThe validation dataset is often used to select a model with minimum error. Testing that model on completely unseen data gives a realistic estimate of the performance of the model. When a model is finally chosen, its accuracy with the validation dataset is still an optimistic estimate of how it would perform with unseen data. This is because (1) the final model has come out as the winner among the competing models based on the fact that its accuracy with the validation dataset is highest, and/or (2) the validation set was used to help build one or more models. Thus, you need to set aside yet another portion of data, which is used neither in training nor in validation, which is called the test dataset. The accuracy of the model on the test data gives a realistic estimate of the performance of the model on completely unseen data.\n__*It has been pointed out that there is a value in the cc variable - 16,000 - that is probably a data input error. The solutions have been prepared without correcting this error, but a solution that includes correcting this error to 1600 would also be fine. (The data could also be used as a small illustration or exercise of data prep and cleaning.)__"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-04-ProbSolutions-PCA.html",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-04-ProbSolutions-PCA.html",
    "title": "Chapter 4: Dimension Reduction",
    "section": "",
    "text": "2019-2020 Galit Shmueli, Peter C. Bruce, Peter Gedeck\n\nData Mining for Business Analytics: Concepts, Techniques, and Applications in Python (First Edition) Galit Shmueli, Peter C. Bruce, Peter Gedeck, and Nitin R. Patel. 2019.\nDate: 2020-03-08\nPython Version: 3.8.2 Jupyter Notebook Version: 5.6.1\nPackages: - matplotlib: 3.2.0 - numpy: 1.18.1 - pandas: 1.0.1 - seaborn: 0.10.0 - scikit-learn: 0.22.2\nThe assistance from Mr. Kuber Deokar and Ms. Anuja Kulkarni in preparing these solutions is gratefully acknowledged.\n\n\n# import required packages for this chapter\nfrom pathlib import Path\n\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.decomposition import PCA\nfrom sklearn import preprocessing\n\nimport matplotlib.pylab as plt\nimport seaborn as sns\n\n%matplotlib inline\n\n\n# Working directory:\n#\n# We assume that data are kept in the same directory as the notebook. If you keep your \n# data in a different folder, replace the argument of the `Path`\nDATA = Path('.')\n# and then load data using \n#\n# pd.read_csv(DATA / ‘filename.csv’)\n\n\nProblem 4.1 Breakfast Cereals\nUse the data for the breakfast cereals example in Section 4.8 to explore and summarize the data as follows:\n4.1.a. Which variables are quantitative/numerical? Which are ordinal? Which are nominal?\n\n# load the data\ncereals_df = pd.read_csv(DATA / 'cereals.csv')\ncereals_df.shape\n\n(77, 16)\n\n\n\n# variable types\ncereals_df.dtypes\n\nname         object\nmfr          object\ntype         object\ncalories      int64\nprotein       int64\nfat           int64\nsodium        int64\nfiber       float64\ncarbo       float64\nsugars      float64\npotass      float64\nvitamins      int64\nshelf         int64\nweight      float64\ncups        float64\nrating      float64\ndtype: object\n\n\nQuantitative variables: calories, protein, fat, sodium, fiber, carbo, sugars, potass, vitamins, weight, cups, shelf, and rating.\nNominal variables: type and mfr.\nOrdinal Variables: shelf.\n4.1.b. Compute the mean, median, min, max, and standard deviation for each of the quantitative variables. This can be done using pandas as shown in Table 4.3.\n\n# data with just quantitative variables\nquant_vars = ['calories', 'protein', 'fat', 'sodium', 'fiber', 'carbo', 'sugars', 'potass', 'vitamins', 'weight', 'cups', \n              'shelf', 'rating']\nall_numeric_df = cereals_df[quant_vars]\npd.DataFrame({'mean': all_numeric_df.mean(),\n              'mediun': all_numeric_df.median(),\n              'min': all_numeric_df.min(),\n              'max': all_numeric_df.max(),\n              'sd': all_numeric_df.std(),\n})\n\n\n\n\n\n\n\n\nmean\nmediun\nmin\nmax\nsd\n\n\n\n\ncalories\n106.883117\n110.000000\n50.000000\n160.000000\n19.484119\n\n\nprotein\n2.545455\n3.000000\n1.000000\n6.000000\n1.094790\n\n\nfat\n1.012987\n1.000000\n0.000000\n5.000000\n1.006473\n\n\nsodium\n159.675325\n180.000000\n0.000000\n320.000000\n83.832295\n\n\nfiber\n2.151948\n2.000000\n0.000000\n14.000000\n2.383364\n\n\ncarbo\n14.802632\n14.500000\n5.000000\n23.000000\n3.907326\n\n\nsugars\n7.026316\n7.000000\n0.000000\n15.000000\n4.378656\n\n\npotass\n98.666667\n90.000000\n15.000000\n330.000000\n70.410636\n\n\nvitamins\n28.246753\n25.000000\n0.000000\n100.000000\n22.342523\n\n\nweight\n1.029610\n1.000000\n0.500000\n1.500000\n0.150477\n\n\ncups\n0.821039\n0.750000\n0.250000\n1.500000\n0.232716\n\n\nshelf\n2.207792\n2.000000\n1.000000\n3.000000\n0.832524\n\n\nrating\n42.665705\n40.400208\n18.042851\n93.704912\n14.047289\n\n\n\n\n\n\n\n4.1.c. Plot a histogram for each of the quantitative variables. Based on the histograms and summary statistics, answer the following questions:\n\n# histograms for quantitative variables\n\nfor var in quant_vars:\n    #fig, axes = plt.subplots(nrows=1, ncols=2)\n    plt.hist(all_numeric_df[var].dropna())\n    plt.xlabel(var)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.1.c.i. Which variables have the largest variability?\nSodium and Potass have the largest variability.\n4.1.c.ii. Which variables seem skewed?\nVariables Fiber, Potass and Rating are all positively skewed.\n4.1.c.iii. Are there any values that seem extreme?\nFor the following variables, extreme values (outliers) are present:\nProtein (extreme values are 5 and 6)\nFat (extreme value is 5)\nFiber (extreme values are 14, 10, 9 and 6)\nVitamins (extreme value is 100)\nWeight (extreme values are 0.5, 1.33, 1.5)\nRating (extreme value is 93.7)\n4.1.d. Plot a side-by-side boxplot comparing the calories in hot vs. cold cereals. What does this plot show us?\n\n# side by side boxplot of calories in hot vs. cold cereals\n\nax = cereals_df.boxplot(column='calories', by='type')\nax.set_ylabel('calories')\nplt.suptitle('')  # Suppress the titles\nplt.title('Distribution of Calories')\n\nText(0.5, 1.0, 'Distribution of Calories')\n\n\n\n\n\n\n\n\n\nWe see that in cold cereals, the different cereals vary in the amount of calories mainly between approximately 90-120, whereas all hot cereals have 100 calories.\n4.1.e. Plot a side-by-side boxplot of consumer rating as a function of the shelf height. If we were to predict consumer rating from shelf height, does it appear that we need to keep all three categories of shelf height?\n\n# side-by-side boxplot of consumer rating as a function of the shelf height\nax = cereals_df.boxplot(column='rating', by='shelf')\nax.set_ylabel('rating')\nplt.suptitle('')  # Suppress the titles\nplt.title('Distribution of customer rating')\n\nText(0.5, 1.0, 'Distribution of customer rating')\n\n\n\n\n\n\n\n\n\nNot much of a story here - both minimum and maximum shelf height have similar ratings, middling height are rated lower.\n4.1.f. Compute the correlation table for the quantitative variables (method corr()). In addition, generate a matrix plot for these variables (see Table 3.4 on how to do this using the seaborn library).\n\n# correlation table for the quantitative variables\n\ncorr = all_numeric_df.corr()\ncorr\n\n\n\n\n\n\n\n\ncalories\nprotein\nfat\nsodium\nfiber\ncarbo\nsugars\npotass\nvitamins\nweight\ncups\nshelf\nrating\n\n\n\n\ncalories\n1.000000\n0.019066\n0.498610\n0.300649\n-0.293413\n0.257638\n0.566533\n-0.072063\n0.265356\n0.696091\n0.087200\n0.097234\n-0.689376\n\n\nprotein\n0.019066\n1.000000\n0.208431\n-0.054674\n0.500330\n-0.025012\n-0.291853\n0.563706\n0.007335\n0.216158\n-0.244469\n0.133865\n0.470618\n\n\nfat\n0.498610\n0.208431\n1.000000\n-0.005407\n0.016719\n-0.300003\n0.302497\n0.200445\n-0.031156\n0.214625\n-0.175892\n0.263691\n-0.409284\n\n\nsodium\n0.300649\n-0.054674\n-0.005407\n1.000000\n-0.070675\n0.297687\n0.058866\n-0.042632\n0.361477\n0.308576\n0.119665\n-0.069719\n-0.401295\n\n\nfiber\n-0.293413\n0.500330\n0.016719\n-0.070675\n1.000000\n-0.380357\n-0.138760\n0.911528\n-0.032243\n0.247226\n-0.513061\n0.297539\n0.584160\n\n\ncarbo\n0.257638\n-0.025012\n-0.300003\n0.297687\n-0.380357\n1.000000\n-0.471184\n-0.365003\n0.219202\n0.138467\n0.367460\n-0.192650\n0.088712\n\n\nsugars\n0.566533\n-0.291853\n0.302497\n0.058866\n-0.138760\n-0.471184\n1.000000\n0.001414\n0.098231\n0.455844\n-0.048961\n0.068377\n-0.763902\n\n\npotass\n-0.072063\n0.563706\n0.200445\n-0.042632\n0.911528\n-0.365003\n0.001414\n1.000000\n-0.005427\n0.419933\n-0.501607\n0.385784\n0.416009\n\n\nvitamins\n0.265356\n0.007335\n-0.031156\n0.361477\n-0.032243\n0.219202\n0.098231\n-0.005427\n1.000000\n0.320324\n0.128405\n0.299262\n-0.240544\n\n\nweight\n0.696091\n0.216158\n0.214625\n0.308576\n0.247226\n0.138467\n0.455844\n0.419933\n0.320324\n1.000000\n-0.199583\n0.190762\n-0.298124\n\n\ncups\n0.087200\n-0.244469\n-0.175892\n0.119665\n-0.513061\n0.367460\n-0.048961\n-0.501607\n0.128405\n-0.199583\n1.000000\n-0.335269\n-0.203160\n\n\nshelf\n0.097234\n0.133865\n0.263691\n-0.069719\n0.297539\n-0.192650\n0.068377\n0.385784\n0.299262\n0.190762\n-0.335269\n1.000000\n0.025159\n\n\nrating\n-0.689376\n0.470618\n-0.409284\n-0.401295\n0.584160\n0.088712\n-0.763902\n0.416009\n-0.240544\n-0.298124\n-0.203160\n0.025159\n1.000000\n\n\n\n\n\n\n\n4.1.f.i. Which pair of variables is most strongly correlated?\nFrom the correlation matrix, fiber and potass are the most strongly (positively) correlated. Fat and calories are also positively correlated.\n4.1.f.ii. How can we reduce the number of variables based on these correlations?\nPairs of variables that have a very strong (positive or negative) correlation contain duplicative information. Therefore, we can omit the variables that are strongly correlated to others. This will avoid multicolinearity.\n4.1.f.iii. How would the correlations change if we normalized the data first?\nCorrelations are unaffected by the normalization of data.\n4.1.g. Consider the first PC of the analysis of the 13 numerical variables in Table 4.11. Describe briefly what this PC represents.\nThe first column is the first principal component. It shows the weights that the different variables carry. We see that it is dominated by sodium (which has the highest weight). This implies that the 1st principal component is measuring how much sodium the cereal contains.\n\n\nProblem 4.2 University Rankings.\nThe dataset on American college and university rankings (available from www.dataminingbook.com) contains information on 1302 American colleges and universities offering an undergraduate program. For each university, there are 17 measurements that include continuous measurements (such as tuition and graduation rate) and categorical measurements (such as location by state and whether it is a private or a public school).\n\n# load the data and review\n\nuniversities_df = pd.read_csv(DATA / 'Universities.csv')\nuniversities_df.head()\n\n\n\n\n\n\n\n\nCollege Name\nState\nPublic (1)/ Private (2)\n# appli. rec'd\n# appl. accepted\n# new stud. enrolled\n% new stud. from top 10%\n% new stud. from top 25%\n# FT undergrad\n# PT undergrad\nin-state tuition\nout-of-state tuition\nroom\nboard\nadd. fees\nestim. book costs\nestim. personal $\n% fac. w/PHD\nstud./fac. ratio\nGraduation rate\n\n\n\n\n0\nAlaska Pacific University\nAK\n2\n193.0\n146.0\n55.0\n16.0\n44.0\n249.0\n869.0\n7560.0\n7560.0\n1620.0\n2500.0\n130.0\n800.0\n1500.0\n76.0\n11.9\n15.0\n\n\n1\nUniversity of Alaska at Fairbanks\nAK\n1\n1852.0\n1427.0\n928.0\nNaN\nNaN\n3885.0\n4519.0\n1742.0\n5226.0\n1800.0\n1790.0\n155.0\n650.0\n2304.0\n67.0\n10.0\nNaN\n\n\n2\nUniversity of Alaska Southeast\nAK\n1\n146.0\n117.0\n89.0\n4.0\n24.0\n492.0\n1849.0\n1742.0\n5226.0\n2514.0\n2250.0\n34.0\n500.0\n1162.0\n39.0\n9.5\n39.0\n\n\n3\nUniversity of Alaska at Anchorage\nAK\n1\n2065.0\n1598.0\n1162.0\nNaN\nNaN\n6209.0\n10537.0\n1742.0\n5226.0\n2600.0\n2520.0\n114.0\n580.0\n1260.0\n48.0\n13.7\nNaN\n\n\n4\nAlabama Agri. & Mech. Univ.\nAL\n1\n2817.0\n1920.0\n984.0\nNaN\nNaN\n3958.0\n305.0\n1700.0\n3400.0\n1108.0\n1442.0\n155.0\n500.0\n850.0\n53.0\n14.3\n40.0\n\n\n\n\n\n\n\n\nvar = list(universities_df.columns)\nvar\n\n['College Name',\n 'State',\n 'Public (1)/ Private (2)',\n \"# appli. rec'd\",\n '# appl. accepted',\n '# new stud. enrolled',\n '% new stud. from top 10%',\n '% new stud. from top 25%',\n '# FT undergrad',\n '# PT undergrad',\n 'in-state tuition',\n 'out-of-state tuition',\n 'room',\n 'board',\n 'add. fees',\n 'estim. book costs',\n 'estim. personal $',\n '% fac. w/PHD',\n 'stud./fac. ratio',\n 'Graduation rate']\n\n\n\n# variable data types\nuniversities_df.dtypes\n\nCollege Name                 object\nState                        object\nPublic (1)/ Private (2)       int64\n# appli. rec'd              float64\n# appl. accepted            float64\n# new stud. enrolled        float64\n% new stud. from top 10%    float64\n% new stud. from top 25%    float64\n# FT undergrad              float64\n# PT undergrad              float64\nin-state tuition            float64\nout-of-state tuition        float64\nroom                        float64\nboard                       float64\nadd. fees                   float64\nestim. book costs           float64\nestim. personal $           float64\n% fac. w/PHD                float64\nstud./fac. ratio            float64\nGraduation rate             float64\ndtype: object\n\n\n4.2.a. Remove all categorical variables. Then remove all records with missing numerical measurements from the dataset.\n\n# remove all three categorical variables\nvar.remove('College Name')\nvar.remove('State')\nvar.remove('Public (1)/ Private (2)')\n\n\nall_numeric_df = universities_df[var]\nall_numeric_df.shape\n\n(1302, 17)\n\n\n\n# drop missing values\nall_numeric_df = all_numeric_df.dropna(how='any')\nall_numeric_df.shape\n\n(471, 17)\n\n\n4.2.b. Conduct a principal components analysis on the cleaned data and comment on the results. Should the data be normalized? Discuss what characterizes the components you consider key.\n\n# PCA\n\npcs = PCA()\npcs.fit(all_numeric_df)\n\n# view the importance of principal components\npcsSummary_df = pd.DataFrame({'Standard deviation': np.sqrt(pcs.explained_variance_),\n                           'Proportion of variance': pcs.explained_variance_ratio_,\n                           'Cumulative proportion': np.cumsum(pcs.explained_variance_ratio_)})\npcsSummary_df = pcsSummary_df.transpose()\npcsSummary_df.columns = ['PC{}'.format(i) for i in range(1, len(pcsSummary_df.columns) + 1)]\npcsSummary_df.round(4)\n\n\n\n\n\n\n\n\nPC1\nPC2\nPC3\nPC4\nPC5\nPC6\nPC7\nPC8\nPC9\nPC10\nPC11\nPC12\nPC13\nPC14\nPC15\nPC16\nPC17\n\n\n\n\nStandard deviation\n7430.9140\n5987.9890\n1854.6412\n1192.5293\n967.4279\n679.6527\n596.9761\n580.6299\n417.6136\n318.1272\n188.8676\n155.6062\n19.0491\n12.5287\n11.0184\n5.33\n2.9059\n\n\nProportion of variance\n0.5614\n0.3645\n0.0350\n0.0145\n0.0095\n0.0047\n0.0036\n0.0034\n0.0018\n0.0010\n0.0004\n0.0002\n0.0000\n0.0000\n0.0000\n0.00\n0.0000\n\n\nCumulative proportion\n0.5614\n0.9259\n0.9609\n0.9753\n0.9848\n0.9895\n0.9932\n0.9966\n0.9984\n0.9994\n0.9997\n1.0000\n1.0000\n1.0000\n1.0000\n1.00\n1.0000\n\n\n\n\n\n\n\n\n# Components\n\nprint('\\nComponents')\npcsComponents_df = pd.DataFrame(pcs.components_.transpose(),columns=pcsSummary_df.columns, index=[var])\npcsComponents_df\n\n\nComponents\n\n\n\n\n\n\n\n\n\nPC1\nPC2\nPC3\nPC4\nPC5\nPC6\nPC7\nPC8\nPC9\nPC10\nPC11\nPC12\nPC13\nPC14\nPC15\nPC16\nPC17\n\n\n\n\n# appli. rec'd\n0.271883\n0.551183\n0.664458\n0.129476\n-0.034246\n0.370333\n-0.120305\n-0.097471\n-0.035166\n-0.009102\n-0.016696\n-0.008734\n0.005788\n0.000754\n-0.002059\n-0.001503\n-0.000081\n\n\n# appl. accepted\n0.194107\n0.321299\n0.190957\n-0.008357\n-0.076674\n-0.813924\n0.353520\n0.103440\n0.075971\n-0.040233\n0.103389\n0.016789\n-0.011578\n-0.002453\n0.003683\n0.003584\n0.000083\n\n\n# new stud. enrolled\n0.084730\n0.101590\n-0.087451\n-0.055253\n-0.036068\n-0.081429\n0.019293\n-0.039063\n0.030435\n0.170403\n-0.965233\n-0.008420\n0.013475\n-0.002658\n-0.005389\n-0.006643\n-0.000082\n\n\n% new stud. from top 10%\n-0.000898\n0.001732\n0.000136\n-0.001906\n0.001236\n0.009145\n-0.003462\n-0.002851\n-0.001772\n0.003426\n-0.013176\n0.004417\n-0.511022\n-0.251865\n0.230605\n0.781370\n0.106686\n\n\n% new stud. from top 25%\n-0.000811\n0.001925\n0.000040\n-0.002352\n0.001009\n0.007166\n-0.003192\n-0.002603\n-0.000749\n0.001050\n-0.006792\n0.007512\n-0.686812\n-0.225707\n0.320215\n-0.610197\n-0.047794\n\n\n# FT undergrad\n0.458121\n0.492263\n-0.635303\n-0.284582\n-0.080402\n0.129196\n-0.127077\n0.011595\n-0.021579\n-0.012258\n0.152137\n-0.000211\n0.001405\n0.001129\n0.000611\n0.000641\n-0.000148\n\n\n# PT undergrad\n0.108253\n0.073410\n-0.285353\n0.942562\n-0.051743\n-0.039789\n-0.018146\n-0.073893\n-0.044043\n0.031981\n0.001279\n-0.003901\n-0.003181\n0.001234\n0.001414\n0.000195\n0.000120\n\n\nin-state tuition\n-0.670187\n0.382489\n-0.082787\n-0.016972\n-0.621759\n0.000517\n-0.060641\n0.006407\n-0.040511\n0.070211\n0.022783\n-0.003362\n0.000362\n-0.000392\n-0.002251\n-0.000151\n0.000114\n\n\nout-of-state tuition\n-0.454535\n0.428685\n-0.129410\n0.018657\n0.748634\n0.010286\n0.141481\n-0.091839\n-0.000236\n-0.056493\n-0.016404\n0.005916\n0.004394\n-0.000145\n0.002655\n-0.000479\n0.000308\n\n\nroom\n-0.033420\n0.055584\n0.040113\n0.065120\n0.115354\n-0.050083\n-0.314426\n0.873995\n-0.318999\n-0.053459\n-0.065262\n-0.050179\n-0.001090\n-0.000420\n0.000894\n-0.000278\n0.000364\n\n\nboard\n-0.034236\n0.040897\n-0.008232\n0.067313\n0.006301\n0.067317\n-0.145646\n0.276553\n0.938293\n-0.095646\n-0.008915\n-0.025704\n-0.000158\n-0.002073\n0.002044\n0.000795\n-0.000196\n\n\nadd. fees\n0.013209\n0.008746\n0.032868\n-0.012755\n0.103097\n-0.024987\n-0.043534\n0.068703\n0.076206\n0.972820\n0.168601\n0.001859\n0.002655\n-0.002461\n0.002785\n-0.001050\n0.000424\n\n\nestim. book costs\n-0.000058\n0.003291\n0.000316\n0.010795\n-0.005223\n0.034097\n0.011155\n0.064045\n0.006482\n-0.002434\n-0.014164\n0.997041\n0.005360\n0.001904\n-0.009205\n0.000856\n0.000313\n\n\nestim. personal $\n0.037557\n0.001185\n-0.054659\n0.031666\n-0.106952\n0.407928\n0.835394\n0.339141\n0.002346\n0.039237\n-0.013501\n-0.046075\n-0.001610\n0.000984\n-0.000207\n-0.000442\n0.000687\n\n\n% fac. w/PHD\n-0.000205\n0.001564\n-0.000995\n-0.000055\n0.004822\n0.000925\n-0.001184\n-0.000218\n0.001612\n0.002839\n0.000403\n-0.006167\n-0.396144\n-0.066826\n-0.915632\n-0.009347\n-0.007621\n\n\nstud./fac. ratio\n0.000295\n-0.000159\n0.000025\n-0.000135\n-0.000201\n-0.000748\n-0.000397\n-0.000359\n0.000470\n-0.000638\n0.000849\n-0.000424\n0.012433\n0.033745\n-0.014961\n-0.112121\n0.992930\n\n\nGraduation rate\n-0.001072\n0.001397\n0.000920\n-0.002172\n0.001129\n0.001077\n-0.001994\n-0.000387\n0.001937\n0.004057\n-0.007366\n0.000551\n-0.331120\n0.938075\n0.074297\n0.066329\n-0.019117\n\n\n\n\n\n\n\n\n# PCA after normalizatin\n\npcs = PCA()\npcs.fit(preprocessing.scale(all_numeric_df))\n\n# view the importance of principal components\npcsSummary_df = pd.DataFrame({'Standard deviation': np.sqrt(pcs.explained_variance_),\n                           'Proportion of variance': pcs.explained_variance_ratio_,\n                           'Cumulative proportion': np.cumsum(pcs.explained_variance_ratio_)})\npcsSummary_df = pcsSummary_df.transpose()\npcsSummary_df.columns = ['PC{}'.format(i) for i in range(1, len(pcsSummary_df.columns) + 1)]\npcsSummary_df.round(4)\n\n\n\n\n\n\n\n\nPC1\nPC2\nPC3\nPC4\nPC5\nPC6\nPC7\nPC8\nPC9\nPC10\nPC11\nPC12\nPC13\nPC14\nPC15\nPC16\nPC17\n\n\n\n\nStandard deviation\n2.2773\n2.1449\n1.0995\n1.0336\n0.9770\n0.8738\n0.8041\n0.7736\n0.7039\n0.6629\n0.6285\n0.5503\n0.4388\n0.3042\n0.2002\n0.1745\n0.1440\n\n\nProportion of variance\n0.3044\n0.2700\n0.0710\n0.0627\n0.0560\n0.0448\n0.0380\n0.0351\n0.0291\n0.0258\n0.0232\n0.0178\n0.0113\n0.0054\n0.0024\n0.0018\n0.0012\n\n\nCumulative proportion\n0.3044\n0.5745\n0.6454\n0.7081\n0.7642\n0.8090\n0.8469\n0.8821\n0.9111\n0.9369\n0.9601\n0.9779\n0.9892\n0.9946\n0.9970\n0.9988\n1.0000\n\n\n\n\n\n\n\n\n# Components\n\nprint('\\nComponents')\npcsComponents_df = pd.DataFrame(pcs.components_.transpose(),columns=pcsSummary_df.columns, index=[var])\npcsComponents_df\n\n\nComponents\n\n\n\n\n\n\n\n\n\nPC1\nPC2\nPC3\nPC4\nPC5\nPC6\nPC7\nPC8\nPC9\nPC10\nPC11\nPC12\nPC13\nPC14\nPC15\nPC16\nPC17\n\n\n\n\n# appli. rec'd\n0.078361\n0.420164\n-0.031982\n0.072621\n-0.016694\n0.112320\n-0.268145\n-0.093570\n0.039628\n-0.087361\n-0.073021\n-0.009995\n0.602996\n0.198790\n0.346774\n-0.344637\n-0.246354\n\n\n# appl. accepted\n0.023659\n0.434471\n-0.031423\n0.118128\n-0.089073\n0.114381\n-0.266285\n-0.080991\n0.022795\n0.035197\n-0.166046\n-0.062100\n0.251257\n-0.240232\n-0.452347\n0.429830\n0.392238\n\n\n# new stud. enrolled\n-0.028802\n0.445556\n-0.038651\n-0.031466\n-0.075981\n0.054079\n-0.098870\n-0.058138\n0.096336\n0.019353\n-0.072613\n0.013719\n-0.486306\n0.059301\n-0.322663\n-0.010969\n-0.645721\n\n\n% new stud. from top 10%\n0.354028\n0.093547\n-0.120129\n-0.372457\n0.162260\n-0.004445\n0.102709\n-0.112334\n0.028676\n-0.326675\n0.209275\n-0.043489\n-0.003825\n0.646399\n-0.185719\n0.168396\n0.171236\n\n\n% new stud. from top 25%\n0.340496\n0.118396\n-0.142720\n-0.385565\n0.158187\n0.092636\n0.136409\n-0.039927\n-0.006007\n-0.314110\n0.234355\n0.010823\n0.037524\n-0.685605\n0.088571\n-0.055470\n-0.105283\n\n\n# FT undergrad\n-0.049586\n0.443583\n-0.004012\n-0.056459\n-0.094781\n0.043504\n-0.043157\n-0.043464\n0.034858\n-0.009057\n-0.061392\n0.050779\n-0.512673\n0.012862\n0.441354\n-0.217176\n0.519944\n\n\n# PT undergrad\n-0.106380\n0.287700\n0.265769\n0.053495\n-0.343681\n-0.188041\n0.509297\n-0.122490\n0.172351\n0.225459\n0.531642\n-0.107999\n0.168015\n-0.006459\n-0.036556\n0.009390\n0.003856\n\n\nin-state tuition\n0.379389\n-0.150248\n0.084350\n0.041064\n-0.172639\n-0.000539\n-0.129328\n0.009974\n0.092325\n0.103905\n-0.044406\n-0.497755\n-0.066563\n-0.041638\n-0.355983\n-0.592824\n0.157093\n\n\nout-of-state tuition\n0.402555\n-0.048728\n0.051577\n0.077658\n-0.158499\n-0.044407\n-0.077965\n-0.010688\n0.044615\n0.151510\n-0.099283\n-0.507936\n-0.101073\n0.006317\n0.449517\n0.507584\n-0.175592\n\n\nroom\n0.273165\n0.052271\n0.250578\n0.454416\n-0.004482\n-0.015068\n-0.122402\n-0.091329\n-0.680595\n-0.180139\n0.308060\n0.153113\n-0.120440\n0.004846\n-0.027487\n-0.009710\n-0.020306\n\n\nboard\n0.290437\n0.010051\n0.252096\n0.301620\n-0.199067\n-0.038477\n0.152138\n0.466412\n0.421339\n-0.419434\n-0.181506\n0.302579\n-0.004475\n0.001880\n-0.011574\n0.033039\n0.004999\n\n\nadd. fees\n-0.012351\n0.169499\n-0.249747\n0.446562\n0.648920\n-0.418437\n0.082359\n0.048174\n0.205132\n-0.013333\n0.098383\n-0.214784\n-0.043950\n-0.039396\n-0.006340\n-0.039115\n0.030237\n\n\nestim. book costs\n0.057302\n0.056689\n0.652241\n-0.044356\n0.518644\n0.421195\n0.190728\n-0.130177\n0.078975\n0.170842\n-0.172220\n-0.033709\n-0.006943\n0.009861\n-0.004617\n0.003225\n0.000384\n\n\nestim. personal $\n-0.144908\n0.156837\n0.403735\n-0.403709\n0.103358\n-0.466598\n-0.289271\n0.505883\n-0.194258\n0.000128\n0.038711\n-0.133197\n0.051416\n-0.013721\n-0.012993\n0.017390\n-0.012251\n\n\n% fac. w/PHD\n0.254201\n0.196852\n-0.189367\n-0.074609\n-0.017278\n-0.180623\n0.533349\n0.145009\n-0.404837\n0.267768\n-0.502748\n0.096447\n0.101835\n0.027889\n-0.050578\n-0.073086\n-0.009757\n\n\nstud./fac. ratio\n-0.278542\n0.101034\n-0.187598\n0.105222\n0.003000\n0.522154\n0.211516\n0.525627\n-0.207194\n-0.181720\n0.136107\n-0.416236\n0.005232\n0.065855\n-0.006340\n0.010918\n-0.003511\n\n\nGraduation rate\n0.325305\n0.024264\n-0.181888\n-0.012600\n0.109137\n0.215325\n-0.200105\n0.389370\n0.112811\n0.598537\n0.353414\n0.333592\n-0.009703\n0.049079\n0.006724\n0.009623\n0.024913\n\n\n\n\n\n\n\nBecause the different variables are measured on different scales, we must first normalize the data, otherwise the variables with the largest scale will dominate the principal components. From the non-standardized output we see that the first component account for over 50% of the total variability (and the first 6 components accounts for over 98% of the total variability). When we standardize the data, those numbers drop to 30% and 81%.\nThe 1st component appears to capture program quality (e.g. student and faculty quality) and expenses (tuition, room, board). The 2nd component is related to the popularity of the program (as reflected by # application received, accepted, # new enrolled students, and # FT undergraduates). The 3rd component is mainly estimated book and personal costs, and the 4th contrasts room & board costs with student quality (% new students from top 10% and top 25%).\n\n\nProblem 4.3 Sales of Toyota Corolla Cars.\nThe file ToyotaCorolla.csv contains data on used cars (Toyota Corollas) on sale during late summer of 2004 in the Netherlands. It has 1436 records containing details on 38 attributes, including Price, Age, Kilometers, HP, and other specifications. The goal will be to predict the price of a used Toyota Corolla based on its specifications.\n\n# load the data\n\ntoyota_df = pd.read_csv(DATA / 'ToyotaCorolla.csv')\ntoyota_df.shape\n\n(1436, 39)\n\n\n\n# variable types\ntoyota_df.dtypes\n\nId                    int64\nModel                object\nPrice                 int64\nAge_08_04             int64\nMfg_Month             int64\nMfg_Year              int64\nKM                    int64\nFuel_Type            object\nHP                    int64\nMet_Color             int64\nColor                object\nAutomatic             int64\nCC                    int64\nDoors                 int64\nCylinders             int64\nGears                 int64\nQuarterly_Tax         int64\nWeight                int64\nMfr_Guarantee         int64\nBOVAG_Guarantee       int64\nGuarantee_Period      int64\nABS                   int64\nAirbag_1              int64\nAirbag_2              int64\nAirco                 int64\nAutomatic_airco       int64\nBoardcomputer         int64\nCD_Player             int64\nCentral_Lock          int64\nPowered_Windows       int64\nPower_Steering        int64\nRadio                 int64\nMistlamps             int64\nSport_Model           int64\nBackseat_Divider      int64\nMetallic_Rim          int64\nRadio_cassette        int64\nParking_Assistant     int64\nTow_Bar               int64\ndtype: object\n\n\ncategorical variables Fuel_Type Fuel Type (Petrol, Diesel, CNG) Color Color (Blue, Red, Grey, Silver, Black, etc.) Met_Color Metallic Color? (Yes=1, No=0) Automatic Automatic ( (Yes=1, No=0) Mfr_Guarantee Within Manufacturer’s Guarantee period (Yes=1, No=0) BOVAG_Guarantee BOVAG (Dutch dealer network) Guarantee (Yes=1, No=0) ABS Anti-Lock Brake System (Yes=1, No=0) Airbag_1 Driver_Airbag (Yes=1, No=0) Airbag_2 Passenger Airbag (Yes=1, No=0) Airco Airconditioning (Yes=1, No=0) Automatic_airco Automatic Airconditioning (Yes=1, No=0) Boardcomputer Boardcomputer (Yes=1, No=0) CD_Player CD Player (Yes=1, No=0) Central_Lock Central Lock (Yes=1, No=0) Powered_Windows Powered Windows (Yes=1, No=0) Power_Steering Power Steering (Yes=1, No=0) Radio Radio (Yes=1, No=0) Mistlamps Mistlamps (Yes=1, No=0) Sport_Model Sport Model (Yes=1, No=0) Backseat_Divider Backseat Divider (Yes=1, No=0) Metallic_Rim Metallic Rim (Yes=1, No=0) Radio_cassette Radio Cassette (Yes=1, No=0) Parking_Assistant Parking assistance system (Yes=1, No=0) Tow_Bar Tow Bar (Yes=1, No=0)\n4.3.b. Explain the relationship between a categorical variable and the series of binary dummy variables derived from it.\nA variable with N categories will be transformed into N dummy variables, with each dummy indicating whether a certain category is present or not. For example, Fuel type has 3 categories: Petrol, Diesel, and CNG. If we convert it to dummy variables, we get 3 dummy variables: Fuel_Type_Petrol (if the fuel type is Petrol then Fuel_Type_Petrol=1, otherwise Fuel_Type_Petrol=0), Fuel_Type_Diesel (if the fuel type is Diesel then Fuel_Type_Diesel=1, otherwise Fuel_Type_Diesel=0) and Fuel_Type_CNG (if the fuel type is CNG then Fuel_Type_CNG=1, otherwise Fuel_Type_CNG=0). Note that when the first two dummies both have values of 0, it indicates that the fuel type is CNG.\n4.3.c. How many dummy binary variables are required to capture the information in a categorical variable with N categories?\nN-1 binary variables are required to capture the information in a categorical variable with N categories. Note that in some routines, e.g. linear regression and logistic regression, use of all N dummies will cause the routine to fail because the nth variable contains redundant information and can be expressed as a linear combination of the others. Only N-1 variables should be used (they contain all the available information about the variable from which they were derived.)\n4.3.d. Use Python to convert the categorical variables in this dataset into dummy variables, and explain in words, for one record, the values in the derived binary dummies.\nWhen we convert Fuel type into dummy variables, we get Fuel_Type_Diesel, Fuel_Type_Petrol and Fuel_Type_CNG. For the 1st record Fuel type is Diesel. Values in dummy variables are as follows: Fuel_Type_Diesel = 1, Fuel_Type_Petrol = 0, Fuel_Type_CNG = 0.\nNote: that not all 42 variables are shown in the output showing some initial observations below.\n\ndf_fuel_type = pd.get_dummies(toyota_df['Fuel_Type'])\n\nnew_df = pd.concat([toyota_df, df_fuel_type], axis=1)\nnew_df.head()\n\n\n\n\n\n\n\n\nId\nModel\nPrice\nAge_08_04\nMfg_Month\nMfg_Year\nKM\nFuel_Type\nHP\nMet_Color\n...\nMistlamps\nSport_Model\nBackseat_Divider\nMetallic_Rim\nRadio_cassette\nParking_Assistant\nTow_Bar\nCNG\nDiesel\nPetrol\n\n\n\n\n0\n1\nTOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors\n13500\n23\n10\n2002\n46986\nDiesel\n90\n1\n...\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n\n\n1\n2\nTOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors\n13750\n23\n10\n2002\n72937\nDiesel\n90\n1\n...\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n\n\n2\n3\nTOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors\n13950\n24\n9\n2002\n41711\nDiesel\n90\n1\n...\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n\n\n3\n4\nTOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors\n14950\n26\n7\n2002\n48000\nDiesel\n90\n0\n...\n0\n0\n1\n0\n0\n0\n0\n0\n1\n0\n\n\n4\n5\nTOYOTA Corolla 2.0 D4D HATCHB SOL 2/3-Doors\n13750\n30\n3\n2002\n38500\nDiesel\n90\n0\n...\n1\n0\n1\n0\n0\n0\n0\n0\n1\n0\n\n\n\n\n5 rows × 42 columns\n\n\n\n4.3.e. Use Python to produce a correlation matrix and matrix plot. Comment on the relationships among variables.\n\n# correlation matric\ncorr_df = new_df[['Price', 'Age_08_04', 'KM', 'HP', 'CC', 'Quarterly_Tax', 'Weight']]\ncorr = corr_df.corr()\ncorr\n\n\n\n\n\n\n\n\nPrice\nAge_08_04\nKM\nHP\nCC\nQuarterly_Tax\nWeight\n\n\n\n\nPrice\n1.000000\n-0.876590\n-0.569960\n0.314990\n0.126389\n0.219197\n0.581198\n\n\nAge_08_04\n-0.876590\n1.000000\n0.505672\n-0.156622\n-0.098084\n-0.198431\n-0.470253\n\n\nKM\n-0.569960\n0.505672\n1.000000\n-0.333538\n0.102683\n0.278165\n-0.028598\n\n\nHP\n0.314990\n-0.156622\n-0.333538\n1.000000\n0.035856\n-0.298432\n0.089614\n\n\nCC\n0.126389\n-0.098084\n0.102683\n0.035856\n1.000000\n0.306996\n0.335637\n\n\nQuarterly_Tax\n0.219197\n-0.198431\n0.278165\n-0.298432\n0.306996\n1.000000\n0.626134\n\n\nWeight\n0.581198\n-0.470253\n-0.028598\n0.089614\n0.335637\n0.626134\n1.000000\n\n\n\n\n\n\n\n\n# correlation heatmap\n\nsns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns)\n\n\n\n\n\n\n\n\nAge_08_04 is negatively correlated with price. KM is negatively correlated with price. Weight is positively correlated with price. KM is positively correlated with Age_08_04. Weight is positively correlated with quarterly tax.\n\n\nProblem 4.4 Chemical Features of Wine.\nTable 4.13 shows the PCA output on data (nonnormalized) in which the variables represent chemical characteristics of wine, and each case is a different wine.\n4.4.a. The data are in the file Wine.csv. Consider the rows labeled “Proportion of Variance.” Explain why the value for PC1 is so much greater than that of any other column.\nPC1 has a very high proportion of the variance because it is composed mostly of proline, which has a much larger scale than the other variables.\n4.4.b. Comment on the use of normalization (standardization) in part (a).\nNormalizing the data would equalize the scales and eliminate the undesired impact of scale on the calculation of the principal components."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.1 Charles Book Club.html",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.1 Charles Book Club.html",
    "title": "Case 21.1 Charles Book Club",
    "section": "",
    "text": "2019 Galit Shmueli, Peter C. Bruce, Peter Gedeck\n\nCase study included in\nData Mining for Business Analytics: Concepts, Techniques, and Applications in Python (First Edition) Galit Shmueli, Peter C. Bruce, Peter Gedeck, and Nitin R. Patel. 2019.\n%matplotlib inline\nfrom pathlib import Path\nfrom itertools import product\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n# from sklearn.tree import DecisionTreeClassifier\n# from sklearn.ensemble import AdaBoostClassifier\n# from sklearn.ensemble import BaggingClassifier\n# from sklearn.ensemble import RandomForestClassifier\n# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n# from sklearn.preprocessing import MinMaxScaler\n# from sklearn.neural_network import MLPClassifier\nimport matplotlib.pylab as plt\n\nfrom dmba import gainsChart\n\nDATA = Path('.').resolve().parent / 'data'"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.1 Charles Book Club.html#data-preparation",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.1 Charles Book Club.html#data-preparation",
    "title": "Case 21.1 Charles Book Club",
    "section": "Data preparation",
    "text": "Data preparation\n\n# Load the data\ndf = pd.read_csv(DATA / 'CharlesBookClub.csv')\n\n\n# Prepare data for model building\ndf.head()\n\n\n\n\n\n\n\n\nSeq#\nID#\nGender\nM\nR\nF\nFirstPurch\nChildBks\nYouthBks\nCookBks\n...\nItalCook\nItalAtlas\nItalArt\nFlorence\nRelated Purchase\nMcode\nRcode\nFcode\nYes_Florence\nNo_Florence\n\n\n\n\n0\n1\n25\n1\n297\n14\n2\n22\n0\n1\n1\n...\n0\n0\n0\n0\n0\n5\n4\n2\n0\n1\n\n\n1\n2\n29\n0\n128\n8\n2\n10\n0\n0\n0\n...\n0\n0\n0\n0\n0\n4\n3\n2\n0\n1\n\n\n2\n3\n46\n1\n138\n22\n7\n56\n2\n1\n2\n...\n1\n0\n0\n0\n2\n4\n4\n3\n0\n1\n\n\n3\n4\n47\n1\n228\n2\n1\n2\n0\n0\n0\n...\n0\n0\n0\n0\n0\n5\n1\n1\n0\n1\n\n\n4\n5\n51\n1\n257\n10\n1\n10\n0\n0\n0\n...\n0\n0\n0\n0\n0\n5\n3\n1\n0\n1\n\n\n\n\n5 rows × 24 columns\n\n\n\nPartition the data into training (60%) and validation (40%). Use seed = 1.\n\n# in this case, we split the full dataframe including all columns\noutcome = 'Yes_Florence'\ntrain_df, valid_df, train_y, valid_y = train_test_split(df, df[outcome], test_size=0.4, random_state=1)\n\ntrain_df.shape, valid_df.shape\n\n((2400, 24), (1600, 24))"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.1 Charles Book Club.html#step-1",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.1 Charles Book Club.html#step-1",
    "title": "Case 21.1 Charles Book Club",
    "section": "Step 1:",
    "text": "Step 1:\nWhat is the response rate for the training data customers taken as a whole? What is the response rate for each of the \\(4\n\\times 5 \\times 3 = 60\\) combinations of RFM categories? Which combinations have response rates in the training data that are above the overall response in the training data?\nResponse rate for training data customers taken as a whole\n\naverage_response_rate = train_df[outcome].mean()\nprint('average response rate for training data', average_response_rate)\nprint('average response rate for validation data', valid_df[outcome].mean())\n\naverage response rate for training data 0.08833333333333333\naverage response rate for validation data 0.07875\n\n\nWhat is the response rate for each of the \\(4 \\times 5 \\times 3 = 60\\) combinations of RFM categories?\n\nRFMcategories = ['Rcode', 'Fcode', 'Mcode']\naverage_by_combination = train_df.groupby(RFMcategories)['Yes_Florence'].mean()\naverage_by_combination\n\nRcode  Fcode  Mcode\n1      1      1        0.000000\n              2        0.000000\n              3        0.000000\n              4        0.200000\n              5        0.173913\n       2      2        0.666667\n              3        0.000000\n              4        0.050000\n              5        0.080000\n       3      3        0.333333\n              4        0.062500\n              5        0.153846\n2      1      1        0.400000\n              2        0.333333\n              3        0.150000\n              4        0.000000\n              5        0.172414\n       2      2        0.142857\n              3        0.166667\n              4        0.161290\n              5        0.043478\n       3      3        0.333333\n              4        0.200000\n              5        0.177778\n3      1      1        0.100000\n              2        0.090909\n              3        0.027778\n              4        0.074074\n              5        0.035088\n       2      2        0.043478\n              3        0.062500\n              4        0.042254\n              5        0.037736\n       3      3        0.000000\n              4        0.065789\n              5        0.136585\n4      1      1        0.000000\n              2        0.076923\n              3        0.140845\n              4        0.111111\n              5        0.053097\n       2      2        0.052632\n              3        0.033333\n              4        0.052174\n              5        0.082707\n       3      3        0.105263\n              4        0.088496\n              5        0.069307\nName: Yes_Florence, dtype: float64\n\n\nWhich combinations have response rates in the training data that are above the overall response in the training data?\n\ncombinations_above_average = average_by_combination[average_by_combination &gt; average_response_rate]\ncombinations_above_average\n\nRcode  Fcode  Mcode\n1      1      4        0.200000\n              5        0.173913\n       2      2        0.666667\n       3      3        0.333333\n              5        0.153846\n2      1      1        0.400000\n              2        0.333333\n              3        0.150000\n              5        0.172414\n       2      2        0.142857\n              3        0.166667\n              4        0.161290\n       3      3        0.333333\n              4        0.200000\n              5        0.177778\n3      1      1        0.100000\n              2        0.090909\n       3      5        0.136585\n4      1      3        0.140845\n              4        0.111111\n       3      3        0.105263\n              4        0.088496\nName: Yes_Florence, dtype: float64\n\n\nThe index of the data series is a multi-index.\n\ncombinations_above_average.index\n\nMultiIndex([(1, 1, 4),\n            (1, 1, 5),\n            (1, 2, 2),\n            (1, 3, 3),\n            (1, 3, 5),\n            (2, 1, 1),\n            (2, 1, 2),\n            (2, 1, 3),\n            (2, 1, 5),\n            (2, 2, 2),\n            (2, 2, 3),\n            (2, 2, 4),\n            (2, 3, 3),\n            (2, 3, 4),\n            (2, 3, 5),\n            (3, 1, 1),\n            (3, 1, 2),\n            (3, 3, 5),\n            (4, 1, 3),\n            (4, 1, 4),\n            (4, 3, 3),\n            (4, 3, 4)],\n           names=['Rcode', 'Fcode', 'Mcode'])\n\n\n\nsum(train_df.groupby(RFMcategories)['Yes_Florence'].sum()[combinations_above_average.index]) / \\\nsum(train_df.groupby(RFMcategories)['Yes_Florence'].count()[combinations_above_average.index])\n\n0.14523281596452328"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.1 Charles Book Club.html#step-2",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.1 Charles Book Club.html#step-2",
    "title": "Case 21.1 Charles Book Club",
    "section": "Step 2:",
    "text": "Step 2:\nSuppose that we decide to send promotional mail only to the “above-average” RFM combinations identified in part 1. Compute the response rate in the validation data using these combinations.\n\n# group validation data by RFM combinations\ngrouped = valid_df.groupby(RFMcategories)\n\n# count the number of responses for each group\nrfm_counts = grouped['Yes_Florence'].count()\nrfm_responses = grouped['Yes_Florence'].sum()\n\n# and subset to combinations that were above average for the training set\nabove_average_rfm_counts = rfm_counts[combinations_above_average.index]\nabove_average_rfm_responses = rfm_responses[combinations_above_average.index]\n\n\n# Total number of responses in validation set when sending to above average combinations\nprint('Number of customers in above average combinations (validation)', above_average_rfm_counts.sum())\nprint('Responses from customers in above average combinations (validation)', above_average_rfm_responses.sum())\n\nprint('response rate', above_average_rfm_responses.sum() / above_average_rfm_counts.sum())\n\nNumber of customers in above average combinations (validation) 606.0\nResponses from customers in above average combinations (validation) 53.0\nresponse rate 0.08745874587458746"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.1 Charles Book Club.html#step-3",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.1 Charles Book Club.html#step-3",
    "title": "Case 21.1 Charles Book Club",
    "section": "Step 3:",
    "text": "Step 3:\nRework parts 1 and 2 with three segments:\n\nSegment 1: RFM combinations that have response rates that exceed twice the overall response rate\nSegment 2: RFM combinations that exceed the overall response rate but do not exceed twice that rate\nSegment 3: the remaining RFM combinations\n\nDraw the lift curve (consisting of three points for these three segments) showing the number of customers in the validation dataset on the \\(x\\)-axis and cumulative number of buyers in the validation dataset on the \\(y\\)-axis.\n\\end{enumerate}\n\ngrouped = valid_df.groupby(RFMcategories)['Yes_Florence']\ncustomers_by_rfm = grouped.count()\nresponse_by_rfm = grouped.sum()\n\n# Create the segments \nsegment_1 = average_by_combination[2 * average_response_rate &lt; average_by_combination]\nsegment_2 = average_by_combination[(average_response_rate &lt; average_by_combination) & (average_by_combination &lt;= 2 * average_response_rate)]\nsegment_3 = average_by_combination[average_by_combination &lt;= average_response_rate]\n\n# Calculate number of customers, responses, and rate for each segment\ncustomers_1 = customers_by_rfm[segment_1.index].sum()\nresponse_1 = response_by_rfm[segment_1.index].sum()\nrate_1 = response_1 / customers_1\nprint(f'Segment 1: {customers_1:.0f} customers, {response_1:.0f} responses, {rate_1:.3f} response rate')\n\ncustomers_2 = customers_by_rfm[segment_2.index].sum()\nresponse_2 = response_by_rfm[segment_2.index].sum()\nrate_2 = response_2 / customers_2\nprint(f'Segment 2: {customers_2:.0f} customers, {response_2:.0f} responses, {rate_2:.3f} response rate')\n\ncustomers_3 = customers_by_rfm[segment_3.index].sum()\nresponse_3 = response_by_rfm[segment_3.index].sum()\nrate_3 = response_3 / customers_3\nprint(f'Segment 3: {customers_3:.0f} customers, {response_3:.0f} responses, {rate_3:.3f} response rate')\n\nSegment 1: 106 customers, 10 responses, 0.094 response rate\nSegment 2: 500 customers, 43 responses, 0.086 response rate\nSegment 3: 990 customers, 72 responses, 0.073 response rate\n\n\n\ndata = {\n    'Number of customers': [customers_1, customers_2, customers_3], \n    'Number of responses': [response_1, response_2, response_3],\n    'Response rate': [rate_1, rate_2, rate_3],\n}\n\nnumber_of_customers_valid = valid_df[outcome].count()\nnumber_of_responses_valid = valid_df[outcome].sum()\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n\nxlim = (0, 1.1 * number_of_customers_valid)\nylim = (0, 1.1 * max(data['Response rate']))\npd.DataFrame(data).plot.scatter(x='Number of customers', y='Response rate', xlim=xlim, ylim=ylim, ax=axes[0])\naxes[0].axhline(valid_df[outcome].mean(), color='grey', linestyle='--')\naxes[0].set_title('Lift chart')\n\nylim = (0, 1.1 * number_of_responses_valid)\npd.DataFrame(data).plot.scatter(x='Number of customers', y='Number of responses', xlim=xlim, ylim=ylim, ax=axes[1])\naxes[1].plot((0, valid_df[outcome].count()), (0, valid_df[outcome].sum()), color='grey', linestyle='--')\naxes[1].set_title('Gains chart')\n\nplt.show()\n\n\n\n\n\n\n\n\nWe can see that for segments 1 and 2, we get an above average response rate (grey line).\nNote: the outcome of this analysis greatly depends on the split of the dataset into training and validation sets. Vary the random_state and observe the effect it has on the response rates for the various segments"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.1 Charles Book Club.html#step-4",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.1 Charles Book Club.html#step-4",
    "title": "Case 21.1 Charles Book Club",
    "section": "Step 4:",
    "text": "Step 4:\nUse the \\(k\\)-NN approach to classify cases with \\(k=1, 2, ..., 11\\), using Florence as the outcome variable. Based on the validation set, find the best \\(k\\). Remember to normalize all five variables. With the validation data, create a gains chart for the best \\(k\\) model, and report the expected gains for the same number of customers that we mailed to in part 2\n\nuniform weights\n\noutcome = 'Yes_Florence'\npredictors = ['M', 'R', 'F', 'FirstPurch', 'Related Purchase']\n\n# train standard scaler using the training set\nscaler = preprocessing.StandardScaler()\nscaler.fit(train_df[predictors])\n\ntrain_norm = scaler.transform(train_df[predictors])\nvalid_norm = scaler.transform(valid_df[predictors])\n\nresults = []\nfor k in range(1, 12):\n    knn = KNeighborsClassifier(n_neighbors=k).fit(train_norm, train_y)\n    results.append({\n        'k': k,\n        'accuracy': accuracy_score(valid_y, knn.predict(valid_norm))\n    })\nresults = pd.DataFrame(results)\nresults.plot.scatter(x='k', y='accuracy')\nresults\n\n\n\n\n\n\n\n\nk\naccuracy\n\n\n\n\n0\n1\n0.846250\n\n\n1\n2\n0.912500\n\n\n2\n3\n0.895625\n\n\n3\n4\n0.917500\n\n\n4\n5\n0.913750\n\n\n5\n6\n0.919375\n\n\n6\n7\n0.916875\n\n\n7\n8\n0.918750\n\n\n8\n9\n0.916875\n\n\n9\n10\n0.920000\n\n\n10\n11\n0.918750\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBased on this result, we select a \\(k\\) value of 6.\n\nknn = KNeighborsClassifier(n_neighbors=6).fit(train_norm, train_y)\n\ngains_df = pd.DataFrame({\n    'actual': valid_y,\n    'prob': knn.predict_proba(valid_norm)[:, 1]\n})\n\ngains_df = gains_df.sort_values(by=['prob'], ascending=False).reset_index(drop=True)\n\ngainsChart(gains_df.actual)\nplt.show()\n\n\n\n\n\n\n\n\n\nprint('Expected response rates for sending to same number of customers as in step 2')\nfor customers in (customers_1, customers_2, customers_3):\n    expected = gains_df.loc[0:customers,][\"actual\"].sum()\n    ratio = expected / customers\n    print(f'  {customers:.0f} customers: expect {expected} responses ({ratio:.3f} response rate)')\n\nExpected response rates for sending to same number of customers as in step 2\n  106 customers: expect 10 responses (0.094 response rate)\n  500 customers: expect 34 responses (0.068 response rate)\n  990 customers: expect 81 responses (0.082 response rate)\n\n\nWith the unweighted \\(k\\)-NN classifier, the probabilities can take only up to \\(k+1\\) unique values. The gains chart therefore depends greatly on the order of the actual values for the same probability.\n\n# unique probability values and their number of occurrence\ngains_df['prob'].value_counts()\n\n0.000000    988\n0.166667    417\n0.333333    150\n0.500000     40\n0.666667      3\n0.833333      2\nName: prob, dtype: int64\n\n\nIt is in this case therefore better to calculate the gains for these discrete probability values and assume linear behavior of the gains chart between these points (random sampling)\n\nprobability_cutoff = [1] + sorted(gains_df['prob'].unique(), reverse=True)\n\ncumulative_count = []\ncumulative_gains = []\nfor cutoff in probability_cutoff:\n    subset = gains_df['actual'][gains_df['prob'] &gt;= cutoff]\n    cumulative_count.append(subset.count())\n    cumulative_gains.append(subset.sum())\ndiscrete_gains = pd.DataFrame({\n    'prob': probability_cutoff,\n    'count': cumulative_count,\n    'gains': cumulative_gains,\n})\nax = discrete_gains.plot(x='count', y='gains', color='grey')\ndiscrete_gains.plot.scatter(x='count', y='gains', ax=ax)\nax.plot((0, valid_df[outcome].count()), (0, valid_df[outcome].sum()), color='grey', linestyle='--')\nax.set_title('Gains chart')\nax.set_xlabel('# records')\nax.set_ylabel('# cumulative gains')\nplt.show()\n\n\n\n\n\n\n\n\nThis gains chart gives a much more realistic picture of the model behavior."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.1 Charles Book Club.html#step-5",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.1 Charles Book Club.html#step-5",
    "title": "Case 21.1 Charles Book Club",
    "section": "Step 5:",
    "text": "Step 5:\nThe \\(k\\)-NN prediction algorithm gives a numerical value, which is a weighted average of the values of the Florence variable for the \\(k\\)-NN with weights that are inversely proportional to distance. Using the best \\(k\\) that you calculated above with \\(k\\)-NN classification, now run a model with \\(k\\)-NN prediction and compute a gains chart for the validation data. Use all 5 predictors and normalized data. What is the range within which a prediction will fall? How does this result compare to the output you get with the \\(k\\)-NN classification?\n\nresults = []\nfor k in range(1, 12):\n    knn = KNeighborsClassifier(n_neighbors=k, weights='distance').fit(train_norm, train_y)\n    results.append({\n        'k': k,\n        'accuracy': accuracy_score(valid_y, knn.predict(valid_norm))\n    })\nresults = pd.DataFrame(results)\nresults.plot.scatter(x='k', y='accuracy')\nplt.show()\n\n\n\n\n\n\n\n\n\n# select k=10\nknn = KNeighborsClassifier(n_neighbors=10, weights='distance').fit(train_norm, train_y)\n\ngains_df = pd.DataFrame({\n    'actual': valid_y,\n    'prob': knn.predict_proba(valid_norm)[:, 1]\n})\n\ngains_df = gains_df.sort_values(by=['prob'], ascending=False).reset_index(drop=True)\n\ngainsChart(gains_df.actual)\nplt.show()\n\n\n\n\n\n\n\n\n\nprint('Expected response rates for sending to same number of customers as in step 2')\nfor customers in (customers_1, customers_2, customers_3):\n    expected = gains_df.loc[0:customers,][\"actual\"].sum()\n    ratio = expected / customers\n    print(f'  {customers:.0f} customers: expect {expected} responses ({ratio:.3f} response rate)')\n\nExpected response rates for sending to same number of customers as in step 2\n  106 customers: expect 13 responses (0.123 response rate)\n  500 customers: expect 39 responses (0.078 response rate)\n  990 customers: expect 80 responses (0.081 response rate)\n\n\nWith the distance weighting, we get more variation in the predicted probabilities. However, it should be noted that for a large majority of customers, we predict a probability of 0. This means that in theory we should construct the gains chart similarly to what we have done for the unweighted \\(k\\)-NN model. This would however only replace the fluctuating gains line above about # records 850 with a straight line.\n\n# Number of unique probability values \nprint(gains_df['prob'].nunique())\n\n# Distribution of discrete values\ngains_df['prob'].value_counts()\n\n835\n\n\n0.000000    746\n1.000000      5\n0.102809      3\n0.046389      2\n0.110845      2\n           ... \n0.412269      1\n0.281657      1\n0.147823      1\n0.054422      1\n0.077897      1\nName: prob, Length: 835, dtype: int64\n\n\nThe density plot shows the distribution of the probabilities more clearly.\n\ngains_df['prob'].plot.density(bw_method=0.05)\nplt.show()"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.1 Charles Book Club.html#step-6",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.1 Charles Book Club.html#step-6",
    "title": "Case 21.1 Charles Book Club",
    "section": "Step 6:",
    "text": "Step 6:\nCreate a cumulative gains chart summarizing the results from the three logistic regression models created above, along with the expected cumulative gains for a random selection of an equal number of customers from the validation dataset.\n\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(15, 4))\n\nresult_1 = pd.DataFrame({\n    'actual': valid_y,\n    'prob': logit_reg_1.predict_proba(valid_df[predictors1])[:, 1]\n})\n\nresult_1 = result_1.sort_values(by=['prob'], ascending=False).reset_index(drop=True)\ngainsChart(result_1.actual, ax=axes[0])\naxes[0].set_title('Model 1')\n\nresult_2 = pd.DataFrame({\n    'actual': valid_y,\n    'prob': logit_reg_2.predict_proba(valid_df[predictors2])[:, 1]\n})\n\nresult_2 = result_2.sort_values(by=['prob'], ascending=False).reset_index(drop=True)\ngainsChart(result_2.actual, ax=axes[1])\naxes[1].set_title('Model 2')\n\nresult_3 = pd.DataFrame({\n    'actual': valid_y,\n    'prob': logit_reg_3.predict_proba(valid_df[predictors3])[:, 1]\n})\n\nresult_3 = result_3.sort_values(by=['prob'], ascending=False).reset_index(drop=True)\ngainsChart(result_3.actual, ax=axes[2])\naxes[2].set_title('Model 3')\nplt.show()\n\n\n\n\n\n\n\n\n\nprint('Expected response rates for sending to same number of customers as in step 2')\nfor model, model_gains in enumerate([result_1, result_2, result_3], 1):\n    print(f'  Model {model}')\n    for customers in (customers_1, customers_2, customers_3):\n        expected = model_gains.loc[0:customers,][\"actual\"].sum()\n        ratio = expected / customers\n        print(f'    {customers:.0f} customers: expect {expected} responses ({ratio:.3f} response rate)')\n\nExpected response rates for sending to same number of customers as in step 2\n  Model 1\n    106 customers: expect 19 responses (0.179 response rate)\n    500 customers: expect 59 responses (0.118 response rate)\n    990 customers: expect 95 responses (0.096 response rate)\n  Model 2\n    106 customers: expect 21 responses (0.198 response rate)\n    500 customers: expect 49 responses (0.098 response rate)\n    990 customers: expect 93 responses (0.094 response rate)\n  Model 3\n    106 customers: expect 11 responses (0.104 response rate)\n    500 customers: expect 52 responses (0.104 response rate)\n    990 customers: expect 93 responses (0.094 response rate)"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.1 Charles Book Club.html#step-7",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.1 Charles Book Club.html#step-7",
    "title": "Case 21.1 Charles Book Club",
    "section": "Step 7:",
    "text": "Step 7:\nIf the cutoff criterion for a campaign is a 30% likelihood of a purchase, find the customers in the validation data that would be targeted and count the number of buyers in this set.\n\ntarget_1 = result_1[result_1.prob &gt;= 0.3]\ntarget_2 = result_2[result_2.prob &gt;= 0.3]\ntarget_3 = result_3[result_3.prob &gt;= 0.3]\n\nfor model, target in enumerate([target_1, target_2, target_3], 1):\n    count = target[\"actual\"].count()\n    expected = target[\"actual\"].sum()\n    print(f'Model {model}')\n    if count &gt; 0:\n        ratio = expected / count\n        print(f'  {count} customers: expect {expected} responses ({ratio:.3f} response rate)')\n    else:\n        print(f'  {count} customers: expect {expected} responses')\n\nModel 1\n  35 customers: expect 10 responses (0.286 response rate)\nModel 2\n  32 customers: expect 9 responses (0.281 response rate)\nModel 3\n  0 customers: expect 0 responses"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.3 Tayko software cataloger.html",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.3 Tayko software cataloger.html",
    "title": "Case 21.3 Tayko software cataloger",
    "section": "",
    "text": "2019 Galit Shmueli, Peter C. Bruce, Peter Gedeck\n\nCase study included in\nData Mining for Business Analytics: Concepts, Techniques, and Applications in Python (First Edition) Galit Shmueli, Peter C. Bruce, Peter Gedeck, and Nitin R. Patel. 2019.\n%matplotlib inline\n\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler\nfrom dmba import stepwise_selection, regressionSummary, gainsChart, AIC_score\n\nDATA = Path('.').resolve().parent / 'data'"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.3 Tayko software cataloger.html#load-the-data",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.3 Tayko software cataloger.html#load-the-data",
    "title": "Case 21.3 Tayko software cataloger",
    "section": "Load the data",
    "text": "Load the data\n\ntayko_df = pd.read_csv(DATA / 'Tayko.csv')\nprint(tayko_df.shape)\ntayko_df.head()\n\n(2000, 25)\n\n\n\n\n\n\n\n\n\nsequence_number\nUS\nsource_a\nsource_c\nsource_b\nsource_d\nsource_e\nsource_m\nsource_o\nsource_h\n...\nsource_x\nsource_w\nFreq\nlast_update_days_ago\n1st_update_days_ago\nWeb order\nGender=male\nAddress_is_res\nPurchase\nSpending\n\n\n\n\n0\n1\n1\n0\n0\n1\n0\n0\n0\n0\n0\n...\n0\n0\n2\n3662\n3662\n1\n0\n1\n1\n128\n\n\n1\n2\n1\n0\n0\n0\n0\n1\n0\n0\n0\n...\n0\n0\n0\n2900\n2900\n1\n1\n0\n0\n0\n\n\n2\n3\n1\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n2\n3883\n3914\n0\n0\n0\n1\n127\n\n\n3\n4\n1\n0\n1\n0\n0\n0\n0\n0\n0\n...\n0\n0\n1\n829\n829\n0\n1\n0\n0\n0\n\n\n4\n5\n1\n0\n1\n0\n0\n0\n0\n0\n0\n...\n0\n0\n1\n869\n869\n0\n0\n0\n0\n0\n\n\n\n\n5 rows × 25 columns\n\n\n\n\ntayko_df.columns\n\nIndex(['sequence_number', 'US', 'source_a', 'source_c', 'source_b', 'source_d',\n       'source_e', 'source_m', 'source_o', 'source_h', 'source_r', 'source_s',\n       'source_t', 'source_u', 'source_p', 'source_x', 'source_w', 'Freq',\n       'last_update_days_ago', '1st_update_days_ago', 'Web order',\n       'Gender=male', 'Address_is_res', 'Purchase', 'Spending'],\n      dtype='object')"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.3 Tayko software cataloger.html#step-1",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.3 Tayko software cataloger.html#step-1",
    "title": "Case 21.3 Tayko software cataloger",
    "section": "Step 1",
    "text": "Step 1\nEach catalog costs approximately $2 to mail (including printing, postage, and mailing costs). Estimate the gross profit that the firm could expect from the remaining 180,000 names if it selects them randomly from the pool.\n\n# Calculate average spending from the 1,000 purchasers in the stratified sample\ncustomersWithPurchase = tayko_df[tayko_df['Purchase'] == 1]\naverageSpending = customersWithPurchase['Spending'].mean()\nprint('Average Spending', averageSpending)\n\n# Expected average spending per customer\nprint('Expected average spending', 0.053 * averageSpending)\n\n# Expected average profit per customer (remove cost of mailing)\nprint('Expected average profit', 0.053 * averageSpending - 2)\n\n# Expected profit from remaining 180,000 names for random selection\nprint('Expected profit', 180000 * (0.053 * averageSpending - 2))\n\nAverage Spending 205.249\nExpected average spending 10.878197\nExpected average profit 8.878197\nExpected profit 1598075.46\n\n\nFor a random selection, the company expects a profit of about 1.6 million USD"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.3 Tayko software cataloger.html#step-2",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.3 Tayko software cataloger.html#step-2",
    "title": "Case 21.3 Tayko software cataloger",
    "section": "Step 2",
    "text": "Step 2\nDevelop a model for classifying a customer as a purchaser or nonpurchaser.\n\nPartition the data randomly into a training set (800 records), validation set (700 records), and test set (500 records).\n\n\n# alternative way using scikit-learn\ntrain, temp = train_test_split(tayko_df, train_size=800, random_state=1)\nvalid, test = train_test_split(temp, train_size=700, random_state=1)\n\nprint('Training   : ', train.shape)\nprint('Validation : ', valid.shape)\nprint('Test : ', test.shape)\n\nTraining   :  (800, 25)\nValidation :  (700, 25)\nTest :  (500, 25)\n\n\n\nRun logistic regression with L2 penalty, using method LogisticRegressionCV, to select the best subset of variables, then use this model to classify the data into purchasers and nonpurchasers. Use only the training set for running the model. (Logistic regression is used because it yields an estimated “probability of purchase,” which is required later in the analysis.)\n\n\npredictors = list(tayko_df.columns)\noutcome = 'Purchase'\npredictors.remove(outcome)\npredictors.remove('Spending')\npredictors.remove('sequence_number')\nprint(predictors)\n\ntrain_X = train[predictors]\ntrain_y = train[outcome]\nvalid_X = valid[predictors]\nvalid_y = valid[outcome]\n\n# scale = MinMaxScaler()\n# train_X = scale.fit_transform(train_X)\nmodel = LogisticRegressionCV(penalty=\"l2\", solver='lbfgs', cv=5, max_iter=500)\nmodel.fit(train_X, train_y)\n\nprint('regularization', model.C_)\nprint('intercept ', model.intercept_[0])\ndf = pd.DataFrame({\n    'coeff': model.coef_[0], \n    'abs_coeff': np.abs(model.coef_[0])\n}, index=predictors)\nprint(df.sort_values(by=['abs_coeff'], ascending=False))\n\n['US', 'source_a', 'source_c', 'source_b', 'source_d', 'source_e', 'source_m', 'source_o', 'source_h', 'source_r', 'source_s', 'source_t', 'source_u', 'source_p', 'source_x', 'source_w', 'Freq', 'last_update_days_ago', '1st_update_days_ago', 'Web order', 'Gender=male', 'Address_is_res']\nregularization [166.81005372]\nintercept  -2.448368340536965\n                         coeff  abs_coeff\nsource_h             -2.609225   2.609225\nFreq                  1.924103   1.924103\nsource_a              1.205172   1.205172\nsource_u              1.194266   1.194266\nsource_c             -0.852761   0.852761\nWeb order             0.838118   0.838118\nsource_x              0.703198   0.703198\nsource_r              0.686858   0.686858\nAddress_is_res       -0.654618   0.654618\nsource_p              0.417491   0.417491\nsource_o             -0.406499   0.406499\nGender=male          -0.359065   0.359065\nsource_b             -0.337667   0.337667\nsource_s             -0.331861   0.331861\nsource_w              0.276314   0.276314\nsource_t              0.269813   0.269813\nsource_m             -0.258662   0.258662\nsource_d              0.102165   0.102165\nUS                   -0.089009   0.089009\nsource_e              0.020144   0.020144\n1st_update_days_ago  -0.000045   0.000045\nlast_update_days_ago -0.000008   0.000008\n\n\n\ndef train_model(variables):\n    if len(variables) == 0:\n        return None\n    model = LogisticRegressionCV(penalty=\"l2\", solver='saga', cv=5, max_iter=5000)\n    return model.fit(train_X[variables], train_y)\n\ndef score_model(model, variables):\n    if len(variables) == 0:\n        return 0\n    logit_reg_valid = model.predict(valid_X[variables])\n    return -accuracy_score(valid_y, [1 if p &gt; 0.5 else 0 for p in logit_reg_valid])\n\nlogreg_model, best_variables = stepwise_selection(predictors, train_model, score_model, \n                                           direction='forward', verbose=True)\nprint(best_variables)\nlogreg_model_predictors = best_variables\n\nVariables: US, source_a, source_c, source_b, source_d, source_e, source_m, source_o, source_h, source_r, source_s, source_t, source_u, source_p, source_x, source_w, Freq, last_update_days_ago, 1st_update_days_ago, Web order, Gender=male, Address_is_res\nStart: score=0.00, constant\nStep: score=-0.71, add Freq\nStep: score=-0.77, add Web order\nStep: score=-0.81, add source_h\nStep: score=-0.82, add source_u\nStep: score=-0.83, add source_c\nStep: score=-0.84, add source_b\nStep: score=-0.84, unchanged None\n['Freq', 'Web order', 'source_h', 'source_u', 'source_c', 'source_b']"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.3 Tayko software cataloger.html#step-3",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.3 Tayko software cataloger.html#step-3",
    "title": "Case 21.3 Tayko software cataloger",
    "section": "Step 3",
    "text": "Step 3\nDevelop a model for predicting spending among the purchasers.\n\nCreate subsets of the training and validation sets for only purchasers’ records by filtering for Purchase = 1.\n\n\npurchaseIDs = list(tayko_df.loc[tayko_df.Purchase == 1].sequence_number)\nlen(purchaseIDs)\n\n1000\n\n\n\nDevelop models for predicting spending with the filtered datasets, using:\n\nMultiple linear regression (use stepwise regression)\nRegression trees\n\n\nChoose one model on the basis of its performance on the validation data.\n\ntrainP = train.loc[train.Purchase == 1]\nvalidP = valid.loc[valid.Purchase == 1]\ntestP = test.loc[test.Purchase == 1]\n\nprint('Training   : ', trainP.shape)\nprint('Validation : ', validP.shape)\nprint('Test : ', testP.shape)\n\nTraining   :  (389, 25)\nValidation :  (361, 25)\nTest :  (250, 25)\n\n\n\noutcome = 'Spending'\n\ntrain_X = trainP[predictors]\ntrain_y = trainP[outcome]\nvalid_X = validP[predictors]\nvalid_y = validP[outcome]\ntest_X = testP[predictors]\ntest_y = testP[outcome]\n\n# Linear regression model\ndef train_model(variables):\n    if len(variables) == 0:\n        return None\n    model = LinearRegression()\n    return model.fit(train_X[variables], train_y)\n\ndef score_model(model, variables):\n    if len(variables) == 0:\n        return mean_squared_error(train_y, [train_y.mean()] * len(train_y), model)\n    return mean_squared_error(train_y, model.predict(train_X[variables]))\n\n\nlinear_model, best_variables = stepwise_selection(predictors, train_model, score_model, \n                                                  direction='stepwise', verbose=True)\nprint(best_variables)\nlinear_predictors = best_variables\n\nVariables: US, source_a, source_c, source_b, source_d, source_e, source_m, source_o, source_h, source_r, source_s, source_t, source_u, source_p, source_x, source_w, Freq, last_update_days_ago, 1st_update_days_ago, Web order, Gender=male, Address_is_res\nStart: score=54593.78, constant\nStep: score=31171.49, add Freq\nStep: score=29827.64, add Address_is_res\nStep: score=28830.95, add last_update_days_ago\nStep: score=28584.23, add source_e\nStep: score=28477.11, add source_d\nStep: score=28372.67, add source_p\nStep: score=28286.78, add source_w\nStep: score=28203.33, add source_t\nStep: score=28136.43, add source_b\nStep: score=28071.55, add Gender=male\nStep: score=28010.85, add source_x\nStep: score=27952.64, add source_h\nStep: score=27902.76, add source_m\nStep: score=27864.70, add source_r\nStep: score=27834.74, add 1st_update_days_ago\nStep: score=27815.09, add US\nStep: score=27806.71, add source_o\nStep: score=27800.61, add source_u\nStep: score=27797.02, add source_c\nStep: score=27794.33, add source_s\nStep: score=27777.20, add source_a\nStep: score=27777.15, add Web order\nStep: score=27777.15, unchanged None\n['Freq', 'Address_is_res', 'last_update_days_ago', 'source_e', 'source_d', 'source_p', 'source_w', 'source_t', 'source_b', 'Gender=male', 'source_x', 'source_h', 'source_m', 'source_r', '1st_update_days_ago', 'US', 'source_o', 'source_u', 'source_c', 'source_s', 'source_a', 'Web order']\n\n\n\npd.DataFrame({'x': valid_y, 'y': linear_model.predict(valid_X[best_variables])}).plot.scatter(x='x', y='y')\nprint('Validation performance')\nregressionSummary(valid_y, linear_model.predict(valid_X[best_variables]))\nprint('Test performance')\nregressionSummary(test_y, linear_model.predict(test_X[best_variables]))\n\nValidation performance\n\nRegression statistics\n\n                      Mean Error (ME) : -3.9070\n       Root Mean Squared Error (RMSE) : 165.2843\n            Mean Absolute Error (MAE) : 106.7053\n          Mean Percentage Error (MPE) : -108.4046\nMean Absolute Percentage Error (MAPE) : 139.4048\nTest performance\n\nRegression statistics\n\n                      Mean Error (ME) : -4.1470\n       Root Mean Squared Error (RMSE) : 158.1129\n            Mean Absolute Error (MAE) : 102.9828\n          Mean Percentage Error (MPE) : -75.3418\nMean Absolute Percentage Error (MAPE) : 112.2414\n\n\n\n\n\n\n\n\n\n\n# regression tree\ndef train_model(variables):\n    if len(variables) == 0:\n        return None\n    model = DecisionTreeRegressor()\n    return model.fit(train_X[variables], train_y)\n\ndef score_model(model, variables):\n    if len(variables) == 0:\n        return mean_squared_error(train_y, [train_y.mean()] * len(train_y), model)\n    return mean_squared_error(train_y, model.predict(train_X[variables]))\n\ntree_model, best_variables = stepwise_selection(predictors, train_model, score_model, direction='forward', verbose=True)\nprint(best_variables)\n\nVariables: US, source_a, source_c, source_b, source_d, source_e, source_m, source_o, source_h, source_r, source_s, source_t, source_u, source_p, source_x, source_w, Freq, last_update_days_ago, 1st_update_days_ago, Web order, Gender=male, Address_is_res\nStart: score=54593.78, constant\nStep: score=4390.71, add last_update_days_ago\nStep: score=350.15, add 1st_update_days_ago\nStep: score=29.11, add Web order\nStep: score=7.53, add source_a\nStep: score=0.00, add source_u\nStep: score=0.00, unchanged None\n['last_update_days_ago', '1st_update_days_ago', 'Web order', 'source_a', 'source_u']\n\n\n\npd.DataFrame({'x': valid_y, 'y': tree_model.predict(valid_X[best_variables])}).plot.scatter(x='x', y='y')\nprint('Validation performance')\nregressionSummary(valid_y, tree_model.predict(valid_X[best_variables]))\nprint('Test performance')\nregressionSummary(test_y, tree_model.predict(test_X[best_variables]))\n\nValidation performance\n\nRegression statistics\n\n                      Mean Error (ME) : 4.5152\n       Root Mean Squared Error (RMSE) : 249.2594\n            Mean Absolute Error (MAE) : 148.6371\n          Mean Percentage Error (MPE) : -98.8897\nMean Absolute Percentage Error (MAPE) : 150.5093\nTest performance\n\nRegression statistics\n\n                      Mean Error (ME) : 4.8760\n       Root Mean Squared Error (RMSE) : 252.7220\n            Mean Absolute Error (MAE) : 146.3480\n          Mean Percentage Error (MPE) : -101.7217\nMean Absolute Percentage Error (MAPE) : 152.5564\n\n\n\n\n\n\n\n\n\nBased on the validation set performance (mean absolute errors), we select the linear regression model."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.3 Tayko software cataloger.html#step-4",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.3 Tayko software cataloger.html#step-4",
    "title": "Case 21.3 Tayko software cataloger",
    "section": "Step 4",
    "text": "Step 4\nReturn to the original test data partition. Note that this test data partition includes both purchasers and nonpurchasers. Create a new data frame called Score Analysis that contains the test data portion of this dataset.\n\nAdd a column to the data frame with the predicted scores from the logistic regression.\nAdd another column with the predicted spending amount from the prediction model chosen.\nAdd a column for “adjusted probability of purchase” by multiplying “predicted probability of purchase” by 0.107. This is to adjust for oversampling the purchasers (see earlier description).\nAdd a column for expected spending: adjusted probability of purchase × predicted spending.\nPlot the cumulative gains chart of the expected spending (cumulative expected spending as a function of number of records targeted).\nUsing this cumulative gains curve, estimate the gross profit that would result from mailing to the 180,000 names on the basis of your data mining models.\n\n\nscoreAnalysis = test.copy()\nscoreAnalysis['purchaseProbability'] = logreg_model.predict_proba(test[logreg_model_predictors])[:,1]\nscoreAnalysis['spendingPrediction'] = linear_model.predict(test[linear_predictors])\nscoreAnalysis['adjustedSpendingPrediction'] = 0.107 * scoreAnalysis['spendingPrediction']\nscoreAnalysis['expectedSpending'] = scoreAnalysis['purchaseProbability'] * scoreAnalysis['adjustedSpendingPrediction']\n\nscoreAnalysis\n\n\n\n\n\n\n\n\nsequence_number\nUS\nsource_a\nsource_c\nsource_b\nsource_d\nsource_e\nsource_m\nsource_o\nsource_h\n...\n1st_update_days_ago\nWeb order\nGender=male\nAddress_is_res\nPurchase\nSpending\npurchaseProbability\nspendingPrediction\nadjustedSpendingPrediction\nexpectedSpending\n\n\n\n\n159\n160\n1\n1\n0\n0\n0\n0\n0\n0\n0\n...\n2369\n0\n1\n0\n1\n547\n0.964127\n298.592482\n31.949396\n30.803279\n\n\n703\n704\n1\n0\n0\n0\n0\n0\n0\n0\n0\n...\n609\n0\n1\n0\n1\n162\n0.545787\n157.398579\n16.841648\n9.191953\n\n\n320\n321\n0\n1\n0\n0\n0\n0\n0\n0\n0\n...\n3975\n0\n0\n1\n1\n189\n0.780768\n133.866369\n14.323701\n11.183488\n\n\n382\n383\n1\n0\n0\n0\n0\n0\n0\n0\n0\n...\n3401\n0\n0\n0\n0\n0\n0.058854\n-2.046334\n-0.218958\n-0.012887\n\n\n830\n831\n1\n0\n0\n0\n0\n1\n0\n0\n0\n...\n930\n0\n1\n1\n0\n0\n0.320615\n12.708986\n1.359862\n0.435992\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1768\n1769\n1\n0\n1\n0\n0\n0\n0\n0\n0\n...\n896\n0\n0\n0\n0\n0\n0.018396\n84.526167\n9.044300\n0.166375\n\n\n1899\n1900\n1\n0\n0\n0\n1\n0\n0\n0\n0\n...\n3684\n0\n1\n0\n1\n122\n0.964127\n226.841087\n24.271996\n23.401290\n\n\n654\n655\n1\n0\n0\n0\n0\n0\n0\n0\n0\n...\n4096\n0\n1\n0\n0\n0\n0.058854\n-30.114153\n-3.222214\n-0.189640\n\n\n1676\n1677\n1\n0\n0\n0\n0\n0\n1\n0\n0\n...\n260\n0\n0\n0\n1\n148\n0.320615\n122.622604\n13.120619\n4.206669\n\n\n1007\n1008\n0\n0\n0\n0\n0\n1\n0\n0\n0\n...\n2499\n0\n0\n0\n1\n338\n0.780768\n248.348061\n26.573242\n20.747538\n\n\n\n\n500 rows × 29 columns\n\n\n\n\nfull_result = scoreAnalysis.sort_values(by=['expectedSpending'], ascending=False)\ngainsChart(full_result.Spending)\n\n\n\n\n\n\n\n\n\n# Calculate average spending based on the model\naverageSpending = scoreAnalysis['expectedSpending'].mean()\nprint('Average expected spending', averageSpending)\n\n# Expected profit from remaining 180,000 names for random selection\nprint('Expected profit', 180000 * (averageSpending - 2))\n\nAverage expected spending 10.337972408788579\nExpected profit 1500835.0335819442"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.8 Catalog Cross-Selling.html",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.8 Catalog Cross-Selling.html",
    "title": "Case 21.8 Catalog Cross-Selling",
    "section": "",
    "text": "2019 Galit Shmueli, Peter C. Bruce, Peter Gedeck\n\nCase study included in\nData Mining for Business Analytics: Concepts, Techniques, and Applications in Python (First Edition) Galit Shmueli, Peter C. Bruce, Peter Gedeck, and Nitin R. Patel. 2019.\n\n\n%matplotlib inline\n\nfrom pathlib import Path\nimport pandas as pd\nfrom mlxtend.frequent_patterns import apriori\nfrom mlxtend.frequent_patterns import association_rules\nimport matplotlib.pylab as plt\n\nDATA = Path('.').resolve().parent / 'data'\n\n\nAssignment\nUsing the dataset CatalogCrossSell.csv, perform an association rules analysis, and comment on the results. Your discussion should provide interpretations in English of the meanings of the various output statistics (lift ratio, confidence, support) and include a very rough estimate (precise calculations are not necessary) of the extent to which this will help Exeter make an informed choice about which catalog to cross-promote to a purchaser.\n\n\nLoad the data\n\ndata = pd.read_csv(DATA / 'CatalogCrossSell.csv')\ndata = data.set_index('Customer Number')\ndata = data.rename(columns={n: n.replace(' Division', '') for n in data.columns})\ndata.head()\n\n\n\n\n\n\n\n\nClothing\nHousewares\nHealth Products\nAutomotive\nPersonal Electronics\nComputers\nGarden\nNovelty Gift\nJewelry\n\n\nCustomer Number\n\n\n\n\n\n\n\n\n\n\n\n\n\n11569\n0\n1\n1\n1\n1\n0\n0\n1\n0\n\n\n13714\n0\n1\n1\n1\n1\n0\n1\n1\n1\n\n\n46391\n0\n1\n1\n1\n1\n0\n1\n1\n1\n\n\n67264\n0\n0\n1\n1\n1\n0\n1\n1\n0\n\n\n67363\n0\n0\n1\n0\n1\n0\n1\n1\n0\n\n\n\n\n\n\n\n\n\nItem-frequency plot\n\n# determine item frequencies\nitemFrequency = data.sum(axis=0) / len(data)\nprint(itemFrequency)\n\n# and plot as histogram\nax = itemFrequency.plot.bar(color='blue')\nplt.ylabel('Item frequency (relative)')\nplt.show()\n\nClothing                0.033013\nHousewares              0.393557\nHealth Products         1.000000\nAutomotive              0.134854\nPersonal Electronics    0.467387\nComputers               0.046819\nGarden                  0.272109\nNovelty Gift            0.227491\nJewelry                 0.356943\ndtype: float64\n\n\n\n\n\n\n\n\n\n\n\nAssociation rules analysis\n\n# create frequent itemsets\nitemsets = apriori(data, min_support=0.1, use_colnames=True)\n\n# and convert into rules\nrules = association_rules(itemsets, metric='confidence', min_threshold=0.5)\nprint(f'Number of rules {len(rules)}')\n\n(rules.sort_values(by=['lift'], ascending=False)\n      .drop(columns=['antecedent support', 'consequent support', 'conviction']))#.head(10)\n\nNumber of rules 54\n\n\n\n\n\n\n\n\n\nantecedents\nconsequents\nsupport\nconfidence\nlift\nleverage\n\n\n\n\n46\n(Novelty Gift, Housewares)\n(Personal Electronics, Health Products)\n0.107843\n0.799703\n1.711009\n0.044814\n\n\n44\n(Housewares, Novelty Gift, Health Products)\n(Personal Electronics)\n0.107843\n0.799703\n1.711009\n0.044814\n\n\n26\n(Novelty Gift, Housewares)\n(Personal Electronics)\n0.107843\n0.799703\n1.711009\n0.044814\n\n\n27\n(Jewelry, Personal Electronics)\n(Housewares)\n0.132053\n0.668693\n1.699099\n0.054333\n\n\n51\n(Jewelry, Personal Electronics)\n(Housewares, Health Products)\n0.132053\n0.668693\n1.699099\n0.054333\n\n\n47\n(Jewelry, Personal Electronics, Health Products)\n(Housewares)\n0.132053\n0.668693\n1.699099\n0.054333\n\n\n25\n(Personal Electronics, Novelty Gift)\n(Housewares)\n0.107843\n0.637870\n1.620780\n0.041305\n\n\n45\n(Personal Electronics, Novelty Gift)\n(Housewares, Health Products)\n0.107843\n0.637870\n1.620780\n0.041305\n\n\n42\n(Personal Electronics, Novelty Gift, Health Pr...\n(Housewares)\n0.107843\n0.637870\n1.620780\n0.041305\n\n\n35\n(Novelty Gift)\n(Personal Electronics, Health Products)\n0.169068\n0.743184\n1.590082\n0.062741\n\n\n34\n(Novelty Gift, Health Products)\n(Personal Electronics)\n0.169068\n0.743184\n1.590082\n0.062741\n\n\n11\n(Novelty Gift)\n(Personal Electronics)\n0.169068\n0.743184\n1.590082\n0.062741\n\n\n50\n(Housewares, Personal Electronics, Health Prod...\n(Jewelry)\n0.132053\n0.560748\n1.570974\n0.047995\n\n\n29\n(Personal Electronics, Housewares)\n(Jewelry)\n0.132053\n0.560748\n1.570974\n0.047995\n\n\n53\n(Personal Electronics, Housewares)\n(Jewelry, Health Products)\n0.132053\n0.560748\n1.570974\n0.047995\n\n\n19\n(Novelty Gift, Health Products)\n(Housewares)\n0.134854\n0.592788\n1.506230\n0.045323\n\n\n3\n(Novelty Gift)\n(Housewares)\n0.134854\n0.592788\n1.506230\n0.045323\n\n\n21\n(Novelty Gift)\n(Housewares, Health Products)\n0.134854\n0.592788\n1.506230\n0.045323\n\n\n52\n(Jewelry, Housewares)\n(Personal Electronics, Health Products)\n0.132053\n0.677618\n1.449801\n0.040969\n\n\n49\n(Jewelry, Housewares, Health Products)\n(Personal Electronics)\n0.132053\n0.677618\n1.449801\n0.040969\n\n\n28\n(Jewelry, Housewares)\n(Personal Electronics)\n0.132053\n0.677618\n1.449801\n0.040969\n\n\n22\n(Jewelry, Health Products)\n(Housewares)\n0.194878\n0.545964\n1.387254\n0.054400\n\n\n24\n(Jewelry)\n(Housewares, Health Products)\n0.194878\n0.545964\n1.387254\n0.054400\n\n\n4\n(Jewelry)\n(Housewares)\n0.194878\n0.545964\n1.387254\n0.054400\n\n\n10\n(Garden)\n(Personal Electronics)\n0.163265\n0.600000\n1.283733\n0.036085\n\n\n31\n(Health Products, Garden)\n(Personal Electronics)\n0.163265\n0.600000\n1.283733\n0.036085\n\n\n32\n(Garden)\n(Personal Electronics, Health Products)\n0.163265\n0.600000\n1.283733\n0.036085\n\n\n2\n(Housewares)\n(Personal Electronics)\n0.235494\n0.598373\n1.280252\n0.051551\n\n\n16\n(Personal Electronics)\n(Housewares, Health Products)\n0.235494\n0.503853\n1.280252\n0.051551\n\n\n15\n(Housewares, Health Products)\n(Personal Electronics)\n0.235494\n0.598373\n1.280252\n0.051551\n\n\n1\n(Personal Electronics)\n(Housewares)\n0.235494\n0.503853\n1.280252\n0.051551\n\n\n17\n(Housewares)\n(Personal Electronics, Health Products)\n0.235494\n0.598373\n1.280252\n0.051551\n\n\n13\n(Personal Electronics, Health Products)\n(Housewares)\n0.235494\n0.503853\n1.280252\n0.051551\n\n\n37\n(Jewelry, Health Products)\n(Personal Electronics)\n0.197479\n0.553251\n1.183711\n0.030649\n\n\n38\n(Jewelry)\n(Personal Electronics, Health Products)\n0.197479\n0.553251\n1.183711\n0.030649\n\n\n12\n(Jewelry)\n(Personal Electronics)\n0.197479\n0.553251\n1.183711\n0.030649\n\n\n30\n(Personal Electronics, Garden)\n(Health Products)\n0.163265\n1.000000\n1.000000\n0.000000\n\n\n6\n(Personal Electronics)\n(Health Products)\n0.467387\n1.000000\n1.000000\n0.000000\n\n\n18\n(Housewares, Garden)\n(Health Products)\n0.132853\n1.000000\n1.000000\n0.000000\n\n\n20\n(Novelty Gift, Housewares)\n(Health Products)\n0.134854\n1.000000\n1.000000\n0.000000\n\n\n23\n(Jewelry, Housewares)\n(Health Products)\n0.194878\n1.000000\n1.000000\n0.000000\n\n\n48\n(Jewelry, Personal Electronics, Housewares)\n(Health Products)\n0.132053\n1.000000\n1.000000\n0.000000\n\n\n14\n(Personal Electronics, Housewares)\n(Health Products)\n0.235494\n1.000000\n1.000000\n0.000000\n\n\n5\n(Automotive)\n(Health Products)\n0.134854\n1.000000\n1.000000\n0.000000\n\n\n8\n(Novelty Gift)\n(Health Products)\n0.227491\n1.000000\n1.000000\n0.000000\n\n\n7\n(Garden)\n(Health Products)\n0.272109\n1.000000\n1.000000\n0.000000\n\n\n43\n(Personal Electronics, Novelty Gift, Housewares)\n(Health Products)\n0.107843\n1.000000\n1.000000\n0.000000\n\n\n41\n(Jewelry, Novelty Gift)\n(Health Products)\n0.107243\n1.000000\n1.000000\n0.000000\n\n\n40\n(Jewelry, Garden)\n(Health Products)\n0.127251\n1.000000\n1.000000\n0.000000\n\n\n39\n(Novelty Gift, Garden)\n(Health Products)\n0.112245\n1.000000\n1.000000\n0.000000\n\n\n36\n(Jewelry, Personal Electronics)\n(Health Products)\n0.197479\n1.000000\n1.000000\n0.000000\n\n\n9\n(Jewelry)\n(Health Products)\n0.356943\n1.000000\n1.000000\n0.000000\n\n\n33\n(Personal Electronics, Novelty Gift)\n(Health Products)\n0.169068\n1.000000\n1.000000\n0.000000\n\n\n0\n(Housewares)\n(Health Products)\n0.393557\n1.000000\n1.000000\n0.000000\n\n\n\n\n\n\n\nThe first rule in the table can be interpreted as follows: If the customer purchases from both the Housewares and Novelty catalog, they will also purchase from Personal Electronics. This rule has support of 0.108, or 10.8%. This is the number of people who bought from all three catalogs. It has confidence of 80%, meaning that 80% of the people who bought from Housewares and Novelty also bought from Personal Electronics. Its lift ratio is 1.71, meaning that if we cross-sell Personal Electronics to a person who bought from Housewares and Novelty, they are 71% more likely to buy something than if we cross-sell Personal Electronics to a random customer.\nThe second and third rule have identical metrics to the first rule. It is likely that these are all based on the same set of customers. Indeed, if we filter the data by those customers who bought from Housewares and Novelty Gift, we get the following item frequencies:\n\ndata[(data['Housewares'] == 1) & (data['Novelty Gift'] == 1)].sum(axis=0)\n\nClothing                 51\nHousewares              674\nHealth Products         674\nAutomotive              177\nPersonal Electronics    539\nComputers                59\nGarden                  344\nNovelty Gift            674\nJewelry                 377\ndtype: int64\n\n\nAll of these 674 customers also bought from Health Products. Health Products is the most selling category (see item frequency). In fact, the item frequency for Health Products is 1 which means that every customer has purchased from this catalog. This makes rules all rules that include Health Products (like 2 and 3) redundant.\nIt makes sense to repeat the analysis excluding information from “Health Products”.\n\n\nAssociation rules analysis without “Health Products”\n\n# create frequent itemsets\nitemsets = apriori(data.drop(columns=['Health Products']), min_support=0.1, use_colnames=True)\n\n# and convert into rules\nrules = association_rules(itemsets, metric='confidence', min_threshold=0.5)\nprint(f'Number of rules {len(rules)}')\n\n(rules.sort_values(by=['lift'], ascending=False)\n      .drop(columns=['antecedent support', 'consequent support', 'conviction']))\n\nNumber of rules 12\n\n\n\n\n\n\n\n\n\nantecedents\nconsequents\nsupport\nconfidence\nlift\nleverage\n\n\n\n\n8\n(Novelty Gift, Housewares)\n(Personal Electronics)\n0.107843\n0.799703\n1.711009\n0.044814\n\n\n9\n(Jewelry, Personal Electronics)\n(Housewares)\n0.132053\n0.668693\n1.699099\n0.054333\n\n\n7\n(Personal Electronics, Novelty Gift)\n(Housewares)\n0.107843\n0.637870\n1.620780\n0.041305\n\n\n5\n(Novelty Gift)\n(Personal Electronics)\n0.169068\n0.743184\n1.590082\n0.062741\n\n\n11\n(Personal Electronics, Housewares)\n(Jewelry)\n0.132053\n0.560748\n1.570974\n0.047995\n\n\n2\n(Novelty Gift)\n(Housewares)\n0.134854\n0.592788\n1.506230\n0.045323\n\n\n10\n(Jewelry, Housewares)\n(Personal Electronics)\n0.132053\n0.677618\n1.449801\n0.040969\n\n\n3\n(Jewelry)\n(Housewares)\n0.194878\n0.545964\n1.387254\n0.054400\n\n\n4\n(Garden)\n(Personal Electronics)\n0.163265\n0.600000\n1.283733\n0.036085\n\n\n0\n(Personal Electronics)\n(Housewares)\n0.235494\n0.503853\n1.280252\n0.051551\n\n\n1\n(Housewares)\n(Personal Electronics)\n0.235494\n0.598373\n1.280252\n0.051551\n\n\n6\n(Jewelry)\n(Personal Electronics)\n0.197479\n0.553251\n1.183711\n0.030649\n\n\n\n\n\n\n\nThere are now fewer rules. The first rule is identical to the rules 1 to 3 in our initial analysis. The same is true for rule 2 that corresponds to rules 4 to 6 above.\nWe can see that a critical analysis of the results can lead to a revised approach that leads to simpler, yet identical results."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.4 Political Persuasion.html",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.4 Political Persuasion.html",
    "title": "Case 21.4 Political Persuasion",
    "section": "",
    "text": "2019 Galit Shmueli, Peter C. Bruce, Peter Gedeck\n\nCase study included in\nData Mining for Business Analytics: Concepts, Techniques, and Applications in Python (First Edition) Galit Shmueli, Peter C. Bruce, Peter Gedeck, and Nitin R. Patel. 2019.\n%matplotlib inline\nimport warnings\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pylab as plt\n\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n\nfrom dmba import classificationSummary, gainsChart\n\nDATA = Path('.').resolve().parent / 'data'"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.4 Political Persuasion.html#logistic-regression",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.4 Political Persuasion.html#logistic-regression",
    "title": "Case 21.4 Political Persuasion",
    "section": "Logistic regression",
    "text": "Logistic regression\n\nlogit_reg = LogisticRegressionCV(penalty=\"l2\", solver='saga', cv=5, max_iter=8000)\nlogit_reg.fit(train_X, train_y)\nlogit_reg_confusion = confusionMatrices(logit_reg, 'Logistic regression')\n\nLogistic regression - training results\nConfusion Matrix (Accuracy 0.6221)\n\n       Prediction\nActual    0    1\n     0 3694    1\n     1 2247    6\nLogistic regression - validation results\nConfusion Matrix (Accuracy 0.6347)\n\n       Prediction\nActual    0    1\n     0 2570    1\n     1 1479    2"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.4 Political Persuasion.html#decision-tree-classifier",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.4 Political Persuasion.html#decision-tree-classifier",
    "title": "Case 21.4 Political Persuasion",
    "section": "Decision tree classifier",
    "text": "Decision tree classifier\n\ndtree = DecisionTreeClassifier()\ndtree.fit(train_X, train_y)\ndtree_confusion = confusionMatrices(dtree, 'Decision tree')\n\nDecision tree - training results\nConfusion Matrix (Accuracy 0.9993)\n\n       Prediction\nActual    0    1\n     0 3695    0\n     1    4 2249\nDecision tree - validation results\nConfusion Matrix (Accuracy 0.6024)\n\n       Prediction\nActual    0    1\n     0 1699  872\n     1  739  742"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.4 Political Persuasion.html#random-forest-classifier",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.4 Political Persuasion.html#random-forest-classifier",
    "title": "Case 21.4 Political Persuasion",
    "section": "Random forest classifier",
    "text": "Random forest classifier\n\nrfModel = RandomForestClassifier(n_estimators=100)\nrfModel.fit(train_X, train_y)\nconfusionMatrices(rfModel, 'Random forest')\n\nRandom forest - training results\nConfusion Matrix (Accuracy 0.9993)\n\n       Prediction\nActual    0    1\n     0 3693    2\n     1    2 2251\nRandom forest - validation results\nConfusion Matrix (Accuracy 0.6540)\n\n       Prediction\nActual    0    1\n     0 2035  536\n     1  866  615"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-07-ProbSolutions-KNN.html",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-07-ProbSolutions-KNN.html",
    "title": "Chapter 7: k-Nearest Neighbors (k-NN)",
    "section": "",
    "text": "2019-2020 Galit Shmueli, Peter C. Bruce, Peter Gedeck\n\nData Mining for Business Analytics: Concepts, Techniques, and Applications in Python (First Edition) Galit Shmueli, Peter C. Bruce, Peter Gedeck, and Nitin R. Patel. 2019.\nDate: 2020-03-08\nPython Version: 3.8.2 Jupyter Notebook Version: 5.6.1\nPackages: - pandas: 1.0.1 - scikit-learn: 0.22.2\nThe assistance from Mr. Kuber Deokar and Ms. Anuja Kulkarni in preparing these solutions is gratefully acknowledged.\n\n\n# Import required packages for this chapter\nfrom pathlib import Path\nimport math\n\nimport pandas as pd\n\nfrom sklearn.metrics import pairwise\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, mean_squared_error\nfrom sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor #, NearestNeighbors\n\n%matplotlib inline\n\n\n# Working directory:\n#\n# We assume that data are kept in the same directory as the notebook. If you keep your \n# data in a different folder, replace the argument of the `Path`\nDATA = Path('.')\n# and then load data using \n#\n# pd.read_csv(DATA / ‘filename.csv’)\n\n\nProblem 7.1 Calculating Distance with Categorical Predictors\nThis exercise with a tiny dataset illustrates the calculation of Euclidean distance, and the creation of binary dummies. The online education company Statistics.com segments its customers and prospects into three main categories: IT professionals (IT), statisticians (Stat), and other (Other). It also tracks, for each customer, the number of years since first contact (years). Consider the following customers; information about whether they have taken a course or not (the outcome to be predicted) is included:\nCustomer 1: Stat, 1 year, did not take course\nCustomer 2: Other, 1.1 year, took course\n7.1.a Consider now the following new prospect:\nProspect 1: IT, 1 year\nUsing the above information on the two customers and one prospect, create one dataset for all three with the categorical predictor variable transformed into 2 binaries, and a similar dataset with the categorical predictor variable transformed into 3 binaries.\n7.1.b. For each derived dataset, calculate the Euclidean distance between the prospect and each of the other two customers. (Note: while it is typical to normalize data for k-NN, this is not an iron-clad rule and you may proceed here without normalization.)\n7.1.c. Using k-NN with k = 1, classify the prospect as taking or not taking a course using each of the two derived datasets. Does it make a difference whether you use 2 or 3 dummies?\nAnswer:\n\n# create a data frame, containing the following columns\n# column 'Name\" containing customer names, i.e. Customer1, Customer2 and Cistomer3.\n# columns 'Stat', 'Other' and 'IT' as three dummies for three main categories of customers. \n# column 'TookCourse' containing whether that customer took the course or not, where 1 indicates a course taken and 0 otherwise.\ndata = [['Customer1', 1, 0, 0, 1, 0], ['Customer2', 0, 1, 0, 1.1, 1], ['Customer3', 0, 0, 1, 1]]\ndf = pd.DataFrame(data,columns=['Name','Stat','Other', 'IT', 'Years', 'CourseTaken'])\nprint (df)\n\n        Name  Stat  Other  IT  Years  CourseTaken\n0  Customer1     1      0   0    1.0          0.0\n1  Customer2     0      1   0    1.1          1.0\n2  Customer3     0      0   1    1.0          NaN\n\n\nNote that Customer3 is the new prospect.\n\n# calculate euclidean distances between customers 1 and 3 and customers 2 and 3 using two dummies.\n# drop the dummy 'Other'\ncol = ['Stat', 'IT', 'Years']\ndf2dummies = df[col]\nd = pairwise.pairwise_distances(df2dummies, metric='euclidean')\n# prettify distance matrix by converting to a dataframe\npd.DataFrame(d, columns=df.Name, index=df.Name)\n\n\n\n\n\n\n\nName\nCustomer1\nCustomer2\nCustomer3\n\n\nName\n\n\n\n\n\n\n\nCustomer1\n0.000000\n1.004988\n1.414214\n\n\nCustomer2\n1.004988\n0.000000\n1.004988\n\n\nCustomer3\n1.414214\n1.004988\n0.000000\n\n\n\n\n\n\n\nIf we use two dummies, the prospect (Customer3) is closest to Customer 2. in this case Euclidean distance between the prospect and the second customer is less than the euclidean distance between the prospect and the first customer. Therefore we classify the new customer as taking the course.\n\n# calculate euclidean distances between customers 1 and 3 and customers 2 and 3 using all three dummies.\nfrom sklearn.metrics import pairwise\n# use only relevant columns\ncol = ['Stat', 'Other', 'IT', 'Years']\ndf3dummies = df[col]\nd = pairwise.pairwise_distances(df3dummies, metric='euclidean')\n# prettify distance matrix by converting to a dataframe\npd.DataFrame(d, columns=df.Name, index=df.Name)\n\n\n\n\n\n\n\nName\nCustomer1\nCustomer2\nCustomer3\n\n\nName\n\n\n\n\n\n\n\nCustomer1\n0.000000\n1.417745\n1.414214\n\n\nCustomer2\n1.417745\n0.000000\n1.417745\n\n\nCustomer3\n1.414214\n1.417745\n0.000000\n\n\n\n\n\n\n\nIf we use all three dummies, the prospect (Customer3) is closest to Customer1. In this case the euclidean distance between the prospect and the first customer is less than the euclidean distance between the prospect and the second customer. Therefore we classify the new customer as “not taking the course”.\n\n\nProblem 7.2 Personal Loan Acceptance\nUniversal Bank is a relatively young bank growing rapidly in terms of overall customer acquisition. The majority of these customers are liability customers (depositors) with varying sizes of relationship with the bank. The customer base of asset customers (borrowers) is quite small, and the bank is interested in expanding this base rapidly to bring in more loan business. In particular, it wants to explore ways of converting its liability customers to personal loan customers (while retaining them as depositors).\nA campaign that the bank ran last year for liability customers showed a healthy conversion rate of over 9% success. This has encouraged the retail marketing department to devise smarter campaigns with better target marketing. The goal is to use k-NN to predict whether a new customer will accept a loan offer. This will serve as the basis for the design of a new campaign.\nThe file UniversalBank.csv contains data on 5000 customers. The data include customer demographic information (age, income, etc.), the customer’s relationship with the bank (mortgage, securities account, etc.), and the customer response to the last personal loan campaign (Personal Loan). Among these 5000 customers, only 480 (= 9.6%) accepted the personal loan that was offered to them in the earlier campaign.\nPartition the data into training (60%) and validation (40%) sets.\n7.2.a Consider the following customer:\nAge = 40, Experience = 10, Income = 84, Family = 2, CCAvg = 2, Education_1= 0, Education_2 = 1, Education_3 = 0, Mortgage = 0, Securities Account = 0, CD Account = 0, Online = 1, and Credit Card = 1. Perform a k-NN classification with all predictors except ID and ZIP code using k = 1. Remember to transform categorical predictors with more than two categories into dummy variables first. Specify the success class as 1 (loan acceptance), and use the default cutoff value of 0.5. How would this customer be classified?\nAnswer:\n\nData preparation\nLoad the data and remove unnecessary columns (ID, ZIP Code). Split the data into training (60%) and validation (40%) sets (use random_state=1).\n\n# Load the data\nbank_df = pd.read_csv(DATA / 'UniversalBank.csv')\n\n# Drop ID and zip code columns\nbank_df = bank_df.drop(columns=['ID', 'ZIP Code'])\n\n# Make sure that the result is as expected\nbank_df.head()\n\n\n\n\n\n\n\n\nAge\nExperience\nIncome\nFamily\nCCAvg\nEducation\nMortgage\nPersonal Loan\nSecurities Account\nCD Account\nOnline\nCreditCard\n\n\n\n\n0\n25\n1\n49\n4\n1.6\n1\n0\n0\n1\n0\n0\n0\n\n\n1\n45\n19\n34\n3\n1.5\n1\n0\n0\n1\n0\n0\n0\n\n\n2\n39\n15\n11\n1\n1.0\n1\n0\n0\n0\n0\n0\n0\n\n\n3\n35\n9\n100\n1\n2.7\n2\n0\n0\n0\n0\n0\n0\n\n\n4\n35\n8\n45\n4\n1.0\n2\n0\n0\n0\n0\n0\n1\n\n\n\n\n\n\n\n\n# modify column names\nbank_df.columns = [c.replace(' ', '_').replace('=', '_') for c in bank_df.columns]\nlist(bank_df.columns)\n\n['Age',\n 'Experience',\n 'Income',\n 'Family',\n 'CCAvg',\n 'Education',\n 'Mortgage',\n 'Personal_Loan',\n 'Securities_Account',\n 'CD_Account',\n 'Online',\n 'CreditCard']\n\n\n\n# create dummy variables for categorical variable, we consider Education as categorical variable\nbank_df['Education'] = bank_df['Education'].astype('category')\nbank_df = pd.get_dummies(bank_df, prefix_sep='_', drop_first=False)\nbank_df.head()\n\n\n\n\n\n\n\n\nAge\nExperience\nIncome\nFamily\nCCAvg\nMortgage\nPersonal_Loan\nSecurities_Account\nCD_Account\nOnline\nCreditCard\nEducation_1\nEducation_2\nEducation_3\n\n\n\n\n0\n25\n1\n49\n4\n1.6\n0\n0\n1\n0\n0\n0\n1\n0\n0\n\n\n1\n45\n19\n34\n3\n1.5\n0\n0\n1\n0\n0\n0\n1\n0\n0\n\n\n2\n39\n15\n11\n1\n1.0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n3\n35\n9\n100\n1\n2.7\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n4\n35\n8\n45\n4\n1.0\n0\n0\n0\n0\n0\n1\n0\n1\n0\n\n\n\n\n\n\n\n\n# split dataset into training (60%) and validation (40%) sets\ntrain_df, valid_df = train_test_split(bank_df, test_size=0.4, random_state=1)\nprint('Training set:', train_df.shape, 'Validation set:', valid_df.shape)\n\nTraining set: (3000, 14) Validation set: (2000, 14)\n\n\n\n# new customer\nnewCustomer = pd.DataFrame([{'Age': 40, 'Experience': 10, 'Income': 84, 'Family': 2, 'CCAvg': 2, 'Mortgage': 0,\n                             'Securities_Account': 0, 'CD_Account': 0, 'Online': 1, 'CreditCard': 1, 'Education_1': 0, \n                             'Education_2': 1, 'Education_3': 0}],\n                            columns=['Age', 'Experience', 'Income', 'Family', 'CCAvg', 'Mortgage', 'Securities_Account',\n                                   'CD_Account', 'Online', 'CreditCard', 'Education_1', 'Education_2', 'Education_3'])\nnewCustomer\n\n\n\n\n\n\n\n\nAge\nExperience\nIncome\nFamily\nCCAvg\nMortgage\nSecurities_Account\nCD_Account\nOnline\nCreditCard\nEducation_1\nEducation_2\nEducation_3\n\n\n\n\n0\n40\n10\n84\n2\n2\n0\n0\n0\n1\n1\n0\n1\n0\n\n\n\n\n\n\n\n\n# normalize training and validation sets. The transformation is trained using the training set only.\n# if you don't convert the integer columns to real numbers (float64), \n# the StandardScaler will raise a DataConversionWarning. This is expected\noutcome = 'Personal_Loan'\npredictors = list(bank_df.columns)\npredictors.remove(outcome)\n\nscaler = preprocessing.StandardScaler()\nscaler.fit(train_df[predictors])\nscaler.transform(train_df[predictors])\n# Transform the predictors of training, validation and newCustomer\ntrain_X = scaler.transform(train_df[predictors])\ntrain_y = train_df[outcome]\nvalid_X = scaler.transform(valid_df[predictors])\nvalid_y = valid_df[outcome]\nnewCustomerNorm = pd.DataFrame(scaler.transform(newCustomer), \n                               columns=['Age', 'Experience', 'Income', 'Family', 'CCAvg', 'Mortgage', 'Securities_Account',\n                                   'CD_Account', 'Online', 'CreditCard', 'Education_1', 'Education_2', 'Education_3'])\nprint(newCustomerNorm)\n\n        Age  Experience    Income    Family     CCAvg  Mortgage  \\\n0 -0.486446   -0.901063  0.220892 -0.352127  0.035689 -0.559242   \n\n   Securities_Account  CD_Account   Online  CreditCard  Education_1  \\\n0           -0.337025   -0.252646  0.83419     1.53728    -0.838795   \n\n   Education_2  Education_3  \n0     1.591719    -0.660895  \n\n\n\n# k-NN using k = 1\nknn = KNeighborsClassifier(n_neighbors=1)\nknn.fit(train_X, train_y)\n\nKNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n                     metric_params=None, n_jobs=None, n_neighbors=1, p=2,\n                     weights='uniform')\n\n\n\n# predicted class\nknn.predict(newCustomerNorm)\n\narray([0])\n\n\n\n# predicted probability\nknn.predict_proba(newCustomerNorm)\n\narray([[1., 0.]])\n\n\nNew customer is predicted to not accept a loan offer.\n7.2.b What is a choice of k that balances between overfitting and ignoring the predictor information?\nAnswer\n\n# Train a classifier for different values of k\nresults = []\nfor k in range(1, 20, 2):\n    knn = KNeighborsClassifier(n_neighbors=k).fit(train_X, train_y)\n    results.append({\n        'k': k,\n        'accuracy': accuracy_score(valid_y, knn.predict(valid_X))\n    })\n\n# Convert results to a pandas data frame\nresults = pd.DataFrame(results)\nresults\n\n\n\n\n\n\n\n\nk\naccuracy\n\n\n\n\n0\n1\n0.9545\n\n\n1\n3\n0.9535\n\n\n2\n5\n0.9565\n\n\n3\n7\n0.9520\n\n\n4\n9\n0.9475\n\n\n5\n11\n0.9465\n\n\n6\n13\n0.9450\n\n\n7\n15\n0.9440\n\n\n8\n17\n0.9415\n\n\n9\n19\n0.9405\n\n\n\n\n\n\n\n\n# plot accuracy vs. k\n_ = results.plot.scatter(x='k', y='accuracy', xlim=[0, 20])\n\n\n\n\n\n\n\n\nWe choose the best k, which minimizes the misclassification rate in the validation set. Our best k is k=5\n7.2.c. Show the confusion matrix for the validation data that results from using the best k.\nAnswer\n\n# k-NN model for k = 5\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(train_X, train_y)\n\nknnPredOpt = knn.predict(valid_X)\nprint(confusion_matrix(valid_y, knnPredOpt))\nprint('Accuracy :', accuracy_score(valid_y, knnPredOpt))\n\n[[1803    4]\n [  83  110]]\nAccuracy : 0.9565\n\n\n7.2.d. Consider the following customer: Age = 40, Experience = 10, Income = 84, Family = 2, CCAvg = 2, Education_1 = 0, Education_2 = 1, Education_3 = 0, Mortgage = 0, Securities Account = 0, CD Account = 0, Online = 1 and Credit Card = 1. Classify the customer using the best k.\nAnswer\n\n# predicted class\nknn.predict(newCustomerNorm)\n\narray([0])\n\n\n\n# predicted probability\nknn.predict_proba(newCustomerNorm)\n\narray([[1., 0.]])\n\n\nNew customer is predicted to not accept a loan offer.\n7.2.e. Repartition the data, this time into training, validation, and test sets (50% : 30% : 20%). Apply the k-NN method with the k chosen above. Compare the confusion matrix of the test set with that of the training and validation sets. Comment on the differences and their reason.\nAnswer\n\n# partition the data into training (50%), validation (30%) and test (20%) sets\ntrain_df, temp_df = train_test_split(bank_df, test_size=0.5, random_state=1)\nvalid_df, test_df = train_test_split(temp_df, test_size=0.4, random_state=1)\n\nprint('Training : ', train_df.shape)\nprint('Validation : ', valid_df.shape)\nprint('Test : ', test_df.shape)\n\nTraining :  (2500, 14)\nValidation :  (1500, 14)\nTest :  (1000, 14)\n\n\n\n# normalize training and validation sets. The transformation is trained using the training set only.\n# if you don't convert the integer columns to real numbers (float64), \n# the StandardScaler will raise a DataConversionWarning. This is expected\noutcome = 'Personal_Loan'\npredictors = list(bank_df.columns)\npredictors.remove(outcome)\n\nscaler = preprocessing.StandardScaler()\nscaler.fit(train_df[predictors])\n\n# Transform the predictors of training validation and newCustomer\ntrain_X = scaler.transform(train_df[predictors])\ntrain_y = train_df[outcome]\nvalid_X = scaler.transform(valid_df[predictors])\nvalid_y = valid_df[outcome]\ntest_X = scaler.transform(test_df[predictors])\ntest_y = test_df[outcome]\ntest_X = pd.DataFrame(test_X, columns=['Age', 'Experience', 'Income', 'Family', 'CCAvg', 'Mortgage', 'Securities_Account',\n                                   'CD_Account', 'Online', 'CreditCard', 'Education_1', 'Education_2', 'Education_3'])\ntest_y = pd.DataFrame(test_y, columns=['Personal_Loan'])\n\n\n# k-NN model for best k = 5\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(train_X, train_y)\n\nKNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n                     weights='uniform')\n\n\n\n# confusion matrix of training set\nknnPredOpt = knn.predict(train_X)\nprint(confusion_matrix(train_y, knnPredOpt))\nprint('Accuracy :', accuracy_score(train_y, knnPredOpt))\n\n[[2255    4]\n [  75  166]]\nAccuracy : 0.9684\n\n\n\n# confusion matrix of validation set\nknnPredOpt = knn.predict(valid_X)\nprint(confusion_matrix(valid_y, knnPredOpt))\nprint('Accuracy :', accuracy_score(valid_y, knnPredOpt))\n\n[[1346    3]\n [  67   84]]\nAccuracy : 0.9533333333333334\n\n\n\n# confusion matrix of test set\nknnPredOpt = knn.predict(test_X)\nprint(confusion_matrix(test_y, knnPredOpt))\nprint('Accuracy :', accuracy_score(test_y, knnPredOpt))\n\n[[907   5]\n [ 35  53]]\nAccuracy : 0.96\n\n\nThe error rate increases from the training set to the validation set, but decreases from the validation set to the test set. The differences are small, but this decreased performance, at least in the test set, is unexpected but we can ignore it as difference is very small - both the training and validation sets are used in setting the optimal k so there can be overfitting. The test set was not used to select the optimal k.\n\n\n\nProblem 7.3 Predicting Housing Median Prices\nThe file BostonHousing.csv contains information on over 500 census tracts in Boston, where for each tract multiple variables are recorded. The last column (CAT. MEDV) was derived from MEDV, such that it obtains the value 1 if MEDV &gt; 30 and 0 otherwise. Consider the goal of predicting the median value (MEDV) of a tract, given the information in the first 12 columns. Partition the data into training (60%) and validation (40%) sets.\n7.3.a. Perform a k-NN prediction with all 12 predictors (ignore the CAT. MEDV column), trying values of k from 1 to 5. Make sure to normalize the data. What is the best k? What does it mean?\nAnswer\n\nData preparation\nLoad the data and remove unnecessary columns (CAT. MEDV). Split the data into training (60%) and validation (40%) sets (use random_state=1).\n\n# Load the data\nhouse_df = pd.read_csv(DATA / 'BostonHousing.csv')\n\n# Drop CAT.MEDV column\nhouse_df = house_df.drop(columns=['CAT. MEDV'])\n\n# Make sure that the result is as expected\nhouse_df.head()\n\n\n\n\n\n\n\n\nCRIM\nZN\nINDUS\nCHAS\nNOX\nRM\nAGE\nDIS\nRAD\nTAX\nPTRATIO\nLSTAT\nMEDV\n\n\n\n\n0\n0.00632\n18.0\n2.31\n0\n0.538\n6.575\n65.2\n4.0900\n1\n296\n15.3\n4.98\n24.0\n\n\n1\n0.02731\n0.0\n7.07\n0\n0.469\n6.421\n78.9\n4.9671\n2\n242\n17.8\n9.14\n21.6\n\n\n2\n0.02729\n0.0\n7.07\n0\n0.469\n7.185\n61.1\n4.9671\n2\n242\n17.8\n4.03\n34.7\n\n\n3\n0.03237\n0.0\n2.18\n0\n0.458\n6.998\n45.8\n6.0622\n3\n222\n18.7\n2.94\n33.4\n\n\n4\n0.06905\n0.0\n2.18\n0\n0.458\n7.147\n54.2\n6.0622\n3\n222\n18.7\n5.33\n36.2\n\n\n\n\n\n\n\n\n# split dataset into training (60%) and validation (40%) sets\ntrain_df, valid_df = train_test_split(house_df, test_size=0.4, random_state=1)\nprint('Training set:', train_df.shape, 'Validation set:', valid_df.shape)\n\nTraining set: (303, 13) Validation set: (203, 13)\n\n\n\n# normalize training and validation sets. The transformation is trained using the training set only.\n# if you don't convert the integer columns to real numbers (float64), the StandardScaler will raise a DataConversionWarning. \n# This is expected\noutcome = 'MEDV'\npredictors = list(house_df.columns)\npredictors.remove(outcome)\n\nscaler = preprocessing.StandardScaler()\nscaler.fit(train_df[predictors])\n\n# Transform the predictors of training validation and newCustomer\ntrain_X = scaler.transform(train_df[predictors])\ntrain_y = train_df[outcome]\nvalid_X = scaler.transform(valid_df[predictors])\nvalid_y = valid_df[outcome]\n\n\n# Train a regressor for different values of k\nresults = []\nfor k in range(1, 6):\n    knn = KNeighborsRegressor(n_neighbors=k).fit(train_X, train_y)\n    results.append({\n        'k': k,\n        'RMSE': math.sqrt(mean_squared_error(valid_y, knn.predict(valid_X)))\n    })\n\n# Convert results to a pandas data frame\nresults = pd.DataFrame(results)\nresults\n\n\n\n\n\n\n\n\nk\nRMSE\n\n\n\n\n0\n1\n5.403228\n\n\n1\n2\n4.778562\n\n\n2\n3\n4.671801\n\n\n3\n4\n4.789219\n\n\n4\n5\n5.014823\n\n\n\n\n\n\n\nHere best k = 3. This means that, for a given record, MEDV is predicted by averaging the MEDVs for the 3 closest records, proximity being measured by the distance between the vectors of predictor values.\n7.3.b. Predict the MEDV for a tract with the following information, using the best k:\nCRIM = 0.2, ZN = 0, INDUS = 7, CHAS = 0, NOX = 0.538, RM = 6, AGE = 62, DIS = 4.7, RAD = 4, TAX = 307, PTRATIO = 21, LSTAT = 10.\nAnswer\n\n# new tract\nnewTract = pd.DataFrame([{'CRIM': 0.2, 'ZN': 0, 'INDUS': 7, 'CHAS': 0, 'NOX': 0.538, 'RM': 6, 'AGE': 62, 'DIS': 4.7, 'RAD': 4, \n                          'TAX': 307, 'PTRATIO': 21, 'LSTAT': 10}],\n                       columns=['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'LSTAT'])\nnewTract\n\n\n\n\n\n\n\n\nCRIM\nZN\nINDUS\nCHAS\nNOX\nRM\nAGE\nDIS\nRAD\nTAX\nPTRATIO\nLSTAT\n\n\n\n\n0\n0.2\n0\n7\n0\n0.538\n6\n62\n4.7\n4\n307\n21\n10\n\n\n\n\n\n\n\n\n# normalize new record\nnewTractNorm = pd.DataFrame(scaler.transform(newTract), \n                               columns=['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'LSTAT'])\n\nnewTractNorm\n\n\n\n\n\n\n\n\nCRIM\nZN\nINDUS\nCHAS\nNOX\nRM\nAGE\nDIS\nRAD\nTAX\nPTRATIO\nLSTAT\n\n\n\n\n0\n-0.403622\n-0.481603\n-0.620687\n-0.293294\n-0.153758\n-0.358814\n-0.243285\n0.400608\n-0.640284\n-0.604731\n1.197866\n-0.421956\n\n\n\n\n\n\n\n\n# train knn model with k=3\nknn = KNeighborsRegressor(n_neighbors=3)\nknn.fit(train_X, train_y)\n\nKNeighborsRegressor(algorithm='auto', leaf_size=30, metric='minkowski',\n                    metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n                    weights='uniform')\n\n\n\n# predict value of new tract\nknn.predict(newTractNorm)\n\narray([18.76666667])\n\n\nThe predicted price of new tract is $18.77k.\n7.3.c. If we used the above k-NN algorithm to score the training data, what would be the error of the training set?\nAnswer\nIn the training set, the error is zero because the training cases are matched to themselves.\n7.3.d. Why is the validation data error overly optimistic compared to the error rate when applying this k-NN predictor to new data?\nAnswer\nThe validation error measures the error for the “best k” among multiple k’s tried out for the validation data, so that particular k is optimized for the particular validation data set that was used in selecting it. It may not be as suitable for the new data.\n7.3.e. If the purpose is to predict MEDV for several thousands of new tracts, what would be the disadvantage of using k-NN prediction? List the operations that the algorithm goes through in order to produce each prediction.\nAnswer\nKNN does not yield a uniform rule that can be applied to each new record to be predicted – the whole “model building” process has to be repeated for each new record to be classified.\nSpecifically, the algorithm must calculate the distance from a new record to each of the training records, select the n-closest training records, determine the average target value for the n-closest training records, then score that target value to the new record, then repeat this process for each of the new records."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-14-ProbSolutions-AR.html",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-14-ProbSolutions-AR.html",
    "title": "Chapter 14: Association Rules and Collaborative Filtering",
    "section": "",
    "text": "2019-2020 Galit Shmueli, Peter C. Bruce, Peter Gedeck\n\nData Mining for Business Analytics: Concepts, Techniques, and Applications in Python (First Edition) Galit Shmueli, Peter C. Bruce, Peter Gedeck, and Nitin R. Patel. 2019.\nDate: 2020-03-08\nPython Version: 3.8.2 Jupyter Notebook Version: 5.6.1\nPackages: - mlxtend: 0.17.2 - numpy: 1.18.1 - pandas: 1.0.1 - scipy: 1.4.1 - scikit-learn: 0.22.2 - scikit-surprise: 1.1.0\nThe assistance from Mr. Kuber Deokar and Ms. Anuja Kulkarni in preparing these solutions is gratefully acknowledged.\n# Import required packages for this chapter\nfrom pathlib import Path\n\nimport pandas as pd\nimport numpy as np\nfrom scipy.spatial.distance import cosine\nfrom mlxtend.frequent_patterns import apriori\nfrom mlxtend.frequent_patterns import association_rules\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nfrom surprise import Dataset\nfrom surprise import Reader\nfrom surprise import KNNBasic\n\n%matplotlib inline\n# Working directory:\n#\n# We assume that data are kept in the same directory as the notebook. If you keep your \n# data in a different folder, replace the argument of the `Path`\nDATA = Path('.')\n# and then load data using \n#\n# pd.read_csv(DATA / ‘filename.csv’)"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-14-ProbSolutions-AR.html#solution-14.1",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-14-ProbSolutions-AR.html#solution-14.1",
    "title": "Chapter 14: Association Rules and Collaborative Filtering",
    "section": "Solution 14.1",
    "text": "Solution 14.1\nAssociation rules is not the correct approach. It determines associations among items listed in the columns (demographic and other descriptor variables in this database), not associations between rows (customers in this database). Cluster analysis would be more appropriate."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-14-ProbSolutions-AR.html#solution-14.2",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-14-ProbSolutions-AR.html#solution-14.2",
    "title": "Chapter 14: Association Rules and Collaborative Filtering",
    "section": "Solution 14.2",
    "text": "Solution 14.2\n\n# create frequent itemsets\nitemsets = apriori(df, min_support=0.01, use_colnames=True)\n\n# and convert into rules\nrules = association_rules(itemsets, metric='confidence', min_threshold=0.1)\nrules.sort_values(by=['lift'], ascending=False).head(6)\n\nprint(rules.sort_values(by=['lift'], ascending=False)\n      .drop(columns=['antecedent support', 'consequent support', 'conviction'])\n      .head(6))\n\n                antecedents             consequents   support  confidence  \\\n316            (Intro, DOE)        (SW, Regression)  0.019178    0.411765   \n321        (SW, Regression)            (Intro, DOE)  0.019178    0.350000   \n319       (Regression, DOE)             (Intro, SW)  0.019178    0.636364   \n318             (Intro, SW)       (Regression, DOE)  0.019178    0.200000   \n249     (Intro, DataMining)  (Forecast, Regression)  0.013699    0.250000   \n248  (Forecast, Regression)     (Intro, DataMining)  0.013699    0.357143   \n\n         lift  leverage  \n316  7.514706  0.016626  \n321  7.514706  0.016626  \n319  6.636364  0.016288  \n318  6.636364  0.016288  \n249  6.517857  0.011597  \n248  6.517857  0.011597  \n\n\n\n# filter rules to have only one consequent\nsingle_rules = rules[[len(c) == 1 for c in rules.consequents]]\n(single_rules.sort_values(by=['lift'], ascending=False)\n      .drop(columns=['antecedent support', 'consequent support', 'conviction'])\n      .head(6))\n\n\n\n\n\n\n\n\nantecedents\nconsequents\nsupport\nconfidence\nlift\nleverage\n\n\n\n\n243\n(Forecast, Intro, Regression)\n(DataMining)\n0.013699\n0.714286\n4.010989\n0.010283\n\n\n262\n(Intro, Survey, DOE)\n(Cat Data)\n0.010959\n0.800000\n3.842105\n0.008107\n\n\n233\n(Intro, DataMining, Cat Data)\n(Regression)\n0.016438\n0.750000\n3.601974\n0.011875\n\n\n255\n(Intro, Survey, Cat Data)\n(Forecast)\n0.013699\n0.500000\n3.578431\n0.009871\n\n\n245\n(Intro, DataMining, Regression)\n(Forecast)\n0.013699\n0.500000\n3.578431\n0.009871\n\n\n312\n(Intro, Regression, DOE)\n(SW)\n0.019178\n0.777778\n3.504801\n0.013706\n\n\n\n\n\n\n\nInterpreting some rules:\n\nThe first rule is “If Intro, Regression and Forecasting are taken, Data Mining is also taken.” It has confidence of 71.4%, and a lift of 4.\nThe second rule is “If Intro, Survey and DOE are taken, Categorical Data is also taken.” It has confidence of 80% and lift of 3.84.\n\nThe support (a U c) for all rules is very low. under 4%. This means that the applicability of these rules is not great, and also that the chances are greater that we are not picking up true associations that will persist into the future – just random noise."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-14-ProbSolutions-AR.html#solution-14.3",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-14-ProbSolutions-AR.html#solution-14.3",
    "title": "Chapter 14: Association Rules and Collaborative Filtering",
    "section": "Solution 14.3",
    "text": "Solution 14.3\n\ncourse_df = pd.read_csv(DATA / 'Coursetopics.csv')\ncourse_df.head()\n\n\n\n\n\n\n\n\nIntro\nDataMining\nSurvey\nCat Data\nRegression\nForecast\nDOE\nSW\n\n\n\n\n0\n1\n1\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n1\n0\n0\n0\n0\n0\n\n\n2\n0\n1\n0\n1\n1\n0\n0\n1\n\n\n3\n1\n0\n0\n0\n0\n0\n0\n0\n\n\n4\n1\n1\n0\n0\n0\n0\n0\n0\n\n\n\n\n\n\n\n\nratings = []\nfor customer, row in course_df.iterrows():\n    for course, value in row.iteritems():\n        if value==0: continue\n        ratings.append([customer, course, value])\nratings = pd.DataFrame(ratings, columns=['customer', 'course', 'rating'])\n\nreader = Reader(rating_scale=(1, 1))\ndata = Dataset.load_from_df(ratings, reader)\ntrainset = data.build_full_trainset()\nsim_options = {'name': 'cosine', 'user_based': False}  # compute cosine similarities between users\nalgo = KNNBasic(sim_options=sim_options)\nalgo.fit(trainset)\n\npredictions = []\nfor user in course_df.index:\n    predictions.append([algo.predict(user, course).est for course in course_df])\npredictions = pd.DataFrame(predictions, columns=course_df.columns)\npredictions.head()\n\nComputing the cosine similarity matrix...\nDone computing similarity matrix.\n\n\n\n\n\n\n\n\n\nIntro\nDataMining\nSurvey\nCat Data\nRegression\nForecast\nDOE\nSW\n\n\n\n\n0\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n2\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n3\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n4\n1\n1\n1\n1\n1\n1\n1\n1\n\n\n\n\n\n\n\nThe resulting predictions are all 1. This is because the input is not a rating matrix but a binary one."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-14-ProbSolutions-AR.html#solution-14.4.a",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-14-ProbSolutions-AR.html#solution-14.4.a",
    "title": "Chapter 14: Association Rules and Collaborative Filtering",
    "section": "Solution 14.4.a",
    "text": "Solution 14.4.a\nSelect several values in the matrix and explain their meaning.\nThe “0” in the first row, first column under “bag” indicates that, in the first transaction (i.e. the first row), no bag was purchased. The “1” to its right indicates that blush was purchased in that first transaction."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-14-ProbSolutions-AR.html#solution-14.4.b",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-14-ProbSolutions-AR.html#solution-14.4.b",
    "title": "Chapter 14: Association Rules and Collaborative Filtering",
    "section": "Solution 14.4.b",
    "text": "Solution 14.4.b\nConsider the results of the association rules analysis shown in the book.\n\nSolution 14.4.b.i\nFor the first row, explain the confidence output and how it is calculated.\nIf Blush, Concealer, Mascara, Eye.shadow, Lipstick were purchased, 30% of the time Eyebrow.Pencils were also purchased. The calculation is:\n\n100 * (# transactions with Blush + Concealer + Mascara + Eye.shadow + Lipstick) / (# transactions with Eyebrow.Pencils)\n\n\n\nSolution 14.4.b.ii\nFor the first row, explain the support output and how it is calculated.\nThe Support of 0.013 means there were 13 transactions in which &gt; Blush + Concealer + Mascara + Eye.shadow + Lipstick + Eyebrow.Pencils\nwere purchased.\n\n\nSolution 14.4.b.iii\nFor the first row, explain the lift and how it is calculated.\nLift Ratio = 7.19 means we are 7.19 times more likely to find a transaction with Eyebrow.Pencils IF we look only in those transactions where &gt; Blush + Concealer + Mascara + Eye.shadow + Lipstick\nare purchased, compared to searching randomly in all transactions. The calculation:\n\n((# trans with Eyebrow.Pencils + Blush + Concealer + Mascara + Eye.shadow + Lipstick)/ (# trans with Blush + Concealer + Mascara + Eye.shadow + Lipstick)) / ((# trans with Eyebrow.Pencils) / (all transactions))\n\n\n\nSolution 14.4.b.iv\nFor the first row, explain the rule that is represented there in words.\nThe rule is “If a transaction includes Blush + Concealer + Mascara + Eye.shadow + Lipstick, it will also include Eyebrow.Pencils.”\nIf we are searching for transactions with Eyebrow.Pencils, limiting our search to transactions with Blush + Concealer + Mascara + Eye.shadow + Lipstick will increase our probability of success by a factor of 7.19."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-14-ProbSolutions-AR.html#solution-14.4.c",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-14-ProbSolutions-AR.html#solution-14.4.c",
    "title": "Chapter 14: Association Rules and Collaborative Filtering",
    "section": "Solution 14.4.c",
    "text": "Solution 14.4.c\nNow, use the complete dataset on the cosmetics purchases (in the file Cosmetics.csv). Using Python, apply association rules to these data (for apriori use min_support=0.1 and use_colnames=True, for association_rules use default parameters).\n\nfrequentItemsets = apriori(cosmetics_df, min_support=0.1, use_colnames=True)\n\nrules = association_rules(frequentItemsets)\n\n(rules.sort_values(by=['lift'], ascending=False).head(20)\n     .drop(columns=['antecedent support', 'consequent support', 'conviction']))\n\n\n\n\n\n\n\n\nantecedents\nconsequents\nsupport\nconfidence\nlift\nleverage\n\n\n\n\n0\n(Brushes)\n(Nail Polish)\n0.149\n1.000000\n3.571429\n0.107280\n\n\n21\n(Concealer, Eye shadow, Blush)\n(Mascara)\n0.119\n0.959677\n2.688172\n0.074732\n\n\n4\n(Eye shadow, Blush)\n(Mascara)\n0.169\n0.928571\n2.601040\n0.104026\n\n\n6\n(Eye shadow, Nail Polish)\n(Mascara)\n0.119\n0.908397\n2.544529\n0.072233\n\n\n11\n(Eye shadow, Concealer)\n(Mascara)\n0.179\n0.890547\n2.494530\n0.107243\n\n\n13\n(Eye shadow, Bronzer)\n(Mascara)\n0.124\n0.879433\n2.463397\n0.073663\n\n\n23\n(Eye shadow, Concealer, Eyeliner)\n(Mascara)\n0.114\n0.876923\n2.456367\n0.067590\n\n\n5\n(Mascara, Blush)\n(Eye shadow)\n0.169\n0.918478\n2.410704\n0.098896\n\n\n17\n(Lipstick, Eye shadow)\n(Mascara)\n0.110\n0.852713\n2.388552\n0.063947\n\n\n18\n(Lipstick, Mascara)\n(Eye shadow)\n0.110\n0.909091\n2.386065\n0.063899\n\n\n22\n(Concealer, Mascara, Blush)\n(Eye shadow)\n0.119\n0.908397\n2.384244\n0.069089\n\n\n14\n(Bronzer, Mascara)\n(Eye shadow)\n0.124\n0.905109\n2.375615\n0.071803\n\n\n1\n(Eye shadow)\n(Mascara)\n0.321\n0.842520\n2.359999\n0.184983\n\n\n2\n(Mascara)\n(Eye shadow)\n0.321\n0.899160\n2.359999\n0.184983\n\n\n7\n(Nail Polish, Mascara)\n(Eye shadow)\n0.119\n0.888060\n2.330865\n0.067946\n\n\n19\n(Eye shadow, Eyeliner)\n(Mascara)\n0.151\n0.829670\n2.324007\n0.086026\n\n\n12\n(Concealer, Mascara)\n(Eye shadow)\n0.179\n0.877451\n2.303021\n0.101276\n\n\n16\n(Mascara, Lip Gloss)\n(Eye shadow)\n0.158\n0.872928\n2.291150\n0.089039\n\n\n15\n(Mascara, Foundation)\n(Eye shadow)\n0.166\n0.864583\n2.269248\n0.092848\n\n\n20\n(Mascara, Eyeliner)\n(Eye shadow)\n0.151\n0.862857\n2.264717\n0.084325\n\n\n\n\n\n\n\n\nSolution 14.4.c.i\nInterpret the first three rules in the output in words.\n\n(rules.sort_values(by=['lift'], ascending=False).head(3)\n     .drop(columns=['antecedent support', 'consequent support', 'conviction']))\n\n\n\n\n\n\n\n\nantecedents\nconsequents\nsupport\nconfidence\nlift\nleverage\n\n\n\n\n0\n(Brushes)\n(Nail Polish)\n0.149\n1.000000\n3.571429\n0.107280\n\n\n21\n(Concealer, Eye shadow, Blush)\n(Mascara)\n0.119\n0.959677\n2.688172\n0.074732\n\n\n4\n(Eye shadow, Blush)\n(Mascara)\n0.169\n0.928571\n2.601040\n0.104026\n\n\n\n\n\n\n\n\nFirst row: If brushes are purchased, nail polish is purchased. This rule has 100% confidence – purchasing a brush guarantees purchase of nail polish. It has lift of 3.6, and support of about 15% (149 transactions out of 1000) for the two items together.\nSecond row: if nail Blush, Concealer and Eye.shadow are purchased, Mascara is purchased. This rule has confidence of 96% – if Blush, Concealer and Eye.shadow are purchased, Mascara is 96% likely to be purchased as well. It has lift of 3.571, and support of about 12%.\nThird row: If Blush and Eye.shadow are purchased, Mascara is also purchased. This rule has confidence of 93%, lift of 2.6, and support of about 17%.\n\n\n\nSolution 14.4.c.ii\nReviewing the first couple of dozen rules, comment on their redundancy and how you would assess their utility.\nFirst, a note about utility. From a static retail presentation perspective (buy X together with Y), the shopper’s attention can probably only handle a couple of rules. Coupon and web offer generating systems have no such limit, because, while one or two offers are presented to a give customer at a given time, other customers, and this customer at a different time may receive different offers.\nMany rules come in pairs that are mirror images of one another, so we can tackle them that way.\nThe first rule is certain so no need to make an offer.\nAll remaining rules involve mascara, mostly as a consequent. Mascara is a good bet as a companion product in general – say for a retail display.\nRules 2-10 could be consolidated into a general offer covering the 5 products that keep reappearing in these “multi-item” rules: eyeliner, mascara, concealer, eyeshadow, and blush. These seem to be the favorites of big spenders, so a “buy 3, 50% off on two others” or something similar might work."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-14-ProbSolutions-AR.html#solution-14.5.a",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-14-ProbSolutions-AR.html#solution-14.5.a",
    "title": "Chapter 14: Association Rules and Collaborative Filtering",
    "section": "Solution 14.5.a",
    "text": "Solution 14.5.a\nFirst consider a user-based collaborative filter. This requires computing correlations between all student pairs. For which students is it possible to compute correlations with E.N.? Compute them.\nWe need to identify the users that share ratings with E.N. These are: L.N., M.H., J.H., D.U., and D.S. However, only L.N. and D.S. share more than one rating with E.N.\nTo compute this correlation, we first compute average rating by each of these students. Note that the average is computed over a different number of courses for each of these students, because they each rated a different set of courses.\nAverage ratings:\n\nLN: (4 + 3 + 2 + 4 + 2) / 5 = 3\nEN: (4 + 4 + 4 + 3) / 4 = 3.75\nDS: (4 + 2 + 4) / 3 = 3.33\n\nCo-rated courses for users EN and LN: SQL, R Prog, Regression.\n\nDenominator LN: sqrt((4-3)^2 + (4-3)^2 + (2-3)^2) = 1.732051\nDenominator EN: sqrt((4-3.75)^2 + (4-3.75)^2 + (3-3.75)^2) = 0.8291562\n\nCorr(LN, EN) = ((4-3)(4-3.75) + (4-3)(4-3.75) + (2-3)(3-3.75)) / (1.732051  0.8291562) = 0.8703882\nCo-rated courses for users EN and LN: SQL, DM in R, R Prog.\n\nDenominator EN: sqrt((4-3.75)^2 + (4-3.75)^2 + (4-3.75)^2) = 0.4330127\nDenominator DS: sqrt((4-3.33)^2 + (2-3.33)^2 + (4-3.33)^2) = 1.633003\n\nCorr(EN, DS) = ((4-3.75)(4-3.33) + (4-3.75)(2-3.33) + (4-3.75)(4-3.33)) / (0.4330127  1.633003) = 0.003535513"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-14-ProbSolutions-AR.html#solution-14.5.b",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-14-ProbSolutions-AR.html#solution-14.5.b",
    "title": "Chapter 14: Association Rules and Collaborative Filtering",
    "section": "Solution 14.5.b",
    "text": "Solution 14.5.b\nBased on the single nearest student to E.N., which single course should we recommend to E.N.? Explain why.\nFrom the correlations computed in (a) above, student LN is nearest to EN. Among the courses that LN has taken (but not taken by EN), Python is highly preferred by LN. So Python should be recommended to EN."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-14-ProbSolutions-AR.html#solution-14.5.c",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-14-ProbSolutions-AR.html#solution-14.5.c",
    "title": "Chapter 14: Association Rules and Collaborative Filtering",
    "section": "Solution 14.5.c",
    "text": "Solution 14.5.c\nUse scikit-learn function sklearn.metrics.pairwise.cosine_similarity to compute the cosine similarity between users.\nCo-rated courses for users EN and LN: SQL, R Prog, Regression.\n\nDenominator LN: sqrt(4^2 + 4^2 + 2^2) = 6\nDenominator EN: sqrt(4^2 + 4^2 + 3^2) = 6.403124\n\nCosine(LN, EN) = (44 + 44 + 23) / (6  6.403124) = 0.9891005\nCo-rated courses for users EN and LN: SQL, DM in R, R Prog.\n\nDenominator EN: sqrt(4^2 + 4^2 + 4^2) = 6.928203\nDenominator DS: sqrt(4^2 + 2^2 + 4^2) = 6\n\nCosine(EN, DS) = (44 + 42 + 44) / (6.928203  6) = 0.9622505\n\nprint('cosine(LN, EN) = ',cosine_similarity(rating_df.loc[['LN', 'EN'], ['SQL', 'R Prog', 'Regression']])[0, 1])\nprint('cosine(EN, DS) = ',cosine_similarity(rating_df.loc[['EN', 'DS'], ['SQL', 'DM in R', 'R Prog']])[0, 1])\n\ncosine(LN, EN) =  0.9891004919611718\ncosine(EN, DS) =  0.9622504486493764\n\n\nHere is an implementation of cosine similarity that handles NaN data.\n\ndef cosine_similarity_NA(data):\n    m = data.shape[0]\n    # Initialize the similarity matrix to np.nan\n    result = np.full((m, m), np.nan)\n    # Iterate over all pairs of columns\n    for i in range(m):\n        # maski is true if a value exists in column i\n        maski = ~np.isnan(data.iloc[i])\n        for j in range(i, m):\n            # maskij is true if a value exists in both column i and j\n            maskij = maski & ~np.isnan(data.iloc[j])\n            if np.any(maskij):\n                # if there are values in both columns, calculate the cosine similarity\n                # for these\n                result[i, j] = 1 - cosine(data.iloc[i][maskij], data.iloc[j][maskij])\n                result[j, i] = result[i, j]\n    return pd.DataFrame(result, columns=data.index, index=data.index)\n\ncosineSimilarity = cosine_similarity_NA(rating_df)\ncosineSimilarity\n\n\n\n\n\n\n\nUnnamed: 0\nLN\nMH\nJH\nEN\nDU\nFL\nGL\nAH\nSA\nRW\nBA\nMG\nAF\nKG\nDS\n\n\nUnnamed: 0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLN\n1.0000\n0.960000\n1.000000\n0.98910\n1.000000\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1.0\nNaN\nNaN\n1.00000\n\n\nMH\n0.9600\n1.000000\n0.989949\n1.00000\n0.989949\n1.0\n1.0\n1.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1.00000\n\n\nJH\n1.0000\n0.989949\n1.000000\n1.00000\n1.000000\n1.0\n1.0\n1.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1.00000\n\n\nEN\n0.9891\n1.000000\n1.000000\n1.00000\n1.000000\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n0.96225\n\n\nDU\n1.0000\n0.989949\n1.000000\n1.00000\n1.000000\n1.0\n1.0\n1.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1.00000\n\n\nFL\nNaN\n1.000000\n1.000000\nNaN\n1.000000\n1.0\n1.0\n1.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nGL\nNaN\n1.000000\n1.000000\nNaN\n1.000000\n1.0\n1.0\n1.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nAH\nNaN\n1.000000\n1.000000\nNaN\n1.000000\n1.0\n1.0\n1.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nSA\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\nNaN\n\n\nRW\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\nNaN\n\n\nBA\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\nNaN\n\n\nMG\n1.0000\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\nNaN\n\n\nAF\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\nNaN\n\n\nKG\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\nNaN\n\n\nDS\n1.0000\n1.000000\n1.000000\n0.96225\n1.000000\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n1.00000\n\n\n\n\n\n\n\nWe can also calculate the cosine similarity after converting the rating matrix into binary form (course taken or not). In this case, we get:\n\nbinary_df = rating_df.copy()\nbinary_df[~np.isnan(binary_df)] = 1\nbinary_df[np.isnan(binary_df)] = 0\nprint('cosine(LN, EN) = ', cosine_similarity(binary_df)[0, 3])\nprint('cosine(EN, DS) = ', cosine_similarity(binary_df)[3, 14])\n\ncosine(LN, EN) =  0.6708203932499369\ncosine(EN, DS) =  0.8660254037844388"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-14-ProbSolutions-AR.html#solution-14.5.d",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-14-ProbSolutions-AR.html#solution-14.5.d",
    "title": "Chapter 14: Association Rules and Collaborative Filtering",
    "section": "Solution 14.5.d",
    "text": "Solution 14.5.d\nBased on the cosine similarities of the nearest students to E.N., which course should be recommended to E.N.?\nFrom the cosine similarities based on course ratings, student LN is nearest to EN. Among the courses that LN has taken (but not taken by EN), Python is highly preferred by LN. So Python should be recommended to EN.\nIf we use the binary matrix, student DS is more similar to EN based on courses taken. However, as DS hasn’t taken any courses other than the ones EN already took, we cannot make a recommendation in this case."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-14-ProbSolutions-AR.html#solution-14.5.e",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-14-ProbSolutions-AR.html#solution-14.5.e",
    "title": "Chapter 14: Association Rules and Collaborative Filtering",
    "section": "Solution 14.5.e",
    "text": "Solution 14.5.e\nWhat is the conceptual difference between using the correlation as opposed to cosine similarities? [Hint: how are the missing values in the matrix handled in each case?]\nIf we consider the rating matrix, both methods basically only consider co-rated items. Correlation uses the not co-rated items to calculate the averages which will impact the correlation.\nIf we calculate the cosine-similarity after converting to a binary form, we use all items in the similarity calculation. Using the actual ratings only on co-rated items does not take into consideration items that are not co-rated, which may be useful information.\nUsing the binary form, is more useful if not all items are rated by most users. On the other hand, if most items are rated by most users, using the actual ratings will add power to the analysis, compared to just using binary data."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-14-ProbSolutions-AR.html#solution-14.5.f",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-14-ProbSolutions-AR.html#solution-14.5.f",
    "title": "Chapter 14: Association Rules and Collaborative Filtering",
    "section": "Solution 14.5.f",
    "text": "Solution 14.5.f\nWith large datasets, it is computationally difficult to compute user-based recommendations in real time, and an item-based approach is used instead. Returning to the rating data (not the binary matrix), let’s now take that approach.\n\nSolution 14.5.f.i\nIf the goal is still to find a recommendation for E.N., for which course pairs is it possible and useful to calculate correlations?\nThere is enough data to find correlations for the following pairs:\n- SQL - Spatial\n- SQL - DM in R\n- SQL - Python\n- DM in R - R Prog\n- Spatial - Python\nHowever, EN has already taken SQL, DM in R, and R Prog. Hence, only the Spatial and Python correlations are useful.\n\n\nSolution 14.5.f.ii\nJust looking at the data, and without yet calculating course pair correlations, which course would you recommend to E.N., relying on item-based filtering? Calculate two course pair correlations involving your guess and report the results.\nThe SQL - Spatial ratings match the best, and there are more co-rated items, so Spatial would be the best guess.\n\nprint(cosine_similarity(rating_df.loc[['MH', 'JH', 'DU'], ['SQL', 'Spatial']].transpose()))\ncosine_similarity_NA(rating_df.transpose())\n\n[[1.         0.99037514]\n [0.99037514 1.        ]]\n\n\n\n\n\n\n\n\n\nSQL\nSpatial\nPA1\nDM in R\nPython\nForecast\nR Prog\nHadoop\nRegression\n\n\n\n\nSQL\n1.000000\n0.990375\nNaN\n0.948683\n0.96\n1.0\n1.000000\nNaN\n0.980581\n\n\nSpatial\n0.990375\n1.000000\nNaN\nNaN\n1.00\nNaN\nNaN\nNaN\nNaN\n\n\nPA1\nNaN\nNaN\n1.0\nNaN\nNaN\n1.0\nNaN\n1.0\nNaN\n\n\nDM in R\n0.948683\nNaN\nNaN\n1.000000\nNaN\nNaN\n0.948683\nNaN\n1.000000\n\n\nPython\n0.960000\n1.000000\nNaN\nNaN\n1.00\n1.0\n1.000000\nNaN\n1.000000\n\n\nForecast\n1.000000\nNaN\n1.0\nNaN\n1.00\n1.0\n1.000000\nNaN\n1.000000\n\n\nR Prog\n1.000000\nNaN\nNaN\n0.948683\n1.00\n1.0\n1.000000\nNaN\n0.980581\n\n\nHadoop\nNaN\nNaN\n1.0\nNaN\nNaN\nNaN\nNaN\n1.0\nNaN\n\n\nRegression\n0.980581\nNaN\nNaN\n1.000000\n1.00\n1.0\n0.980581\nNaN\n1.000000"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-14-ProbSolutions-AR.html#solution-14.5.g",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-14-ProbSolutions-AR.html#solution-14.5.g",
    "title": "Chapter 14: Association Rules and Collaborative Filtering",
    "section": "Solution 14.5.g",
    "text": "Solution 14.5.g\nApply item-based collaborative filtering to this dataset (using Python) and based on the results, recommend a course to E.N.\n\n# convert the rating_df dataframe into a format suitable for the Surprise package\nratings = []\nfor customer, row in rating_df.iterrows():\n    for course, value in row.iteritems():\n        if np.isnan(value): continue\n        ratings.append([customer, course, value])\nratings = pd.DataFrame(ratings, columns=['customer', 'course', 'rating'])\n\nreader = Reader(rating_scale=(1, 4))\ndata = Dataset.load_from_df(ratings, reader)\ntrainset = data.build_full_trainset()\n# compute cosine similarities between items\nsim_options = {'name': 'cosine', 'user_based': False}  \nalgo = KNNBasic(sim_options=sim_options)\nalgo.fit(trainset)\n\ncourses = rating_df.columns\nfor course in courses: \n    print(course, algo.predict('EN', course).est)\n\nComputing the cosine similarity matrix...\nDone computing similarity matrix.\nSQL 3.7504416393899813\nSpatial 4\nPA1 3.433333333333333\nDM in R 3.743416490252569\nPython 3.6621621621621623\nForecast 3.6666666666666665\nR Prog 3.7504416393899813\nHadoop 3.433333333333333\nRegression 3.747548783981962\n\n\nThe item-based collaborative filtering recommends the Spatial course to E.N."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-06-ProbSolutions-MLR.html",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-06-ProbSolutions-MLR.html",
    "title": "Chapter 6: Multiple Linear Regression - Solution",
    "section": "",
    "text": "2019-2020 Galit Shmueli, Peter C. Bruce, Peter Gedeck\n\nData Mining for Business Analytics: Concepts, Techniques, and Applications in Python (First Edition) Galit Shmueli, Peter C. Bruce, Peter Gedeck, and Nitin R. Patel. 2019.\nDate: 2020-03-08\nPython Version: 3.8.2 Jupyter Notebook Version: 5.6.1\nPackages: - dmba: 0.0.12 - matplotlib: 3.2.0 - numpy: 1.18.1 - pandas: 1.0.1 - seaborn: 0.10.0 - scikit-learn: 0.22.2 - statsmodels: 0.11.1\nThe assistance from Mr. Kuber Deokar and Ms. Anuja Kulkarni in preparing these solutions is gratefully acknowledged.\n\n\n# import required functionality for this chapter\nfrom pathlib import Path\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport statsmodels.formula.api as sm\n\nimport matplotlib.pylab as plt\nimport seaborn as sns\n\nfrom dmba import regressionSummary, exhaustive_search\nfrom dmba import backward_elimination, forward_selection, stepwise_selection\nfrom dmba import adjusted_r2_score, AIC_score #, BIC_score\n\n%matplotlib inline\n\n\n# Working directory:\n#\n# We assume that data are kept in the same directory as the notebook. If you keep your \n# data in a different folder, replace the argument of the `Path`\nDATA = Path('.')\n# and then load data using \n#\n# pd.read_csv(DATA / ‘filename.csv’)\n\n\nProblem 6.1 Predicting Bostom Housing Prices\nThe file BostonHousing.csv contains information collected by the US Bureau of the Census concerning housing in the area of Boston, Massachusetts. The dataset includes information on 506 census housing tracts in the Boston area. The goal is to predict the median house price in new tracts based on information such as crime rate, pollution, and number of rooms. The dataset contains 13 predictors, and the outcome variable is the median house price (MEDV). Table 6.11 describes each of the predictors and the outcome variable.\n\n\n\nTABLE_6.11\n\n\n\n# load the data\nhousing_df = pd.read_csv(DATA / 'BostonHousing.csv')\n# display column/variable names\ncolumns = list(housing_df.columns)\nprint(\"Variables in the data are: \")\nprint(columns)\n# review first 5 records in the data\nprint(\"\\nFirst 5 records in the data are:\")\nhousing_df.head()\n\nVariables in the data are: \n['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'LSTAT', 'MEDV', 'CAT. MEDV']\n\nFirst 5 records in the data are:\n\n\n\n\n\n\n\n\n\nCRIM\nZN\nINDUS\nCHAS\nNOX\nRM\nAGE\nDIS\nRAD\nTAX\nPTRATIO\nLSTAT\nMEDV\nCAT. MEDV\n\n\n\n\n0\n0.00632\n18.0\n2.31\n0\n0.538\n6.575\n65.2\n4.0900\n1\n296\n15.3\n4.98\n24.0\n0\n\n\n1\n0.02731\n0.0\n7.07\n0\n0.469\n6.421\n78.9\n4.9671\n2\n242\n17.8\n9.14\n21.6\n0\n\n\n2\n0.02729\n0.0\n7.07\n0\n0.469\n7.185\n61.1\n4.9671\n2\n242\n17.8\n4.03\n34.7\n1\n\n\n3\n0.03237\n0.0\n2.18\n0\n0.458\n6.998\n45.8\n6.0622\n3\n222\n18.7\n2.94\n33.4\n1\n\n\n4\n0.06905\n0.0\n2.18\n0\n0.458\n7.147\n54.2\n6.0622\n3\n222\n18.7\n5.33\n36.2\n1\n\n\n\n\n\n\n\n6.1.a. Why should the data be partitioned into training and validation sets? What will the training set be used for? What will the validation set be used for?\nAnswer: 1. The data should be partitioned into training and validation sets because we need two sets of data: one to build the model that depicts the relationship between the predictor variables and the predicted variable, and another to validate the model’s predictive accuracy. 2. The training data set is used to build the model. The algorithm ‘discovers’ the model using this data set. 3. The validation data is used to ‘validate’ the model. In this process, the model (built using the training data set) is used to make predictions with the validation data - data that were not used to fit the model. In this way we get an unbiased estimate of how well the model performs. We compute measures of ‘error’, which reflect the prediction accuracy.\n6.1.b. Fit a multiple linear regression model to the median house price (MEDV) as a function of CRIM, CHAS, and RM. Write the equation for predicting the median house price from the predictors in the model.\nAnswer:\n\n# select columns for regression analysis\nhousing_df.columns\noutcome = 'MEDV'\npredictors = ['CRIM', 'CHAS', 'RM']\n\nx = housing_df[predictors]\ny = housing_df[outcome]\n\n\n# fit the regression model y on x\nhousing_lm = LinearRegression()\nhousing_lm.fit(x,y)\n\n# regression model, print coefficients\nprint('intercept ', housing_lm.intercept_)\nprint(pd.DataFrame({'Predictor': x.columns, 'coefficient': housing_lm.coef_}))\n\nintercept  -28.810682506359125\n  Predictor  coefficient\n0      CRIM    -0.260724\n1      CHAS     3.763037\n2        RM     8.278180\n\n\n6.1.c. Using the estimated regression model, what median house price is predicted for a tract in the Boston area that does not bound the Charles River, has a crime rate of 0.1, and where the average number of rooms per house is 6?\nAnswer:\n\nnew_df = pd.DataFrame(\n    [[0.1, 0, 6]],\n    columns=['CRIM', 'CHAS', 'RM'])\nnew_df\n\n\n\n\n\n\n\n\nCRIM\nCHAS\nRM\n\n\n\n\n0\n0.1\n0\n6\n\n\n\n\n\n\n\n\nhousing_lm_pred = housing_lm.predict(new_df)\nprint('Predicted value for median house price based on the model built using  dataset is:', housing_lm_pred)\n\nPredicted value for median house price based on the model built using  dataset is: [20.83232392]\n\n\nThe predicted value of median house price based on the model built using training dataset is $20832.32.\n6.1.d.i. Reduce the number of predictors: Which predictors are likely to be measuring the same thing among the 13 predictors? Discuss the relationships among INDUS, NOX, and TAX.\nAnswer: There are several variables that measure levels of industrialization, which are expected to be positively correlated. These include INDUS, NOX (pollution), and TAX.\nWe expect a positive relationship between NOX (nitric oxides concentration, a pollutant), INDUS (proportion of non-retail business acres per town) and TAX (tax rate), because areas that have a high proportion of non-retail businesses tend to have higher taxes and more pollution.\n6.1.d.ii. Compute the correlation table for the 12 numerical predictors and search for highly correlated pairs. These have potential redundancy and can cause multicollinearity. Choose which ones to remove based on this table.\nAnswer:\n\n# variables in the data\nhousing_df.columns\n\nIndex(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX',\n       'PTRATIO', 'LSTAT', 'MEDV', 'CAT. MEDV'],\n      dtype='object')\n\n\n\n# Create a new dataframe with predictors\npredictors_df = housing_df\n\ncolumns = list(housing_df.columns)\ncolumns.remove('CHAS')\ncolumns.remove('MEDV')\ncolumns.remove('CAT. MEDV')\npredictors_df = predictors_df[columns]\npredictors_df.columns\n\nIndex(['CRIM', 'ZN', 'INDUS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX',\n       'PTRATIO', 'LSTAT'],\n      dtype='object')\n\n\n\ncorr = predictors_df.corr()\n# corr.style.background_gradient()\n_ = sns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, vmin=-1, vmax=1, \n            cmap=\"RdBu\")\n\n\n\n\n\n\n\n\n\n# correlation table\ncorr\n\n\n\n\n\n\n\n\nCRIM\nZN\nINDUS\nNOX\nRM\nAGE\nDIS\nRAD\nTAX\nPTRATIO\nLSTAT\n\n\n\n\nCRIM\n1.000000\n-0.200469\n0.406583\n0.420972\n-0.219247\n0.352734\n-0.379670\n0.625505\n0.582764\n0.289946\n0.455621\n\n\nZN\n-0.200469\n1.000000\n-0.533828\n-0.516604\n0.311991\n-0.569537\n0.664408\n-0.311948\n-0.314563\n-0.391679\n-0.412995\n\n\nINDUS\n0.406583\n-0.533828\n1.000000\n0.763651\n-0.391676\n0.644779\n-0.708027\n0.595129\n0.720760\n0.383248\n0.603800\n\n\nNOX\n0.420972\n-0.516604\n0.763651\n1.000000\n-0.302188\n0.731470\n-0.769230\n0.611441\n0.668023\n0.188933\n0.590879\n\n\nRM\n-0.219247\n0.311991\n-0.391676\n-0.302188\n1.000000\n-0.240265\n0.205246\n-0.209847\n-0.292048\n-0.355501\n-0.613808\n\n\nAGE\n0.352734\n-0.569537\n0.644779\n0.731470\n-0.240265\n1.000000\n-0.747881\n0.456022\n0.506456\n0.261515\n0.602339\n\n\nDIS\n-0.379670\n0.664408\n-0.708027\n-0.769230\n0.205246\n-0.747881\n1.000000\n-0.494588\n-0.534432\n-0.232471\n-0.496996\n\n\nRAD\n0.625505\n-0.311948\n0.595129\n0.611441\n-0.209847\n0.456022\n-0.494588\n1.000000\n0.910228\n0.464741\n0.488676\n\n\nTAX\n0.582764\n-0.314563\n0.720760\n0.668023\n-0.292048\n0.506456\n-0.534432\n0.910228\n1.000000\n0.460853\n0.543993\n\n\nPTRATIO\n0.289946\n-0.391679\n0.383248\n0.188933\n-0.355501\n0.261515\n-0.232471\n0.464741\n0.460853\n1.000000\n0.374044\n\n\nLSTAT\n0.455621\n-0.412995\n0.603800\n0.590879\n-0.613808\n0.602339\n-0.496996\n0.488676\n0.543993\n0.374044\n1.000000\n\n\n\n\n\n\n\nIf the correlation between variables if greater than 0.7 we can say that the two variables are highly correlated. From the above table, the pairs of highly correlated variables are:\n\nNOX and INDUS: Correlation coefficient = 0.763651\nTAX and INDUS: Correlation coefficient = 0.72076\nAGE and NOX: Correlation coefficient = 0.73147\nDIS and NOX: Correlation coefficient = -0.76923\nDIS and AGE: Correlation coefficient = -0.747881\nTAX and RAD: Correlation coefficient = 0.910228\n\nAccording to the correlation table, we might be able to remove some variables that do not add much information to others that we keep. We might remove INDUS, AGE and TAX.\n6.1.d.iii. Use stepwise regression with the three options (backward, forward, both) to reduce the remaining predictors as follows: Run stepwise on the training set. Choose the top model from each stepwise run. Then use each of these models separately to predict the validation set. Compare RMSE, MAPE, and mean error, as well as lift charts. Finally, describe the best model.\nAnswer:\n\n# partition the data into training (60%) and validation (40%) sets\npredictors = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'LSTAT']\noutcome = 'MEDV'\n\n# partition the data\nX = pd.get_dummies(housing_df[predictors], drop_first=True)\ny = housing_df[outcome]\ntrain_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size=0.4, random_state=1)\n\nprint('Training set:', train_X.shape, 'Validation set:', valid_X.shape)\n\nTraining set: (303, 12) Validation set: (203, 12)\n\n\nNOTE: There is currently no support in scikit-learn or statsmodels for stepwise regression. It is however straightforward to implement such an approach in a few lines of code using the dmba implementation.\n\n# backward elimination\n\ndef train_model(variables):\n    model = LinearRegression()\n    model.fit(train_X[variables], train_y)\n    return model\n\ndef score_model(model, variables):\n    return AIC_score(train_y, model.predict(train_X[variables]), model)\n\nbest_model, best_variables = backward_elimination(train_X.columns, train_model, score_model, verbose=True)\n\nprint(\"Best Subset:\", best_variables)\n\nVariables: CRIM, ZN, INDUS, CHAS, NOX, RM, AGE, DIS, RAD, TAX, PTRATIO, LSTAT\nStart: score=1807.23\nStep: score=1805.30, remove AGE\nStep: score=1803.57, remove INDUS\nStep: score=1803.57, remove None\nBest Subset: ['CRIM', 'ZN', 'CHAS', 'NOX', 'RM', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'LSTAT']\n\n\n\n# forward selection\n# The initial model is the constant model - this requires special handling in train_model and score_model\n\ndef train_model(variables):\n    if len(variables) == 0:\n        return None\n    model = LinearRegression()\n    model.fit(train_X[variables], train_y)\n    return model\n\ndef score_model(model, variables):\n    if len(variables) == 0:\n        return AIC_score(train_y, [train_y.mean()] * len(train_y), model, df=1)\n    return AIC_score(train_y, model.predict(train_X[variables]), model)\n\nbest_model, best_variables = forward_selection(train_X.columns, train_model, score_model, verbose=True)\n\nprint(\"Best Subset:\", best_variables)\n\nVariables: CRIM, ZN, INDUS, CHAS, NOX, RM, AGE, DIS, RAD, TAX, PTRATIO, LSTAT\nStart: score=2191.75, constant\nStep: score=1934.91, add LSTAT\nStep: score=1874.18, add RM\nStep: score=1842.54, add PTRATIO\nStep: score=1837.69, add CHAS\nStep: score=1835.00, add NOX\nStep: score=1817.90, add DIS\nStep: score=1811.82, add ZN\nStep: score=1810.16, add CRIM\nStep: score=1808.01, add RAD\nStep: score=1803.57, add TAX\nStep: score=1803.57, add None\nBest Subset: ['LSTAT', 'RM', 'PTRATIO', 'CHAS', 'NOX', 'DIS', 'ZN', 'CRIM', 'RAD', 'TAX']\n\n\n\n# stepwise (both) method\nbest_model, best_variables = stepwise_selection(train_X.columns, train_model, score_model, verbose=True)\nprint(\"Best Subset:\", best_variables)\n\nVariables: CRIM, ZN, INDUS, CHAS, NOX, RM, AGE, DIS, RAD, TAX, PTRATIO, LSTAT\nStart: score=2191.75, constant\nStep: score=1934.91, add LSTAT\nStep: score=1874.18, add RM\nStep: score=1842.54, add PTRATIO\nStep: score=1837.69, add CHAS\nStep: score=1835.00, add NOX\nStep: score=1817.90, add DIS\nStep: score=1811.82, add ZN\nStep: score=1810.16, add CRIM\nStep: score=1808.01, add RAD\nStep: score=1803.57, add TAX\nStep: score=1803.57, unchanged None\nBest Subset: ['LSTAT', 'RM', 'PTRATIO', 'CHAS', 'NOX', 'DIS', 'ZN', 'CRIM', 'RAD', 'TAX']\n\n\nAbove we see that all three subset selection methods gave the same best subset/model: Each model is designed using the following 10 variables: LSTAT, RM, PTRATIO, CHAS, NOX, DIS, ZN, CRIM, RAD, TAX.\n\n# fit the model with best subset variables\noutcome = 'MEDV'\npredictors = ['LSTAT', 'RM', 'PTRATIO', 'CHAS', 'NOX', 'DIS', 'ZN', 'CRIM', 'RAD', 'TAX']\n\nX = train_X[predictors]\ny = train_y\n\n# fit the regression model y on X\nhouse_lm = LinearRegression()\nhouse_lm.fit(X,y)\n\n# regression model, print coefficients\nprint('intercept', house_lm.intercept_)\nprint(pd.DataFrame({'Predictor': X.columns, 'coefficient': house_lm.coef_}))\n\n# print performance measures (training set)\nprint(\"\\nModel performance on training data:\")\nregressionSummary(train_y, house_lm.predict(train_X[predictors]))\n\n# predict prices in validation set, print first few predicted/actual values and residuals\nhouse_lm_pred = house_lm.predict(valid_X[predictors])\nresult = pd.DataFrame({'Predicted': house_lm_pred, 'Actual': valid_y, 'Residual': valid_y - house_lm_pred})\n\n# print performance measures (validation set)\nprint(\"\\nModel performance on validation data:\")\nregressionSummary(valid_y, house_lm_pred)\n\nintercept 38.95615649828216\n  Predictor  coefficient\n0     LSTAT    -0.514444\n1        RM     3.480964\n2   PTRATIO    -0.804964\n3      CHAS     2.359986\n4       NOX   -17.866926\n5       DIS    -1.438596\n6        ZN     0.066221\n7      CRIM    -0.114137\n8       RAD     0.262455\n9       TAX    -0.011166\n\nModel performance on training data:\n\nRegression statistics\n\n                      Mean Error (ME) : -0.0000\n       Root Mean Squared Error (RMSE) : 4.5615\n            Mean Absolute Error (MAE) : 3.1662\n          Mean Percentage Error (MPE) : -3.4181\nMean Absolute Percentage Error (MAPE) : 16.4898\n\nModel performance on validation data:\n\nRegression statistics\n\n                      Mean Error (ME) : -0.0393\n       Root Mean Squared Error (RMSE) : 5.0771\n            Mean Absolute Error (MAE) : 3.5746\n          Mean Percentage Error (MPE) : -5.1561\nMean Absolute Percentage Error (MAPE) : 16.9733\n\n\n\n\nProblem 6.2 Predicting Software Reselling Profits.\nTayko Software is a software catalog firm that sells games and educational software. It started out as a software manufacturer and then added third-party titles to its oﬀerings. It recently revised its collection of items in a new catalog, which it mailed out to its customers. This mailing yielded 2000 purchases. Based on these data, Tayko wants to devise a model for predicting the spending amount that a purchasing customer will yield. The file Tayko.csv contains information on 2000 purchases. Table 6.12 describes the variables to be used in the problem (the Excel file contains additional variables).\n\n\n\nTABLE 6.12\n\n\n\n# load the data\ntayko_df = pd.read_csv(DATA / 'Tayko.csv')\n# check data dimension\ntayko_df.shape\n\n(2000, 25)\n\n\n\n# check variable names\nlist(tayko_df.columns)\n\n['sequence_number',\n 'US',\n 'source_a',\n 'source_c',\n 'source_b',\n 'source_d',\n 'source_e',\n 'source_m',\n 'source_o',\n 'source_h',\n 'source_r',\n 'source_s',\n 'source_t',\n 'source_u',\n 'source_p',\n 'source_x',\n 'source_w',\n 'Freq',\n 'last_update_days_ago',\n '1st_update_days_ago',\n 'Web order',\n 'Gender=male',\n 'Address_is_res',\n 'Purchase',\n 'Spending']\n\n\n\n# modify column names\ntayko_df.columns = [c.replace(' ', '_').replace('=', '_') for c in tayko_df.columns]\n\n\n# review first few record\ntayko_df.head()\n\n\n\n\n\n\n\n\nsequence_number\nUS\nsource_a\nsource_c\nsource_b\nsource_d\nsource_e\nsource_m\nsource_o\nsource_h\n...\nsource_x\nsource_w\nFreq\nlast_update_days_ago\n1st_update_days_ago\nWeb_order\nGender_male\nAddress_is_res\nPurchase\nSpending\n\n\n\n\n0\n1\n1\n0\n0\n1\n0\n0\n0\n0\n0\n...\n0\n0\n2\n3662\n3662\n1\n0\n1\n1\n128\n\n\n1\n2\n1\n0\n0\n0\n0\n1\n0\n0\n0\n...\n0\n0\n0\n2900\n2900\n1\n1\n0\n0\n0\n\n\n2\n3\n1\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n2\n3883\n3914\n0\n0\n0\n1\n127\n\n\n3\n4\n1\n0\n1\n0\n0\n0\n0\n0\n0\n...\n0\n0\n1\n829\n829\n0\n1\n0\n0\n0\n\n\n4\n5\n1\n0\n1\n0\n0\n0\n0\n0\n0\n...\n0\n0\n1\n869\n869\n0\n0\n0\n0\n0\n\n\n\n\n5 rows × 25 columns\n\n\n\n\n# check data types of variables in the data\ntayko_df.dtypes\n\nsequence_number         int64\nUS                      int64\nsource_a                int64\nsource_c                int64\nsource_b                int64\nsource_d                int64\nsource_e                int64\nsource_m                int64\nsource_o                int64\nsource_h                int64\nsource_r                int64\nsource_s                int64\nsource_t                int64\nsource_u                int64\nsource_p                int64\nsource_x                int64\nsource_w                int64\nFreq                    int64\nlast_update_days_ago    int64\n1st_update_days_ago     int64\nWeb_order               int64\nGender_male             int64\nAddress_is_res          int64\nPurchase                int64\nSpending                int64\ndtype: object\n\n\n6.2.a Explore the spending amount by creating a pivot table for the categorical variables and computing the average and standard deviation of spending in each category.\nAnswer:\n\n# select only required variables\nselected_var = ['US', 'Freq', 'last_update_days_ago', 'Web_order', 'Gender_male', 'Address_is_res', 'Spending']\ntayko_df = tayko_df[selected_var]\nlist(selected_var)\n\n['US',\n 'Freq',\n 'last_update_days_ago',\n 'Web_order',\n 'Gender_male',\n 'Address_is_res',\n 'Spending']\n\n\n\n# pivot table: spending by gender\npd.pivot_table(tayko_df, index= 'Gender_male', values= \"Spending\",\n               aggfunc= [np.mean, np.std])\n\n\n\n\n\n\n\n\nmean\nstd\n\n\n\nSpending\nSpending\n\n\nGender_male\n\n\n\n\n\n\n0\n107.339642\n190.83233\n\n\n1\n98.350810\n183.02006\n\n\n\n\n\n\n\n\n# pivot table: spending by web order\npd.pivot_table(tayko_df, index= 'Web_order', values= \"Spending\",\n               aggfunc= [np.mean, np.std])\n\n\n\n\n\n\n\n\nmean\nstd\n\n\n\nSpending\nSpending\n\n\nWeb_order\n\n\n\n\n\n\n0\n82.902439\n173.417088\n\n\n1\n129.199531\n200.463840\n\n\n\n\n\n\n\n\n# pivot table: spending by adress\npd.pivot_table(tayko_df, index= 'Address_is_res', values= \"Spending\",\n               aggfunc= [np.mean, np.std])\n\n\n\n\n\n\n\n\nmean\nstd\n\n\n\nSpending\nSpending\n\n\nAddress_is_res\n\n\n\n\n\n\n0\n105.306162\n199.521159\n\n\n1\n93.174208\n132.204281\n\n\n\n\n\n\n\n\n# pivot table: spending by whether it is US address\npd.pivot_table(tayko_df, index= 'US', values= \"Spending\",\n               aggfunc= [np.mean, np.std])\n\n\n\n\n\n\n\n\nmean\nstd\n\n\n\nSpending\nSpending\n\n\nUS\n\n\n\n\n\n\n0\n101.216524\n174.844401\n\n\n1\n102.924803\n189.275664\n\n\n\n\n\n\n\n6.2.b. Explore the relationship between spending and each of the two continuous predictors by creating two scatterplots (Spending vs. Freq, and Spending vs. last_update_days_ago. Does there seem to be a linear relationship?\nAnswer:\n\n# plot of spending against frequency\nplt.scatter(tayko_df.Freq, tayko_df.Spending)\nplt.title('Scatter plot showing Spending against Frequency')\nplt.ylabel('Spending')\nplt.xlabel('Frequency')\nplt.show()\n\n\n\n\n\n\n\n\nThe overall relationship between Spending and Freq does not appear to be linear. However, there do appear to be two groups of points (Spending &lt; 800 and Spending &gt; 800), and within each group there appears to be a linear relationship between SPENDING and FREQ.\n\n# plot Spending against last_update_days_ago\nplt.scatter(tayko_df.last_update_days_ago, tayko_df.Spending)\nplt.title('Scatter plot showing Spending against last_update_days_ago')\nplt.ylabel('Spending')\nplt.xlabel('last_update_days_ago')\nplt.show()\n\n\n\n\n\n\n\n\nThere appears to be no relationship between Spending and Last_Update but two grous of points similar to what we seen in case of Spending and Frequency above.\n6.2.c. To fit a predictive model for spending:\n6.2.c.i. Partition the 2000 records into training and validation sets.\nAnswer:\n\n# predictors and outcome\npredictors = ['US', 'Freq', 'last_update_days_ago', 'Web_order', 'Gender_male', 'Address_is_res']\noutcome = 'Spending'\nX = tayko_df[predictors]\ny = tayko_df[outcome]\n\n# partition data\ntrain_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size = 0.4, random_state = 1)\nprint('Training set:', train_X.shape, 'Validation set:', valid_X.shape)\n\nTraining set: (1200, 6) Validation set: (800, 6)\n\n\n6.2.c.ii. Run a multiple linear regression model for Spending vs. all six predictors. Give the estimated predictive equation.\nAnswer:\n\n# fit the regression model\ntayko_lm = LinearRegression()\ntayko_lm.fit(train_X, train_y)\n\n# print coefficients\nprint('Intercept', tayko_lm.intercept_)\nprint(pd.DataFrame({'Predictor': X.columns, 'coefficient': tayko_lm.coef_}))\n\nIntercept 10.17629741458822\n              Predictor  coefficient\n0                    US    -4.620293\n1                  Freq    91.274450\n2  last_update_days_ago    -0.010374\n3             Web_order    18.628731\n4           Gender_male    -9.111366\n5        Address_is_res   -75.815354\n\n\nThe Regression equation is:\nSpending = 10.176297 + (-4.620293 * US) + (91.274450 * Freq) + (-0.010347 * last_update_days_ago) + (18.628731 * Web_order) + (-9.111366 * Gender_male) + (-75.815354 * Address_is_res)\n6.2.c.iii. Based on this model, what type of purchaser is most likely to spend a large amount of money?\nAnswer:\nA purchaser with frequent catalog transactions in the last year and who has purchased by Web order at least once. This is based on the signs of the coefficients and on their statistical significance (low p-values).\n\n# here we use sm.ols method in statsmodels which provides more information (P-values etc.) on model fit \n# run a linear regression of Spending on the predictors in the training set\ntrain_df = train_X.join(train_y)\n\npredictors = train_X.columns\nformula = 'Spending ~ ' + ' + '.join(predictors)\n\ntayko_lm = sm.ols(formula=formula, data=train_df).fit()\nprint(tayko_lm.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:               Spending   R-squared:                       0.525\nModel:                            OLS   Adj. R-squared:                  0.522\nMethod:                 Least Squares   F-statistic:                     219.3\nDate:                Sun, 08 Mar 2020   Prob (F-statistic):          1.26e-188\nTime:                        20:19:30   Log-Likelihood:                -7506.3\nNo. Observations:                1200   AIC:                         1.503e+04\nDf Residuals:                    1193   BIC:                         1.506e+04\nDf Model:                           6                                         \nCovariance Type:            nonrobust                                         \n========================================================================================\n                           coef    std err          t      P&gt;|t|      [0.025      0.975]\n----------------------------------------------------------------------------------------\nIntercept               10.1763     13.543      0.751      0.453     -16.395      36.747\nUS                      -4.6203      9.636     -0.479      0.632     -23.526      14.285\nFreq                    91.2744      2.809     32.490      0.000      85.763      96.786\nlast_update_days_ago    -0.0104      0.003     -3.029      0.003      -0.017      -0.004\nWeb_order               18.6287      7.526      2.475      0.013       3.863      33.394\nGender_male             -9.1114      7.318     -1.245      0.213     -23.468       5.245\nAddress_is_res         -75.8154      9.162     -8.275      0.000     -93.790     -57.841\n==============================================================================\nOmnibus:                      956.068   Durbin-Watson:                   1.989\nProb(Omnibus):                  0.000   Jarque-Bera (JB):            32247.650\nSkew:                           3.402   Prob(JB):                         0.00\nKurtosis:                      27.468   Cond. No.                     1.01e+04\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 1.01e+04. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\n6.2.c.iv. If we used backward elimination to reduce the number of predictors, which predictor would be dropped first from the model?\nAnswer:\nBased on the above model, US has the highest p-value. Therefore backward elimination would drop “US” first from the model.\n6.2.c.v. Show how the prediction and the prediction error are computed for the first purchase in the validation set.\nAnswer:\n\nvalid_df = valid_X.join(valid_y)\nfirst_obs_valid = valid_df.head(1)\nprint(first_obs_valid)\n\n     US  Freq  last_update_days_ago  Web_order  Gender_male  Address_is_res  \\\n674   1     2                  1346          0            1               1   \n\n     Spending  \n674         0  \n\n\nHere, we need to calculate the predicted value for the first observation of validation set. The values corresponding to the first observation of validation set are:\n\nUS = 1; Freq = 1; last_update_days_ago = 869; Web_order = 0; Gender_male = 0; Address_is_res = 0\n\n\nSpending = 10.176297+(-4.620293*US)+(91.274450*Freq)+(-0.010347*last_update_days_ago)+(18.628731*Web_order)+ (-9.111366*Gender_male)+(-75.815354*Address_is_res)\nprint(Spending)\n\n87.838911\n\n\n\n# prediction error. subtract 0 from predicted value as response (Spending) value for the first record in the validation data\n# is zero.\nprint('Prediction Error = ',Spending - 0)\n\nPrediction Error =  87.838911\n\n\n6.2.c.vi. Evaluate the predictive accuracy of the model by examining its performance on the validation set.\nAnswer:\nThe Average error is 7 dollars, with an RMSE of 137.7 dollars. This level of error is practically small given that values of spending in the data are in the range 0 - 1500 dollars. We also see that the validation set performance is approximately similar to the training set performance, indicating no overfitting.\n\n# print performance measures (training set)\nprint(\"\\nModel performance on training data:\")\nregressionSummary(train_y, tayko_lm.predict(train_X[predictors]))\n\n# predict prices in validation set, print first few predicted/actual values and residuals\n# we go back to the model we fitted using sklearn\ntayko_lm_pred = tayko_lm.predict(valid_X[predictors])\nresult = pd.DataFrame({'Predicted': tayko_lm_pred, 'Actual': valid_y, 'Residual': valid_y - tayko_lm_pred})\nprint(\"First few  predicted/actual values and residuals:\\n\")\nprint(result.head(5))\n\n# print performance measures (validation set)\nprint(\"\\nModel performance on validation data:\")\nregressionSummary(valid_y, tayko_lm_pred)\n\n\nModel performance on training data:\n\nRegression statistics\n\n               Mean Error (ME) : 0.0000\nRoot Mean Squared Error (RMSE) : 125.9999\n     Mean Absolute Error (MAE) : 79.4772\nFirst few  predicted/actual values and residuals:\n\n       Predicted  Actual    Residual\n674    89.214915       0  -89.214915\n1699  202.231362     184  -18.231362\n1282   49.159303       0  -49.159303\n1315  824.841659    1289  464.158341\n1210    0.121196       0   -0.121196\n\nModel performance on validation data:\n\nRegression statistics\n\n               Mean Error (ME) : 7.1933\nRoot Mean Squared Error (RMSE) : 136.7397\n     Mean Absolute Error (MAE) : 83.6010\n\n\n6.2.c.vii. Create a histogram of the model residuals. Do they appear to follow a normal distribution? How does this aﬀect the predictive performance of the model?\nAnswer:\nThe histogram of residuals doesn’t appear to be normal. However, this is not a great handicap – the assumption of normally distributed residuals is required for classically-derived confidence intervals, but not so much in data mining, where redictive accuracy is assessed in another way (evaluating performance on the validation data).\n\n# compute and plot the residuals for validation data\nall_residuals = valid_y - tayko_lm_pred\npd.DataFrame({'Residuals': all_residuals}).hist(bins=30)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nProblem 6.3 Predicting Airfare on New Routes.\nThe following problem takes place in the United States in the late 1990s, when many major US cities were facing issues with airport congestion, partly as a result of the 1978 deregulation of airlines. Both fares and routes were freed from regulation, and low-fare carriers such as Southwest (SW) began competing on existing routes and starting nonstop service on routes that previously lacked it. Building completely new airports is generally not feasible, but sometimes decommissioned military bases or smaller municipal airports can be reconfgured as regional or larger commercial airports. There are numerous players and interests involved in the issue (airlines, city, state and federal authorities, civic groups, the military, airport operators), and an aviation consulting firm is seeking advisory contracts with these players. The frm needs predictive models to support its consulting service. One thing the frm might want to be able to predict is fares, in the event a new airport is brought into service. The frm starts with the fle Airfares.csv, which contains real data that were collected between Q3-1996 and Q2-1997. The variables in these data are listed in Table 6.13, and are believed to be important in predicting FARE. Some airport-to-airport data are available, but most data are at the city-to-city level. One question that will be of interest in the analysis is the eﬀect that the presence or absence of Southwest has on FARE.\n\n\n\nTABLE6.13\n\n\n\n# load the data and review\nairfares_df = pd.read_csv(DATA / 'Airfares.csv')\n# check data dimension\nprint('\\ndimension\\n',airfares_df.shape)\n# view few records\nprint('\\nfirst five record\\n')\nprint(airfares_df.head())\n# variable types\nprint('\\nData Type')\nairfares_df.dtypes\n\n\ndimension\n (638, 18)\n\nfirst five record\n\n  S_CODE                  S_CITY E_CODE                  E_CITY  COUPON  NEW  \\\n0      *  Dallas/Fort Worth   TX      *  Amarillo            TX    1.00    3   \n1      *  Atlanta             GA      *  Baltimore/Wash Intl MD    1.06    3   \n2      *  Boston              MA      *  Baltimore/Wash Intl MD    1.06    3   \n3    ORD  Chicago             IL      *  Baltimore/Wash Intl MD    1.06    3   \n4    MDW  Chicago             IL      *  Baltimore/Wash Intl MD    1.06    3   \n\n  VACATION   SW       HI  S_INCOME  E_INCOME    S_POP    E_POP        SLOT  \\\n0       No  Yes  5291.99   28637.0   21112.0  3036732   205711        Free   \n1       No   No  5419.16   26993.0   29838.0  3532657  7145897        Free   \n2       No   No  9185.28   30124.0   29838.0  5787293  7145897        Free   \n3       No  Yes  2657.35   29260.0   29838.0  7830332  7145897  Controlled   \n4       No  Yes  2657.35   29260.0   29838.0  7830332  7145897        Free   \n\n   GATE  DISTANCE    PAX    FARE  \n0  Free       312   7864   64.11  \n1  Free       576   8820  174.47  \n2  Free       364   6452  207.76  \n3  Free       612  25144   85.47  \n4  Free       612  25144   85.47  \n\nData Type\n\n\nS_CODE       object\nS_CITY       object\nE_CODE       object\nE_CITY       object\nCOUPON      float64\nNEW           int64\nVACATION     object\nSW           object\nHI          float64\nS_INCOME    float64\nE_INCOME    float64\nS_POP         int64\nE_POP         int64\nSLOT         object\nGATE         object\nDISTANCE      int64\nPAX           int64\nFARE        float64\ndtype: object\n\n\n\n# preprocess\n# remove first four variables 'S_CODE', 'S_CITY', 'E_CODE', 'E_CITY'\nselected_var = ['COUPON', 'NEW', 'VACATION', 'SW', 'HI', 'S_INCOME', 'E_INCOME', 'S_POP', 'E_POP', 'SLOT', 'GATE',\n                'DISTANCE', 'PAX', 'FARE']\nairfares_df = airfares_df[selected_var]\nairfares_df.dtypes\n\nCOUPON      float64\nNEW           int64\nVACATION     object\nSW           object\nHI          float64\nS_INCOME    float64\nE_INCOME    float64\nS_POP         int64\nE_POP         int64\nSLOT         object\nGATE         object\nDISTANCE      int64\nPAX           int64\nFARE        float64\ndtype: object\n\n\n6.3.a. Explore the numerical predictors and response (FARE) by creating a correlation table and examining some scatterplots between FARE and those predictors. What seems to be the best single predictor of FARE?\nAnswer:\n\n# create a new dataframe with numerical predictors\npredictors_df = airfares_df\n\ncolumns = list(airfares_df.columns)\ncolumns.remove('VACATION')\ncolumns.remove('SW')\ncolumns.remove('SLOT')\ncolumns.remove('GATE')\npredictors_df = predictors_df[columns]\npredictors_df.columns\n\nIndex(['COUPON', 'NEW', 'HI', 'S_INCOME', 'E_INCOME', 'S_POP', 'E_POP',\n       'DISTANCE', 'PAX', 'FARE'],\n      dtype='object')\n\n\n\n#correlation table\npredictors_df.corr()\n#corr.style.background_gradient()\n\n\n\n\n\n\n\n\nCOUPON\nNEW\nHI\nS_INCOME\nE_INCOME\nS_POP\nE_POP\nDISTANCE\nPAX\nFARE\n\n\n\n\nCOUPON\n1.000000\n0.020223\n-0.347252\n-0.088403\n0.046889\n-0.107763\n0.094970\n0.746805\n-0.336974\n0.496537\n\n\nNEW\n0.020223\n1.000000\n0.054147\n0.026597\n0.113377\n-0.016672\n0.058568\n0.080965\n0.010495\n0.091730\n\n\nHI\n-0.347252\n0.054147\n1.000000\n-0.027382\n0.082393\n-0.172495\n-0.062456\n-0.312375\n-0.168961\n0.025195\n\n\nS_INCOME\n-0.088403\n0.026597\n-0.027382\n1.000000\n-0.138864\n0.517187\n-0.272280\n0.028153\n0.138197\n0.209135\n\n\nE_INCOME\n0.046889\n0.113377\n0.082393\n-0.138864\n1.000000\n-0.144059\n0.458418\n0.176531\n0.259961\n0.326092\n\n\nS_POP\n-0.107763\n-0.016672\n-0.172495\n0.517187\n-0.144059\n1.000000\n-0.280143\n0.018437\n0.284611\n0.145097\n\n\nE_POP\n0.094970\n0.058568\n-0.062456\n-0.272280\n0.458418\n-0.280143\n1.000000\n0.115640\n0.314698\n0.285043\n\n\nDISTANCE\n0.746805\n0.080965\n-0.312375\n0.028153\n0.176531\n0.018437\n0.115640\n1.000000\n-0.102482\n0.670016\n\n\nPAX\n-0.336974\n0.010495\n-0.168961\n0.138197\n0.259961\n0.284611\n0.314698\n-0.102482\n1.000000\n-0.090705\n\n\nFARE\n0.496537\n0.091730\n0.025195\n0.209135\n0.326092\n0.145097\n0.285043\n0.670016\n-0.090705\n1.000000\n\n\n\n\n\n\n\nDISTANCE is the best single predictor of FARE with correlation coefficient of 0.67.\n\n# Plot of coupon against fare\nplt.scatter(airfares_df.COUPON, airfares_df.FARE)\nplt.title('Scatter plot showing COUPON against Fare')\nplt.xlabel('Coupon')\nplt.ylabel('Fare')\nplt.show()\n\n\n\n\n\n\n\n\n\n# Plot of distance against fare\nplt.scatter(airfares_df.DISTANCE, airfares_df.FARE)\nplt.title('Scatter plot showing Distance against Fare')\nplt.xlabel('Distance')\nplt.ylabel('Fare')\nplt.show()\n\n\n\n\n\n\n\n\n6.3.b. Explore the categorical predictors (excluding the first four) by computing the percentage of ﬂights in each category. Create a pivot table with the average fare in each category. Which categorical predictor seems best for predicting FARE?\nAnswer:\n\npivot1 = pd.pivot_table(airfares_df, index= 'VACATION', values= \"FARE\",\n               aggfunc= [np.mean])\nprint(pivot1)\n\n                mean\n                FARE\nVACATION            \nNo        173.552500\nYes       125.980882\n\n\n\npd.pivot_table(airfares_df, index= 'SW', values= \"FARE\",\n               aggfunc= [np.mean])\n\n\n\n\n\n\n\n\nmean\n\n\n\nFARE\n\n\nSW\n\n\n\n\n\nNo\n188.182793\n\n\nYes\n98.382268\n\n\n\n\n\n\n\n\npd.pivot_table(airfares_df, index= 'SLOT', values= \"FARE\",\n               aggfunc= [np.mean])\n\n\n\n\n\n\n\n\nmean\n\n\n\nFARE\n\n\nSLOT\n\n\n\n\n\nControlled\n186.059396\n\n\nFree\n150.825680\n\n\n\n\n\n\n\n\npd.pivot_table(airfares_df, index= 'GATE', values= \"FARE\",\n               aggfunc= [np.mean])\n\n\n\n\n\n\n\n\nmean\n\n\n\nFARE\n\n\nGATE\n\n\n\n\n\nConstrained\n193.129032\n\n\nFree\n153.095953\n\n\n\n\n\n\n\nSW is the single best categorical predictor of FARE.\n6.3.c. Find a model for predicting the average fare on a new route:\n6.3.c.i. Convert categorical variables (e.g., SW) into dummy variables. Then, partition the data into training and validation sets. The model will be fit to the training data and evaluated on the validation set.\nAnswer:\nThe columns VACATION, SW, SLOT, and GATE are not numeric. Convert them to numeric.\n\nairfares_df['VACATION'] = [1 if v == 'Yes' else 0 for v in airfares_df['VACATION']]\nairfares_df['SW'] = [1 if v == 'Yes' else 0 for v in airfares_df['SW']]\nairfares_df['SLOT'] = [1 if v == 'Controlled' else 0 for v in airfares_df['SLOT']]\nairfares_df['GATE'] = [1 if v == 'Constrained' else 0 for v in airfares_df['GATE']]\n\n\n#partition the data into training (60%) and validation (40%) sets\npredictors = ['COUPON', 'NEW', 'VACATION', 'SW', 'HI', 'S_INCOME', 'E_INCOME', 'S_POP', 'E_POP', 'SLOT', 'GATE', 'DISTANCE',\n              'PAX']\noutcome = 'FARE'\n\n# partition the data\nX = pd.get_dummies(airfares_df[predictors], drop_first=True)\ny = airfares_df[outcome]\ntrain_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size=0.4, random_state=1)\n\nprint('Training set:', train_X.shape, 'Validation set:', valid_X.shape)\n\nTraining set: (382, 13) Validation set: (256, 13)\n\n\n6.3.c.ii. Use stepwise regression to reduce the number of predictors. You can ignore the frst four predictors (S_CODE, S_CITY, E_CODE, E_CITY). Report the estimated model selected.\nAnswer:\n\n# stepwise regression\ndef train_model(variables):\n    if len(variables) == 0:\n        return None\n    model = LinearRegression()\n    model.fit(train_X[variables], train_y)\n    return model\n\ndef score_model(model, variables):\n    if len(variables) == 0:\n        return AIC_score(train_y, [train_y.mean()] * len(train_y), model, df=1)\n    return AIC_score(train_y, model.predict(train_X[variables]), model)\n\nbest_model, best_variables = stepwise_selection(train_X.columns, train_model, score_model, verbose=True)\nprint(\"Best Subset:\", best_variables)\n\nVariables: COUPON, NEW, VACATION, SW, HI, S_INCOME, E_INCOME, S_POP, E_POP, SLOT, GATE, DISTANCE, PAX\nStart: score=4379.42, constant\nStep: score=4149.88, add DISTANCE\nStep: score=4025.89, add SW\nStep: score=3913.59, add VACATION\nStep: score=3890.27, add HI\nStep: score=3873.33, add GATE\nStep: score=3852.81, add SLOT\nStep: score=3850.30, add PAX\nStep: score=3844.58, add E_POP\nStep: score=3826.43, add S_POP\nStep: score=3821.88, add E_INCOME\nStep: score=3821.88, unchanged None\nBest Subset: ['DISTANCE', 'SW', 'VACATION', 'HI', 'GATE', 'SLOT', 'PAX', 'E_POP', 'S_POP', 'E_INCOME']\n\n\nAccording to the output the model with 10 predictors (11 coefficients) is the best one.\n6.3.c.iii. Repeat (ii) using exhaustive search instead of stepwise regression. Compare the resulting best model to the one you obtained in (ii) in terms of the predictors that are in the model.\nAnswer:\n\n# exhaustive search\ndef train_model(variables):\n    model = LinearRegression()\n    model.fit(train_X[variables], train_y)\n    return model\n\ndef score_model(model, variables):\n    pred_y = model.predict(train_X[variables])\n    # we negate as score is optimized to be as low as possible\n    return -adjusted_r2_score(train_y, pred_y, model)\n\nallVariables = train_X.columns\nresults = exhaustive_search(allVariables, train_model, score_model)\n\ndata = []\nfor result in results:\n    model = result['model']\n    variables = result['variables']\n    AIC = AIC_score(train_y, model.predict(train_X[variables]), model)\n    \n    d = {'n': result['n'], 'r2adj': -result['score'], 'AIC': AIC}\n    d.update({var: var in result['variables'] for var in allVariables})\n    data.append(d)\npd.set_option('display.width', 100)\nprint(pd.DataFrame(data, columns=('n', 'r2adj', 'AIC') + tuple(sorted(allVariables))))\npd.reset_option('display.width')\n\n     n     r2adj          AIC  COUPON  DISTANCE  E_INCOME  E_POP   GATE     HI    NEW    PAX  \\\n0    1  0.453107  4149.881509   False      True     False  False  False  False  False  False   \n1    2  0.605715  4025.892420   False      True     False  False  False  False  False  False   \n2    3  0.706909  3913.585125   False      True     False  False  False  False  False  False   \n3    4  0.724977  3890.268211   False      True     False  False  False   True  False  False   \n4    5  0.737584  3873.328296   False      True     False  False   True   True  False  False   \n5    6  0.751947  3852.808698   False      True     False  False   True   True  False  False   \n6    7  0.758747  3843.170960   False      True     False   True  False   True  False   True   \n7    8  0.765105  3833.945866   False      True     False   True   True   True  False   True   \n8    9  0.770266  3826.433471   False      True     False   True   True   True  False   True   \n9   10  0.773567  3821.876901   False      True      True   True   True   True  False   True   \n10  11  0.773595  3822.798222   False      True      True   True   True   True  False   True   \n11  12  0.773559  3823.825398   False      True      True   True   True   True   True   True   \n12  13  0.773292  3825.237680    True      True      True   True   True   True   True   True   \n\n     SLOT     SW  S_INCOME  S_POP  VACATION  \n0   False  False     False  False     False  \n1   False   True     False  False     False  \n2   False   True     False  False      True  \n3   False   True     False  False      True  \n4   False   True     False  False      True  \n5    True   True     False  False      True  \n6   False   True     False   True      True  \n7   False   True     False   True      True  \n8    True   True     False   True      True  \n9    True   True     False   True      True  \n10   True   True      True   True      True  \n11   True   True      True   True      True  \n12   True   True      True   True      True  \n\n\nStepwise selection produced best model with these 10 variables (11 coefficients): ‘DISTANCE’, ‘SW’, ‘VACATION’, ‘HI’, ‘GATE’, ‘SLOT’, ‘PAX’, ‘E_POP’, ‘S_POP’, ‘E_INCOME’. It excluded variables named COUPON, NEW and S_INCOME.\nExhaustive search also produced a model with 10 predictors (11 coefficients) which is the best one (according to adjusted R-squared and AIC values). It excluded the same variables named COUPON, NEW and S_INCOME.\n6.3.c.iv. Compare the predictive accuracy of both models (ii) and (iii) using measures such as RMSE and average error and lift charts.\nAnswer:\nSince models are same they will have the same predictive accuracy.\n\n# predictors and outcome\npredictors = ['DISTANCE', 'SW', 'VACATION', 'HI', 'GATE', 'SLOT', 'PAX', 'E_POP', 'S_POP', 'E_INCOME']\noutcome = 'FARE'\nX = train_X[predictors]\ny = train_y\n\n# fit the regression model y on x\nairfares_lm = LinearRegression()\nairfares_lm.fit(X,y)\n\n# print coefficients\nprint('intercept ', airfares_lm.intercept_)\nprint(pd.DataFrame({'Predictor': X.columns, 'coefficient': airfares_lm.coef_}))\n\n# print performance measures (training set)\nprint(\"\\nModel performance on training data:\")\nregressionSummary(train_y, airfares_lm.predict(train_X[predictors]))\n\n# predict prices in validation set, print first few predicted/actual values and residuals\nairfares_lm_pred = airfares_lm.predict(valid_X[predictors])\n\n# print performance measures (validation set)\nprint(\"\\nModel performance on validation data:\")\nregressionSummary(valid_y, airfares_lm_pred)\n\nintercept  17.56489387958925\n  Predictor  coefficient\n0  DISTANCE     0.075558\n1        SW   -43.031272\n2  VACATION   -35.865596\n3        HI     0.007188\n4      GATE    21.410803\n5      SLOT    13.915304\n6       PAX    -0.000829\n7     E_POP     0.000004\n8     S_POP     0.000004\n9  E_INCOME     0.001148\n\nModel performance on training data:\n\nRegression statistics\n\n                      Mean Error (ME) : -0.0000\n       Root Mean Squared Error (RMSE) : 34.8867\n            Mean Absolute Error (MAE) : 27.1374\n          Mean Percentage Error (MPE) : -4.5313\nMean Absolute Percentage Error (MAPE) : 20.1672\n\nModel performance on validation data:\n\nRegression statistics\n\n                      Mean Error (ME) : -1.8591\n       Root Mean Squared Error (RMSE) : 36.1129\n            Mean Absolute Error (MAE) : 28.5252\n          Mean Percentage Error (MPE) : -6.7084\nMean Absolute Percentage Error (MAPE) : 21.7198\n\n\n6.3.c.v. Using model (iii), predict the average fare on a route with the following characteristics: COUPON = 1.202, NEW = 3, VACATION = No, SW = No, HI = 4442.141, S_INCOME = 28,760, E_INCOME = 27,664, S_POP = 4,557,004, E_POP = 3,195,503, SLOT = Free, GATE = Free, PAX = 12,782, DISTANCE = 1976 miles.\nAnswer:\n\n# Fare when SW does not cover this route\n# enter new data in data frame format\nnew_data = pd.DataFrame([\n    {'VACATION': 0, 'SW': 0, 'HI': 4442.141, 'E_INCOME': 27664, 'S_POP': 4557004, 'E_POP': 3195503, 'SLOT': 1, 'GATE': 1,\n     'PAX': 12782, 'DISTANCE': 1976}])\n    \nprint(new_data)\n\n   VACATION  SW        HI  E_INCOME    S_POP    E_POP  SLOT  GATE    PAX  \\\n0         0   0  4442.141     27664  4557004  3195503     1     1  12782   \n\n   DISTANCE  \n0      1976  \n\n\n\n# predict Fare when SW does not cover this route\npred = airfares_lm.predict(new_data[predictors])\nprint(pred)\n\n[287.04763737]\n\n\n6.3.c.vi. Predict the reduction in average fare on the route in (v) if Southwest decides to cover this route [using model (iii)].\nAnswer:\n\n# predict Fare when SW decides to cover this route\nnew_data = pd.DataFrame([\n    {'VACATION': 0, 'SW': 1, 'HI': 4442.141, 'S_INCOME': 28760, 'E_INCOME': 27664, 'S_POP': 4557004, 'E_POP': 3195503, 'SLOT': 1, 'GATE': 1,\n     'PAX': 12782, 'DISTANCE': 1976}])\npred1 = airfares_lm.predict(new_data[predictors])\nprint(pred1)\n\n[244.01636531]\n\n\n\n# reduction in average fare after southwest decided to cover this route\nreduction = pred - pred1\nprint(reduction)\n\n[43.03127206]\n\n\nSo the reduction in average fare after southwest decided to cover this route is $43.\n6.3.c.vii. In reality, which of the factors will not be available for predicting the average fare from a new airport (i.e., before ﬂights start operating on those routes)? Which ones can be estimated? How?\nAnswer:\nSeveral of the variables would not be available until after flights start operating on the route.\nCOUPON - Not Available\nNEW - Not Available\nVACATION - Available\nSW - Not Available\nHI - Not Available\nS_INCOME - Available\nE_INCOME - Available\nS_POP - Available\nE_POP - Available\nSLOT - Available\nGATE - Available\nDISTANCE - Available\nPAX - Not Available\n6.3.c.viii. Select a model that includes only factors that are available before ﬂights begin to operate on the new route. Use an exhaustive search to fnd such a model.\nAnswer:\n\n# partition the data into training (60%) and validation (40%) sets\npredictors = ['VACATION', 'S_INCOME', 'E_INCOME', 'S_POP', 'E_POP', 'SLOT', 'GATE', 'DISTANCE']\noutcome = 'FARE'\n\n# partition the data\nX = pd.get_dummies(airfares_df[predictors], drop_first=True)\ny = airfares_df[outcome]\ntrain_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size=0.4, random_state=1)\n\nprint('Training set:', train_X.shape, 'Validation set:', valid_X.shape)\n\nTraining set: (382, 8) Validation set: (256, 8)\n\n\n\n# exhaustive search\ndef train_model(variables):\n    model = LinearRegression()\n    model.fit(train_X[variables], train_y)\n    return model\n\ndef score_model(model, variables):\n    pred_y = model.predict(train_X[variables])\n    # we negate as score is optimized to be as low as possible\n    return -adjusted_r2_score(train_y, pred_y, model)\n\nallVariables = train_X.columns\nresults = exhaustive_search(allVariables, train_model, score_model)\n\ndata = []\nfor result in results:\n    model = result['model']\n    variables = result['variables']\n    AIC = AIC_score(train_y, model.predict(train_X[variables]), model)\n    \n    d = {'n': result['n'], 'r2adj': -result['score'], 'AIC': AIC}\n    d.update({var: var in result['variables'] for var in allVariables})\n    data.append(d)\npd.set_option('display.width', 100)\nprint(pd.DataFrame(data, columns=('n', 'r2adj', 'AIC') + tuple(sorted(allVariables))))\npd.reset_option('display.width')\n\n   n     r2adj          AIC  DISTANCE  E_INCOME  E_POP   GATE   SLOT  S_INCOME  S_POP  VACATION\n0  1  0.453107  4149.881509      True     False  False  False  False     False  False     False\n1  2  0.562472  4065.645719      True     False  False  False  False     False  False      True\n2  3  0.607897  4024.763754      True     False  False   True  False     False  False      True\n3  4  0.639707  3993.431429      True     False  False   True   True     False  False      True\n4  5  0.655706  3977.066487      True      True  False   True   True     False  False      True\n5  6  0.660329  3972.885153      True      True  False   True   True      True  False      True\n6  7  0.664409  3969.248469      True      True   True   True   True      True  False      True\n7  8  0.663807  3970.910599      True      True   True   True   True      True   True      True\n\n\nModel with 7 predictors (8 coefficients) is the best one among other models in this case. That excludes the predictor S_POP.\n6.3.c.ix. Use the model in (viii) to predict the average fare on a route with characteristics COUPON = 1.202, NEW = 3, VACATION = No, SW = No, HI = 4442.141, S_INCOME = 28,760, E_INCOME = 27,664, S_ POP = 4,557,004, E_POP = 3,195,503, SLOT = Free, GATE = Free, PAX = 12782, DISTANCE = 1976 miles.\nAnswer:\n\n# predictors and outcome\npredictors = ['DISTANCE', 'VACATION', 'GATE', 'SLOT', 'E_POP', 'E_INCOME', 'S_INCOME']\noutcome = 'FARE'\nX = train_X[predictors]\ny = train_y\n\n#fit the regression model y on x\nairfares_lm = LinearRegression()\nairfares_lm.fit(X,y)\n\n# print coefficients\nprint('intercept ', airfares_lm.intercept_)\nprint(pd.DataFrame({'Predictor': X.columns, 'coefficient': airfares_lm.coef_}))\n\n# print performance measures (training set)\nprint(\"\\nModel performance on training data:\")\nregressionSummary(train_y, airfares_lm.predict(train_X[predictors]))\n\n# predict prices in validation set, print first few predicted/actual values and residuals\nairfares_lm_pred = airfares_lm.predict(valid_X[predictors])\n\n# print performance measures (validation set)\nprint(\"\\nModel performance on validation data:\")\nregressionSummary(valid_y, airfares_lm_pred)\n\nintercept  -40.8073738366258\n  Predictor  coefficient\n0  DISTANCE     0.077434\n1  VACATION   -41.230763\n2      GATE    38.056133\n3      SLOT    18.903701\n4     E_POP     0.000002\n5  E_INCOME     0.001974\n6  S_INCOME     0.002240\n\nModel performance on training data:\n\nRegression statistics\n\n                      Mean Error (ME) : -0.0000\n       Root Mean Squared Error (RMSE) : 42.6426\n            Mean Absolute Error (MAE) : 33.6897\n          Mean Percentage Error (MPE) : -8.2491\nMean Absolute Percentage Error (MAPE) : 25.3452\n\nModel performance on validation data:\n\nRegression statistics\n\n                      Mean Error (ME) : 0.3728\n       Root Mean Squared Error (RMSE) : 43.8816\n            Mean Absolute Error (MAE) : 35.2079\n          Mean Percentage Error (MPE) : -9.9589\nMean Absolute Percentage Error (MAPE) : 26.4530\n\n\n\n# predict Fare for new data\nprint(airfares_lm.predict(new_data[predictors]))\n\n[295.58795228]\n\n\nSo the average fare on this route is $295.59.\n6.3.c.x. Compare the predictive accuracy of this model with model (iii). Is this model good enough, or is it worthwhile reevaluating the model once ﬂights begin on the new route?\nAnswer:\nRMS error is higher in model (viii). We need to reevaluate the model once flights commence on the new routes.\n6.3.d. In competitive industries, a new entrant with a novel business plan can have a disruptive eﬀect on existing firms. If a new entrant’s business model is sustainable, other players are forced to respond by changing their business practices. If the goal of the analysis was to evaluate the eﬀect of Southwest Airlines’ presence on the airline industry rather than predicting fares on new routes, how would the analysis be diﬀerent? Describe technical and conceptual aspects.\nAnswer:\nThe analysis might look also at fares overall, instead of just on the new routes, PAX overall instead of just on the new routes; both are available.\nAlso industry profit, which is not among the variables.\nOther differences would be:\n\nIn an explanatory model, we would include predictors even if they are only known retrospectively (such predictors cannot be used in a predictive model).\nAlthough predictive accuracy is important for validating the usefulness of an explanatory model, the focus in explanatory modeling is on the correct specification of the model in the sense that the “right model” was fit to the data.\n\nSince explanatory models can be conducted with relatively small data sets, it is common not to partition the data, but rather use the same set of data to fit the model and to assess goodness-of-fit (with measures like R-squared, and residual analysis).\n\nThe emphasis in an explanatory model would be on the interpretation of the regression coefficients, in contrast to the predictive model that concentrates on predictive accuracy.\n\n\n\nProblem 6.4 Predicting Prices of Used Cars.\nThe file ToyotaCorolla.csv contains data on used cars (Toyota Corolla) on sale during late summer of 2004 in the Netherlands. It has 1436 records containing details on 38 attributes, including Price, Age, Kilometers, HP, and other specifcations. The goal is to predict the price of a used Toyota Corolla based on its specifcations. (The example in Section 6.3 is a subset of this dataset.)\nSplit the data into training (50%), validation (30%), and test (20%) datasets.\nRun a multiple linear regression with the outcome variable Price and predictor variables Age_08_04, KM, Fuel_Type, HP, Automatic, Doors, Quarterly_Tax, Mfr_Guarantee, Guarantee_Period, Airco, Automatic_airco, CD_Player, Powered_Windows, Sport_Model, and Tow_Bar.\n\n# load the data\ncar_df = pd.read_csv(DATA / 'ToyotaCorolla.csv')\n# review first five records\ncar_df.head(5)\n\n\n\n\n\n\n\n\nId\nModel\nPrice\nAge_08_04\nMfg_Month\nMfg_Year\nKM\nFuel_Type\nHP\nMet_Color\n...\nPowered_Windows\nPower_Steering\nRadio\nMistlamps\nSport_Model\nBackseat_Divider\nMetallic_Rim\nRadio_cassette\nParking_Assistant\nTow_Bar\n\n\n\n\n0\n1\nTOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors\n13500\n23\n10\n2002\n46986\nDiesel\n90\n1\n...\n1\n1\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n1\n2\nTOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors\n13750\n23\n10\n2002\n72937\nDiesel\n90\n1\n...\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n2\n3\nTOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors\n13950\n24\n9\n2002\n41711\nDiesel\n90\n1\n...\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n3\n4\nTOYOTA Corolla 2.0 D4D HATCHB TERRA 2/3-Doors\n14950\n26\n7\n2002\n48000\nDiesel\n90\n0\n...\n0\n1\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n4\n5\nTOYOTA Corolla 2.0 D4D HATCHB SOL 2/3-Doors\n13750\n30\n3\n2002\n38500\nDiesel\n90\n0\n...\n1\n1\n0\n1\n0\n1\n0\n0\n0\n0\n\n\n\n\n5 rows × 39 columns\n\n\n\n\n# data dimension\ncar_df.shape\n\n(1436, 39)\n\n\n\n# variable dat types\ncar_df.dtypes\n\nId                    int64\nModel                object\nPrice                 int64\nAge_08_04             int64\nMfg_Month             int64\nMfg_Year              int64\nKM                    int64\nFuel_Type            object\nHP                    int64\nMet_Color             int64\nColor                object\nAutomatic             int64\nCC                    int64\nDoors                 int64\nCylinders             int64\nGears                 int64\nQuarterly_Tax         int64\nWeight                int64\nMfr_Guarantee         int64\nBOVAG_Guarantee       int64\nGuarantee_Period      int64\nABS                   int64\nAirbag_1              int64\nAirbag_2              int64\nAirco                 int64\nAutomatic_airco       int64\nBoardcomputer         int64\nCD_Player             int64\nCentral_Lock          int64\nPowered_Windows       int64\nPower_Steering        int64\nRadio                 int64\nMistlamps             int64\nSport_Model           int64\nBackseat_Divider      int64\nMetallic_Rim          int64\nRadio_cassette        int64\nParking_Assistant     int64\nTow_Bar               int64\ndtype: object\n\n\n\n# partition the data into training (50%), validation (30%) and test (20%) sets\npredictors = ['Age_08_04', 'KM', 'Fuel_Type', 'HP', 'Automatic', 'Doors', 'Quarterly_Tax', 'Mfr_Guarantee', 'Guarantee_Period', \n              'Airco', 'Automatic_airco', 'CD_Player', 'Powered_Windows', 'Sport_Model', 'Tow_Bar']\noutcome = 'Price'\n\nX = pd.get_dummies(car_df[predictors], drop_first=True)\ny = car_df[outcome]\n\ntrain_X, temp_X, train_y, temp_y = train_test_split(X, y, test_size=0.5, random_state=1)\nvalid_X, test_X, valid_y, test_y = train_test_split(temp_X, temp_y, test_size=0.4, random_state=1)\n\nprint('Training : ', train_X.shape)\nprint('Validation : ', valid_X.shape)\nprint('Test : ', test_X.shape)\n\nTraining :  (718, 16)\nValidation :  (430, 16)\nTest :  (288, 16)\n\n\n\n# we fit the model \ncar_lm = LinearRegression(normalize=True)\ncar_lm.fit(train_X, train_y)\n\n# print coefficients\nprint('intercept ', car_lm.intercept_)\nprint(pd.DataFrame({'Predictor': X.columns, 'coefficient': car_lm.coef_}))\n\n# print performance measures\nprint('Training set')\nregressionSummary(train_y, car_lm.predict(train_X))\nprint('Validation set')\nregressionSummary(valid_y, car_lm.predict(valid_X))\n\nintercept  8998.52855293567\n           Predictor  coefficient\n0          Age_08_04  -112.139772\n1                 KM    -0.019437\n2                 HP    39.474311\n3          Automatic   583.265499\n4              Doors   214.445095\n5      Quarterly_Tax    17.192451\n6      Mfr_Guarantee   129.110109\n7   Guarantee_Period    77.305623\n8              Airco    45.831357\n9    Automatic_airco  2956.041165\n10         CD_Player   276.496513\n11   Powered_Windows   521.606032\n12       Sport_Model   517.807321\n13           Tow_Bar  -267.478660\n14  Fuel_Type_Diesel  2163.735433\n15  Fuel_Type_Petrol  1968.284558\nTraining set\n\nRegression statistics\n\n                      Mean Error (ME) : -0.0000\n       Root Mean Squared Error (RMSE) : 1235.9139\n            Mean Absolute Error (MAE) : 914.8383\n          Mean Percentage Error (MPE) : -0.9286\nMean Absolute Percentage Error (MAPE) : 8.9407\nValidation set\n\nRegression statistics\n\n                      Mean Error (ME) : -69.0074\n       Root Mean Squared Error (RMSE) : 1135.0299\n            Mean Absolute Error (MAE) : 909.1486\n          Mean Percentage Error (MPE) : -1.4502\nMean Absolute Percentage Error (MAPE) : 9.3849\n\n\nThe coefficients are for unscaled predictors; this makes them hard to interpret. Build the second model using standardized predictors.\n\nscaler = StandardScaler()\nscaler.fit(train_X * 1.0)\n\ncar_lm_2 = LinearRegression(normalize=True)\ncar_lm_2.fit(scaler.transform(train_X * 1.0), train_y)\n\n# print coefficients\nprint('intercept ', car_lm_2.intercept_)\nprint(pd.DataFrame({'Predictor': X.columns, 'coefficient': car_lm_2.coef_}))\n\n# print performance measures\nprint('\\nTraining set')\nregressionSummary(train_y, car_lm_2.predict(scaler.transform(train_X * 1.0)))\nprint('\\nValidation set')\nregressionSummary(valid_y, car_lm_2.predict(scaler.transform(valid_X * 1.0)))\nprint('\\nTest set')\nregressionSummary(test_y, car_lm_2.predict(scaler.transform(test_X * 1.0)))\n\nintercept  10819.600278551532\n           Predictor  coefficient\n0          Age_08_04 -2147.646599\n1                 KM  -733.233815\n2                 HP   561.560105\n3          Automatic   138.397408\n4              Doors   204.115796\n5      Quarterly_Tax   736.625704\n6      Mfr_Guarantee    63.736167\n7   Guarantee_Period   236.425907\n8              Airco    22.915589\n9    Automatic_airco   716.472989\n10         CD_Player   115.825853\n11   Powered_Windows   258.558294\n12       Sport_Model   238.405391\n13           Tow_Bar  -119.722262\n14  Fuel_Type_Diesel   669.505409\n15  Fuel_Type_Petrol   639.103514\n\nTraining set\n\nRegression statistics\n\n                      Mean Error (ME) : -0.0000\n       Root Mean Squared Error (RMSE) : 1235.9139\n            Mean Absolute Error (MAE) : 914.8383\n          Mean Percentage Error (MPE) : -0.9286\nMean Absolute Percentage Error (MAPE) : 8.9407\n\nValidation set\n\nRegression statistics\n\n                      Mean Error (ME) : -69.0074\n       Root Mean Squared Error (RMSE) : 1135.0299\n            Mean Absolute Error (MAE) : 909.1486\n          Mean Percentage Error (MPE) : -1.4502\nMean Absolute Percentage Error (MAPE) : 9.3849\n\nTest set\n\nRegression statistics\n\n                      Mean Error (ME) : 91.2764\n       Root Mean Squared Error (RMSE) : 1271.8760\n            Mean Absolute Error (MAE) : 958.7654\n          Mean Percentage Error (MPE) : 0.3914\nMean Absolute Percentage Error (MAPE) : 9.6015\n\n\n6.4.a. What appear to be the three or four most important car specifcations for predicting the car’s price?\nAnswer:\nThe four most important car specifications in predicting the car’s price are:\nAge_08_04\nKM\nHP\nAutomatic_airco\n6.4.b. Using metrics you consider useful, assess the performance of the model in predicting prices.\nAnswer:\nThe performance statistics for the training and validation sets are comparable. This indicates that the model is stable and not overfitting."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-03-ProbSolutions-Viz.html",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-03-ProbSolutions-Viz.html",
    "title": "Chapter 3: Data Visualization",
    "section": "",
    "text": "2019-2020 Galit Shmueli, Peter C. Bruce, Peter Gedeck\n\nData Mining for Business Analytics: Concepts, Techniques, and Applications in Python (First Edition) Galit Shmueli, Peter C. Bruce, Peter Gedeck, and Nitin R. Patel. 2019.\nDate: 2020-03-08\nPython Version: 3.8.2 Jupyter Notebook Version: 5.6.1\nPackages: - matplotlib: 3.2.0 - pandas: 1.0.1\nThe assistance from Mr. Kuber Deokar and Ms. Anuja Kulkarni in preparing these solutions is gratefully acknowledged.\n\n\n# import required packages for this chapter\nfrom pathlib import Path\n\nimport pandas as pd\nimport matplotlib.pylab as plt\n\n%matplotlib inline\n\n\n# Working directory:\n#\n# We assume that data are kept in the same directory as the notebook. If you keep your \n# data in a different folder, replace the argument of the `Path`\nDATA = Path('.').resolve().parent / 'data'\nFIGURES = Path('.').resolve().parent / 'figures' / 'chapter_03'\nFIGURES.mkdir(exist_ok=True, parents=True)\n# and then load data using \n#\n# pd.read_csv(DATA / ‘filename.csv’)\n\n\nProblem 3.1 Shipments of Household Appliances: Line Graphs.\nThe file ApplianceShipments.csv contains the series of quarterly shipments (in millions of dollars) of US household appliances between 1985 and 1989.\n3.1.a. Create a well-formatted time plot of the data using Python.\n\n# load the data\n\nshipments_df = pd.read_csv(DATA / 'ApplianceShipments.csv', squeeze=True)\nshipments_df.shape\n\n(20, 2)\n\n\n\nshipments_df.head()\n\n\n\n\n\n\n\n\nQuarter\nShipments\n\n\n\n\n0\nQ1-1985\n4009\n\n\n1\nQ2-1985\n4321\n\n\n2\nQ3-1985\n4224\n\n\n3\nQ4-1985\n3944\n\n\n4\nQ1-1986\n4123\n\n\n\n\n\n\n\n\n# convert the data frame to be suitable for time series analysis\n# create date range for quarters\nshipments_df = shipments_df.iloc[0:,1:2]\ndate_range = pd.date_range(start='1/1/1985', end='12/31/1989', freq='Q')\nshipments_df['Date'] = pd.to_datetime(date_range)\nshipments_ts = pd.Series(shipments_df.Shipments.values, index=shipments_df.Date)\nshipments_ts\n\nDate\n1985-03-31    4009\n1985-06-30    4321\n1985-09-30    4224\n1985-12-31    3944\n1986-03-31    4123\n1986-06-30    4522\n1986-09-30    4657\n1986-12-31    4030\n1987-03-31    4493\n1987-06-30    4806\n1987-09-30    4551\n1987-12-31    4485\n1988-03-31    4595\n1988-06-30    4799\n1988-09-30    4417\n1988-12-31    4258\n1989-03-31    4245\n1989-06-30    4900\n1989-09-30    4585\n1989-12-31    4533\ndtype: int64\n\n\n\n# line plot\nshipments_ts.plot(ylim=(3000, 5500), legend=False)\nplt.xlabel('Quarter')\nplt.ylabel('Shipments (in millon dollars)')\n\nText(0, 0.5, 'Shipments (in millon dollars)')\n\n\n\n\n\n\n\n\n\n3.1.b. Does there appear to be a quarterly pattern? For a closer view of the patterns, zoom in to the range of 3500–5000 on the y-axis.\n\nshipments_ts.plot(ylim=(3500, 5000), legend=False)\nplt.xlabel('Quarter')\nplt.ylabel('Shipments (in millon dollars)')\n\nText(0, 0.5, 'Shipments (in millon dollars)')\n\n\n\n\n\n\n\n\n\nYes, there appears a quarterly pattern in the above time series. The time plot shows a repeating pattern each year (more shipments in Q2 and Q3 and less shipments in Q1 and Q4). Limiting the y-axis to the range of 3500 - 5000 shows the quarterly pattern in the time series a bit more clearly.\n3.1.c. Using Python, create one chart with four separate lines, one line for each of Q1, Q2, Q3, and Q4. In Python, this can be achieved by adding columns for quarter and year. Then group the data frame by quarter and then plot shipment versus year for each quarter as a separate series on a line graph. Zoom in to the range of 3500–5000 on the y-axis. Does there appear to be a difference between quarters?\n\n# generate separate time series for each quarter\nQ1 = shipments_df.iloc[[0, 4, 8, 12, 16], :]\nQ1_ts = pd.Series(Q1.Shipments.values, index=Q1.Date)\nQ2 = shipments_df.iloc[[1, 5, 9, 13, 17], :]\nQ2_ts = pd.Series(Q2.Shipments.values, index=Q2.Date)\nQ3 = shipments_df.iloc[[2, 6, 10, 14, 18], :]\nQ3_ts = pd.Series(Q3.Shipments.values, index=Q3.Date)\nQ4 = shipments_df.iloc[[3, 7, 11, 15, 19], :]\nQ4_ts = pd.Series(Q4.Shipments.values, index=Q4.Date)\n\n\n# line plots\nplt.plot(Q1_ts, data=Q1_ts, label='Q1')\nplt.legend()\nplt.plot(Q2_ts, data=Q2_ts, label='Q2')\nplt.legend()\nplt.plot(Q3_ts, data=Q3_ts, label='Q3')\nplt.legend()\nplt.plot(Q4_ts, data=Q4_ts, label='Q3')\nplt.legend()\nplt.xlabel('Quarter')\nplt.ylabel('Shipments (in millon dollars)')\n\nText(0, 0.5, 'Shipments (in millon dollars)')\n\n\n\n\n\n\n\n\n\nYes, we can see differences between the quarters. From the above plot we can see that the shipments in quarters Q2 and Q3 are larger than in quarters Q1 and Q4.\n3.1.d. Using Python, create a line graph of the series at a yearly aggregated level (i.e., the total shipments in each year).\n\n# yearly aggregated series\nbyYear = shipments_ts.groupby(pd.Grouper(freq='A')).mean()\nbyYear.plot(ylim=(3500, 5000), legend=False)\nplt.xlabel('Quarter')\nplt.ylabel('Shipments (in millon dollars)')\n\nText(0, 0.5, 'Shipments (in millon dollars)')\n\n\n\n\n\n\n\n\n\nShipments show increasing trend from 1985-1987. After that it is mostly flat.\n\n\n3.2. Sales of Riding Mowers: Scatter Plots.\nA company that manufactures riding mowers wants to identify the best sales prospects for an intensive sales campaign. In particular, the manufacturer is interested in classifying households as prospective owners or nonowners on the basis of Income (in $ 1000s) and Lot Size (in 1000 ft2). The marketing expert looked at a random sample of 24 households, given in the file RidingMowers.csv.\n3.2.a. Using Python, create a scatter plot of Lot Size vs. Income, color-coded by the outcome variable owner/nonowner. Make sure to obtain a well-formatted plot (create legible labels and a legend, etc.).\n\n# load the data\nmowers_df = pd.read_csv(DATA / 'RidingMowers.csv', squeeze=True)\nmowers_df.shape\n\n(24, 3)\n\n\n\nmowers_df.head()\n\n\n\n\n\n\n\n\nIncome\nLot_Size\nOwnership\n\n\n\n\n0\n60.0\n18.4\nOwner\n\n\n1\n85.5\n16.8\nOwner\n\n\n2\n64.8\n21.6\nOwner\n\n\n3\n61.5\n20.8\nOwner\n\n\n4\n87.0\n23.6\nOwner\n\n\n\n\n\n\n\n\n# scatter plot of Lot Size vs. Income\nmowers_df.plot.scatter(x='Income', y='Lot_Size',\nc=['C0' if c == 'Owner' else 'C1' for c in mowers_df.Ownership])\n\n\n\n\n\n\n\n\nOwners appears to have higher income and larger lot sizes as compared to non-owners.\n\n\n3.3. Laptop Sales at a London Computer Chain: Bar Charts and Boxplots.\nThe file LaptopSalesJanuary2008.csv contains data for all sales of laptops at a computer chain in London in January 2008. This is a subset of the full dataset that includes data for the entire year.\n\n# load the data\nlaptop_df = pd.read_csv(DATA / 'LaptopSalesJanuary2008.csv', squeeze=True)\n# Check if data is loaded correctly\nprint(mowers_df.shape)\nprint(laptop_df.head())\n\n(24, 3)\n            Date  Configuration Customer Postcode Store Postcode  \\\n0  1/1/2008 0:01            163          EC4V 5BH        SE1 2BN   \n1  1/1/2008 0:02            320           SW4 0JL       SW12 9HD   \n2  1/1/2008 0:04             23          EC3V 1LR         E2 0RY   \n3  1/1/2008 0:04            169          SW1P 3AU        SE1 2BN   \n4  1/1/2008 0:06            365          EC4V 4EG       SW1V 4QQ   \n\n   Retail Price  Screen Size (Inches)  Battery Life (Hours)  RAM (GB)  \\\n0           455                    15                     5         1   \n1           545                    15                     6         1   \n2           515                    15                     4         1   \n3           395                    15                     5         1   \n4           585                    15                     6         2   \n\n   Processor Speeds (GHz) Integrated Wireless?  HD Size (GB)  \\\n0                     2.0                  Yes            80   \n1                     2.0                   No           300   \n2                     2.0                  Yes           300   \n3                     2.0                   No            40   \n4                     2.0                   No           120   \n\n  Bundled Applications?  OS X Customer  OS Y Customer  OS X Store  OS Y Store  \\\n0                   Yes         532041         180995    534057.0    179682.0   \n1                    No         529240         175537    528739.0    173080.0   \n2                   Yes         533095         181047    535652.0    182961.0   \n3                   Yes         529902         179641    534057.0    179682.0   \n4                   Yes         531684         180948    528924.0    178440.0   \n\n   CustomerStoreDistance  \n0            2405.873022  \n1            2507.558574  \n2            3194.001409  \n3            4155.202281  \n4            3729.298057  \n\n\n\n# Print the list of variables to the screen\nlaptop_df.columns\n\nIndex(['Date', 'Configuration', 'Customer Postcode', 'Store Postcode',\n       'Retail Price', 'Screen Size (Inches)', 'Battery Life (Hours)',\n       'RAM (GB)', 'Processor Speeds (GHz)', 'Integrated Wireless?',\n       'HD Size (GB)', 'Bundled Applications?', 'OS X Customer',\n       'OS Y Customer', 'OS X Store', 'OS Y Store', 'CustomerStoreDistance'],\n      dtype='object')\n\n\n\n# Change the variable names to be more suitable for analysis\nlaptop_df.columns = (\"Date\", \"Configuration\", \"Customer_Postcode\", \"Store_Postcode\",\n                     \"Retail_Price\", \"Screen_Size_Inches\", \"Battery_Life_Hours\",\n                     \"RAM_GB\", \"Processor_Speeds_GHz\", \"Integrated_Wireless\", \n                     \"HD_Size_GB\", \"Bundled_Applications\", \"customer_X\", \n                     \"customer_Y\", \"store_X\", \"store_Y\", \"CustomerStoreDistance\")\n\n#laptop_df.columns\n\n3.3.a. Create a bar chart, showing the average retail price by store. Which store has the highest average? Which has the lowest?\n\n# barchart of store vs. mean retail price\n# compute mean retail price per store\nax = laptop_df.groupby('Store_Postcode').mean().Retail_Price.plot(kind='bar')\nax.set_ylabel('Avg. Retail price')\n\nText(0, 0.5, 'Avg. Retail price')\n\n\n\n\n\n\n\n\n\nFrom the above bar chart we can see that store postcode “N17 6QA” has highest average (494) and store postcode “W4 3PH” has lowest average 481 for the retail price.\n3.3.b. To better compare retail prices across stores, create side-by-side boxplots of retail price by store. Now compare the prices in the two stores from (a). Does there seem to be a difference between their price distributions?\n\n# The argument vert creates horizontal boxplots\nax = laptop_df.boxplot(column='Retail_Price', by='Store_Postcode', vert=False)\n# Suppress the titles\nplt.suptitle('')  \nplt.title('')\nplt.show()\n\n\n\n\n\n\n\n\nWe can see that the middle range of the prices is roughly similar across stores (just under 500 dollars, plus or minus 50). The distributions for “N17 6QA” and “W4 3PH” are similar, except N17 6QA is shifted to the right (more expensive) by about 20 dollars compared to W4 3PH.\n\nProblem 3.4 (below) - To be done with an interactive tool such as Tableau or Spotfire.\nThe file Ch-03.4-ProbSolution-Spotfire.pdf demonstrates a solution using Spotfire.\n\n\n\n3.4 Laptop Sales at a London Computer Chain: Interactive Visualization.\nThe next exercises are designed for using an interactive visualization tool. The file LaptopSales.csv is a comma-separated file with nearly 300,000 rows. ENBIS (the European Network for Business and Industrial Statistics) provided these data as part of a contest organized in the fall of 2009.\nScenario: Imagine that you are a new analyst for a company called Acell (a company selling laptops). You have been provided with data about products and sales. You need to help the company with their business goal of planning a product strategy and pricing policies that will maximize Acell’s projected revenues in 2009.\nUsing an interactive visualization tool, answer the following questions.\na. Price Questions:\n\nAt what price are the laptops actually selling?\nDoes price change with time? (Hint: Make sure that the date column is recognized as such. The software should then enable different temporal aggregation choices, e.g., plotting the data by weekly or monthly aggregates, or even by day of week.)\nAre prices consistent across retail outlets?\nHow does price change with configuration?\n\nb. Location Questions:\n\nWhere are the stores and customers located?\nWhich stores are selling the most?\nHow far would customers travel to buy a laptop?\n\n◦ Hint 1: You should be able to aggregate the data, for example, plot the sum or average of the prices.\n◦ Hint 2: Use the coordinated highlighting between multiple visualizations in the same page, for example, select a store in one view to see the matching customers in another visualization.\n◦ Hint 3: Explore the use of filters to see differences. Make sure to filter in the zoomed out view. For example, try to use a “store location” slider as an alternative way to dynamically compare store locations. This might be more useful to spot outlier patterns if there were 50 store locations to compare.\n\nTry an alternative way of looking at how far customers traveled. Do this by creating a new data column that computes the distance between customer and store.\n\nc. Revenue Questions:\n\nHow do the sales volume in each store relate to Acell’s revenues?\nHow does this relationship depend on the configuration?\n\nd. Configuration Questions:\n\nWhat are the details of each configuration? How does this relate to price?\nDo all stores sell all configurations?"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-09-ProbSolutions-CART.html",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-09-ProbSolutions-CART.html",
    "title": "Chapter 9: Classification and Regression Trees (CART)",
    "section": "",
    "text": "2019-2020 Galit Shmueli, Peter C. Bruce, Peter Gedeck\n\nData Mining for Business Analytics: Concepts, Techniques, and Applications in Python (First Edition) Galit Shmueli, Peter C. Bruce, Peter Gedeck, and Nitin R. Patel. 2019.\nDate: 2020-03-08\nPython Version: 3.8.2 Jupyter Notebook Version: 5.6.1\nPackages: - dmba: 0.0.12 - matplotlib: 3.2.0 - numpy: 1.18.1 - pandas: 1.0.1 - scikit-learn: 0.22.2\nThe assistance from Mr. Kuber Deokar and Ms. Anuja Kulkarni in preparing these solutions is gratefully acknowledged.\n# Import required packages for this chapter\nfrom pathlib import Path\nimport warnings\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n\nimport matplotlib.pylab as plt\n\nfrom dmba import plotDecisionTree, gainsChart, liftChart\nfrom dmba import classificationSummary, regressionSummary\n\n%matplotlib inline\n# Working directory:\n#\n# We assume that data are kept in the same directory as the notebook. If you keep your \n# data in a different folder, replace the argument of the `Path`\nDATA = Path('.')\n# and then load data using \n#\n# pd.read_csv(DATA / ‘filename.csv’)"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-09-ProbSolutions-CART.html#data-preprocessing",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-09-ProbSolutions-CART.html#data-preprocessing",
    "title": "Chapter 9: Classification and Regression Trees (CART)",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\n\n# Load the data\nebay_df = pd.read_csv(DATA / 'eBayAuctions.csv')\n\n# convert categorical variables into indicator and drop the first column of each of them\nebay_df = pd.get_dummies(ebay_df, prefix_sep='_')\nebay_df.drop(columns=['Category_Antique/Art/Craft', 'currency_EUR', 'endDay_Fri'], inplace=True)\n\nebay_df.head()\n\n\n\n\n\n\n\n\nsellerRating\nDuration\nClosePrice\nOpenPrice\nCompetitive?\nCategory_Automotive\nCategory_Books\nCategory_Business/Industrial\nCategory_Clothing/Accessories\nCategory_Coins/Stamps\n...\nCategory_SportingGoods\nCategory_Toys/Hobbies\ncurrency_GBP\ncurrency_US\nendDay_Mon\nendDay_Sat\nendDay_Sun\nendDay_Thu\nendDay_Tue\nendDay_Wed\n\n\n\n\n0\n3249\n5\n0.01\n0.01\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n\n\n1\n3249\n5\n0.01\n0.01\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n\n\n2\n3249\n5\n0.01\n0.01\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n\n\n3\n3249\n5\n0.01\n0.01\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n\n\n4\n3249\n5\n0.01\n0.01\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n\n\n\n\n5 rows × 30 columns\n\n\n\n\n# remove question mark from response name\nebay_df.columns = [c.replace('?', '') for c in ebay_df.columns]\n\n# convert variable Duration to categorical data type\nebay_df['Duration'].astype('category')\n\n# Separate out predictors and outcome variable \nX = ebay_df.drop(columns='Competitive')\ny = ebay_df['Competitive']\n\n\n# partition the data into training (60%) and validation (40%) sets. use random_state=1 for reproducibility of results\ntrain_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size=0.4, random_state=1)\n\n9.1.a. Fit a classification tree using all predictors. To avoid overfitting, set the minimum number of records in a terminal node to 50 and the maximum tree depth to 7. Write down the results in terms of rules. (Note: If you had to slightly reduce the number of predictors due to software limitations, or for clarity of presentation, which would be a good variable to choose?)\n\n# fit the tree\nsmallClassTree = DecisionTreeClassifier(max_depth=7, min_samples_split=50, min_impurity_decrease=0.01)\nsmallClassTree.fit(train_X, train_y)\n\nplotDecisionTree(smallClassTree, feature_names=train_X.columns)\n\n'You need to install pydotplus to visualize decision trees'\n\n\n9.1.b. Is this model practical for predicting the outcome of a new auction?\nAnswer:\nNo, because the close price is not known at the start of the auction.\n9.1.c. Describe the interesting and uninteresting information that these rules provide.\nAnswer:\nThe main effect of a competitive auction is to raise the item’s price. So, by including both the opening and closing price as predictors, our tree is naturally going to focus repeatedly on those two variables since they, by themselves, are the most powerful predictors of a competitive auction. In order to make the tree pay attention to more useful variables we will need to remove the closing price as a predictor.\n9.1.d. Fit another classification tree (using a tree with a minimum number of records per terminal node = 50 and maximum depth = 7), this time only with predictors that can be used for predicting the outcome of a new auction. Describe the resulting tree in terms of rules. Make sure to report the smallest set of rules required for classification.\nAnswer:\n\nebay_df.columns\n\nIndex(['sellerRating', 'Duration', 'ClosePrice', 'OpenPrice', 'Competitive',\n       'Category_Automotive', 'Category_Books', 'Category_Business/Industrial',\n       'Category_Clothing/Accessories', 'Category_Coins/Stamps',\n       'Category_Collectibles', 'Category_Computer', 'Category_Electronics',\n       'Category_EverythingElse', 'Category_Health/Beauty',\n       'Category_Home/Garden', 'Category_Jewelry', 'Category_Music/Movie/Game',\n       'Category_Photography', 'Category_Pottery/Glass',\n       'Category_SportingGoods', 'Category_Toys/Hobbies', 'currency_GBP',\n       'currency_US', 'endDay_Mon', 'endDay_Sat', 'endDay_Sun', 'endDay_Thu',\n       'endDay_Tue', 'endDay_Wed'],\n      dtype='object')\n\n\n\n# Select only those variables which can be used for predicting the outcome of new auction.\n\n# Create a new dataframe with predictors\npredictors_df = ebay_df\n\ncolumns = list(ebay_df.columns)\ncolumns\n\ncolumns.remove('ClosePrice')\ncolumns.remove('endDay_Mon')\ncolumns.remove('endDay_Sat')\ncolumns.remove('endDay_Sun')\ncolumns.remove('endDay_Thu')\ncolumns.remove('endDay_Tue')\ncolumns.remove('endDay_Wed')\nebay_df = ebay_df[columns]\nebay_df.columns\n\nIndex(['sellerRating', 'Duration', 'OpenPrice', 'Competitive',\n       'Category_Automotive', 'Category_Books', 'Category_Business/Industrial',\n       'Category_Clothing/Accessories', 'Category_Coins/Stamps',\n       'Category_Collectibles', 'Category_Computer', 'Category_Electronics',\n       'Category_EverythingElse', 'Category_Health/Beauty',\n       'Category_Home/Garden', 'Category_Jewelry', 'Category_Music/Movie/Game',\n       'Category_Photography', 'Category_Pottery/Glass',\n       'Category_SportingGoods', 'Category_Toys/Hobbies', 'currency_GBP',\n       'currency_US'],\n      dtype='object')\n\n\n\n# separate out the predictors and response variable\nX1 = ebay_df.drop(columns='Competitive')\ny1 = ebay_df['Competitive']\n\n# partition the data into training (60%) and validation (40%) sets. Set random_state=1 for reproducibility of results\ntrain_X1, valid_X1, train_y1, valid_y1 = train_test_split(X1, y1, test_size=0.4, random_state=1)\n\n# fit the tree\nsmallClassTree1 = DecisionTreeClassifier(max_depth=7, min_samples_split=50, min_impurity_decrease=0.01)\nsmallClassTree1.fit(train_X1, train_y1)\n\nplotDecisionTree(smallClassTree1, feature_names=train_X1.columns)\n\n'You need to install pydotplus to visualize decision trees'\n\n\nSet of rules\nIf (OpenPrice &lt;= 3.615) then class = 1\nIf (OpenPrice &gt; 3.615) and (sellerRating &lt;= 601.5) then class = 1\nIf (OpenPrice &gt; 3.615) and (sellerRating &gt; 601.5) then class = 0\n9.1.e. Plot the resulting tree on a scatter plot: Use the two axes for the two best (quantitative) predictors. Each auction will appear as a point, with coordinates corresponding to its values on those two predictors. Use different colors or symbols to separate competitive and noncompetitive auctions. Draw lines (you can sketch these by hand or use Python) at the values that create splits. Does this splitting seem reasonable with respect to the meaning of the two predictors? Does it seem to do a good job of separating the two classes?\nAnswer:\n\n# plot sellerRating vs. OpenPrice \nfig, ax = plt.subplots()\n\nsubset = ebay_df.loc[ebay_df['Competitive']== 0]\nax.scatter(subset.sellerRating, subset.OpenPrice, marker='o', label='Competitive=No')\n\nsubset1 = ebay_df.loc[ebay_df['Competitive']== 1]\nax.scatter(subset1.sellerRating, subset1.OpenPrice, marker='D', label='Competitive=Yes')\n\nplt.xlabel('sellerRating')  # set x-axis label\nplt.ylabel('OpenPrice')  # set y-axis label\n    \nhandles, labels = ax.get_legend_handles_labels()\nax.legend(handles, labels)\n\n\n\n\n\n\n\n\nThe splitting points are located way down in the lower left corner so the scatterplot does not reveal much to answer the question. We could do a log transform, or restrict the scatterplot to the smaller values.\n\n# apply log transformation on the variables OpenPrice and sellerRating\ntrain_df = train_X.copy()\ntrain_df['Competitive'] = train_y\ntrain_df['log_OpenPrice'] = np.log(train_df['OpenPrice'])\ntrain_df['log_sellerRating'] = np.log(train_df['sellerRating'])\n\n\n# plot sellerRating vs. OpenPrice \nfig, ax = plt.subplots()\n\nsubset = train_df.loc[train_df['Competitive']== 0]\nax.scatter(subset.log_sellerRating, subset.log_OpenPrice, marker='o', label='Competitive=No')\n\nsubset1 = train_df.loc[train_df['Competitive']== 1]\nax.scatter(subset1.log_sellerRating, subset1.log_OpenPrice, marker='D', label='Competitive=Yes')\n\nplt.xlabel('sellerRating')  # set x-axis label\nplt.ylabel('OpenPrice')  # set y-axis label\n    \nhandles, labels = ax.get_legend_handles_labels()\nax.legend(handles, labels)\n\n\n\n\n\n\n\n\nAlthough it is hard to see a clear separation between the competitive and noncompetitive auctions on the scatter plot, we see a set of competitive auctions with opening price &gt; 10 held by sellers with rating &lt; 1000. This is surprising, because we would expect higher seller ratings to be associated with a higher chance of competitive auctions. We also see that the bulk of auctions with opening price &lt; $1 or so are competitive, and this is not surprising (lower opening bids attract bidders).\n9.1.f. Examine the lift chart and the confusion matrix for the tree. What can you say about the predictive performance of this model?\nAnswer:\n\n# predicted classes\npred_t = smallClassTree1.predict(train_X1)\npred_v = smallClassTree1.predict(valid_X1)\n\n# predicted probabilities for validation set\npred_prob_v = (smallClassTree1.predict_proba(valid_X1))\n\n# put it together in a data frame\ntree_result = pd.DataFrame({'actual': valid_y1,\n                           'p(0)': [p[0] for p in pred_prob_v],\n                           'p(1)': [p[1] for p in pred_prob_v],\n                           'predicted': pred_v})\ntree_result.head()\n\n\n\n\n\n\n\n\nactual\np(0)\np(1)\npredicted\n\n\n\n\n1287\n0\n0.711340\n0.288660\n0\n\n\n1017\n1\n0.270567\n0.729433\n1\n\n\n1047\n0\n0.711340\n0.288660\n0\n\n\n108\n1\n0.270567\n0.729433\n1\n\n\n1084\n1\n0.711340\n0.288660\n0\n\n\n\n\n\n\n\n\n# confusion matrices for training and validation sets\nprint('Training Set: Confusion matrix\\n')\nclassificationSummary(train_y1, pred_t)\nprint(\"\\nValidation Set: Confusion matrix\\n\")\nclassificationSummary(valid_y1, pred_v)\n\nTraining Set: Confusion matrix\n\nConfusion Matrix (Accuracy 0.7058)\n\n       Prediction\nActual   0   1\n     0 345 208\n     1 140 490\n\nValidation Set: Confusion matrix\n\nConfusion Matrix (Accuracy 0.7072)\n\n       Prediction\nActual   0   1\n     0 228 125\n     1 106 330\n\n\n\n# lift chart for validation set\ndf = tree_result.sort_values(by=['p(1)'], ascending=False)\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n\ngainsChart(df.actual, ax=axes[0])\nliftChart(df['p(1)'], title=False, ax=axes[1])\n\n\n\n\n\n\n\n\nFrom the lift chart we see that the model’s predictive performance (i.e. correctly capturing the auctions that are most likely to be competitive) is better than the baseline model, since its lift curve is higher than that of the baseline model. The decile lift chart has a limited set of lift values because the tree is a simple one, producing only three potential predicted probabilities.\n9.1.g. Based on this last tree, what can you conclude from these data about the chances of an auction obtaining at least two bids and its relationship to the auction settings set by the seller (duration, opening price, ending day, currency)? What would you recommend for a seller as the strategy that will most likely lead to a competitive auction?\nAnswer:\nTo get a competitive auction, the most important factor controlled by the seller is the opening price, with lower opening prices attracting more bidders. From the tree we see that if the opening price &lt; \\$3.615, then it will lead to a competitive auction. In particular, it appears that setting the opening price to the minimum of $0.01 (which is eBay’s default) is most likely to lead to a competitive auction."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-09-ProbSolutions-CART.html#data-preprocessing-1",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-09-ProbSolutions-CART.html#data-preprocessing-1",
    "title": "Chapter 9: Classification and Regression Trees (CART)",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\n\n# Load the data\ndelays_df = pd.read_csv(DATA / 'FlightDelays.csv')\ndelays_df.head()\n\n\n\n\n\n\n\n\nCRS_DEP_TIME\nCARRIER\nDEP_TIME\nDEST\nDISTANCE\nFL_DATE\nFL_NUM\nORIGIN\nWeather\nDAY_WEEK\nDAY_OF_MONTH\nTAIL_NUM\nFlight Status\n\n\n\n\n0\n1455\nOH\n1455\nJFK\n184\n01/01/2004\n5935\nBWI\n0\n4\n1\nN940CA\nontime\n\n\n1\n1640\nDH\n1640\nJFK\n213\n01/01/2004\n6155\nDCA\n0\n4\n1\nN405FJ\nontime\n\n\n2\n1245\nDH\n1245\nLGA\n229\n01/01/2004\n7208\nIAD\n0\n4\n1\nN695BR\nontime\n\n\n3\n1715\nDH\n1709\nLGA\n229\n01/01/2004\n7215\nIAD\n0\n4\n1\nN662BR\nontime\n\n\n4\n1039\nDH\n1035\nLGA\n229\n01/01/2004\n7792\nIAD\n0\n4\n1\nN698BR\nontime\n\n\n\n\n\n\n\n\n# convert variable DAY_WEEK to categorical data type\ndelays_df['DAY_WEEK'].astype('category')\n\n0       4\n1       4\n2       4\n3       4\n4       4\n       ..\n2196    6\n2197    6\n2198    6\n2199    6\n2200    6\nName: DAY_WEEK, Length: 2201, dtype: category\nCategories (7, int64): [1, 2, 3, 4, 5, 6, 7]\n\n\n\n# bin CRS_DEP_TIME variable into 8 bins\ndelays_df['binned_CRS_DEP_TIME'] = pd.cut(delays_df.CRS_DEP_TIME, 8, labels=False)\ndelays_df['binned_CRS_DEP_TIME'].astype('category')\n\n0       4\n1       5\n2       3\n3       5\n4       2\n       ..\n2196    0\n2197    5\n2198    5\n2199    3\n2200    5\nName: binned_CRS_DEP_TIME, Length: 2201, dtype: category\nCategories (8, int64): [0, 1, 2, 3, 4, 5, 6, 7]\n\n\n\n# remove DAY_OF_MONTH variable\npredictors_df = delays_df\ncolumns = list(delays_df.columns)\ncolumns.remove('DAY_OF_MONTH')\npredictors_df = predictors_df[columns]\n\n9.2.a. Fit a classification tree to the flight delay variable using all the relevant predictors. Do not include DEP_TIME (actual departure time) in the model because it is unknown at the time of prediction (unless we are generating our predictions of delays after the plane takes off, which is unlikely). Use a tree with maximum depth 8 and minimum impurity decrease = 0.01. Express the resulting tree as a set of rules.\n\n# select only those variables which can be used for predicting the outcome.\n# create a new dataframe with predictors\ncolumns = list(predictors_df.columns)\ncolumns\n\ncolumns.remove('CRS_DEP_TIME')\ncolumns.remove('DEP_TIME')\ncolumns.remove('FL_DATE')\ncolumns.remove('FL_NUM')\ncolumns.remove('TAIL_NUM')\ncolumns.remove('Flight Status')\npredictors_df = predictors_df[columns]\npredictors_df.columns\n\npredictors_df.head()\n\n\n\n\n\n\n\n\nCARRIER\nDEST\nDISTANCE\nORIGIN\nWeather\nDAY_WEEK\nbinned_CRS_DEP_TIME\n\n\n\n\n0\nOH\nJFK\n184\nBWI\n0\n4\n4\n\n\n1\nDH\nJFK\n213\nDCA\n0\n4\n5\n\n\n2\nDH\nLGA\n229\nIAD\n0\n4\n3\n\n\n3\nDH\nLGA\n229\nIAD\n0\n4\n5\n\n\n4\nDH\nLGA\n229\nIAD\n0\n4\n2\n\n\n\n\n\n\n\n\n# create dummies for categorical variables\npredictors_df = pd.get_dummies(predictors_df, prefix_sep='_')\npredictors_df.columns\n\nIndex(['DISTANCE', 'Weather', 'DAY_WEEK', 'binned_CRS_DEP_TIME', 'CARRIER_CO',\n       'CARRIER_DH', 'CARRIER_DL', 'CARRIER_MQ', 'CARRIER_OH', 'CARRIER_RU',\n       'CARRIER_UA', 'CARRIER_US', 'DEST_EWR', 'DEST_JFK', 'DEST_LGA',\n       'ORIGIN_BWI', 'ORIGIN_DCA', 'ORIGIN_IAD'],\n      dtype='object')\n\n\n\n# partition the data into training (60%) and validation (40%) sets. set random_state=1 for the reproducibility of results\nX = predictors_df\ny = delays_df['Flight Status']\n\ntrain_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size=0.4, random_state=1)\ntrain_X.head()\n\n\n\n\n\n\n\n\nDISTANCE\nWeather\nDAY_WEEK\nbinned_CRS_DEP_TIME\nCARRIER_CO\nCARRIER_DH\nCARRIER_DL\nCARRIER_MQ\nCARRIER_OH\nCARRIER_RU\nCARRIER_UA\nCARRIER_US\nDEST_EWR\nDEST_JFK\nDEST_LGA\nORIGIN_BWI\nORIGIN_DCA\nORIGIN_IAD\n\n\n\n\n1215\n229\n0\n7\n7\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n\n\n1476\n214\n0\n3\n0\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n0\n1\n0\n\n\n1897\n214\n0\n2\n1\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n0\n1\n0\n\n\n83\n214\n0\n5\n3\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n0\n1\n0\n\n\n1172\n213\n0\n6\n4\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n0\n\n\n\n\n\n\n\n\n# fit the tree model and draw tree\nsmallClassTree = DecisionTreeClassifier(max_depth=8, min_samples_split=50, min_impurity_decrease=0.01)\nsmallClassTree.fit(train_X, train_y)\n\nprint('Tree has {} nodes'.format(smallClassTree.tree_.node_count))\nplotDecisionTree(smallClassTree, feature_names=train_X.columns)\n\nTree has 3 nodes\n\n\n'You need to install pydotplus to visualize decision trees'\n\n\nThe limited tree has only one splitting variable: Weather\nIf (Weather &lt;= 0.5) then classify as Ontime.\n9.2.b. If you needed to fly between DCA and EWR on a Monday at 7:00 AM, would you be able to use this tree? What other information would you need? Is it available in practice? What information is redundant?\nAnswer:\nWe cannot use this tree, because we must know the Weather. The redundant information is the day of week (Monday) and arrival airport (EWR). The tree requires knowing whether the weather was inclement or not. We may not know the weather in advance.\n9.2.c. Fit the same tree as in (a), this time excluding the Weather predictor. Display both the resulting (small) tree and the full-grown tree. You will find that the small tree contains a single terminal node.\n\n# remove variable Weather from the analysis\npredictors1_df = predictors_df\ncolumns = list(predictors_df.columns)\ncolumns\ncolumns.remove('Weather')\npredictors1_df = predictors1_df[columns]\npredictors1_df.columns\n\nIndex(['DISTANCE', 'DAY_WEEK', 'binned_CRS_DEP_TIME', 'CARRIER_CO',\n       'CARRIER_DH', 'CARRIER_DL', 'CARRIER_MQ', 'CARRIER_OH', 'CARRIER_RU',\n       'CARRIER_UA', 'CARRIER_US', 'DEST_EWR', 'DEST_JFK', 'DEST_LGA',\n       'ORIGIN_BWI', 'ORIGIN_DCA', 'ORIGIN_IAD'],\n      dtype='object')\n\n\n\nX1 = predictors1_df\ny1 = delays_df['Flight Status']\n\ntrain_X1, valid_X1, train_y1, valid_y1 = train_test_split(X1, y1, test_size=0.4, random_state=1)\n\n# full-grown tree\nfullClassTree = DecisionTreeClassifier()\nfullClassTree.fit(train_X1, train_y1)\n\nprint('Tree has {} nodes'.format(fullClassTree.tree_.node_count))\nplotDecisionTree(fullClassTree, feature_names=train_X1.columns)\n\nTree has 591 nodes\n\n\n'You need to install pydotplus to visualize decision trees'\n\n\n\n# small tree\nClassTree = DecisionTreeClassifier(max_depth=8, min_samples_split=50, min_impurity_decrease=0.01)\nClassTree.fit(train_X1, train_y1)\n\nprint('Tree has {} nodes'.format(ClassTree.tree_.node_count))\nplotDecisionTree(ClassTree, feature_names=train_X1.columns)\n\nTree has 1 nodes\n\n\n'You need to install pydotplus to visualize decision trees'\n\n\n9.2.c.i. How is the small tree used for classification? (What is the rule for classifying?)\nAnswer:\nIn the small tree we get a single terminal node labeled “ontime.” Therefore any new flight will be classified as being “on time”.\n9.2.c.ii To what is this rule equivalent?\nAnswer:\nThis is equivalent to the naïve rule, which is the majority rule. In this dataset most of the flights arrived on time, and therefore the naïve rule is to classify a new flight as arriving on time.\n9.2.c.iii. Examine the full-grown tree. What are the top three predictors according to this tree?\nAnswer:\nCARRIER=US, CARRIER=DL, binned_CRS_DEP_TIME.\n9.2.c.iv. Why, technically, does the small tree result in a single node?\nAnswer:\nThe small tree results in a single node because adding splits would violate one of the constraints we used to limit tree growth.\n9.2.c.v. What is the disadvantage of using the top levels of the full-grown tree as opposed to the small tree?\nAnswer:\nSimply using the top layers of the full decision tree would be an ad hoc visual approach, and does not assure an optimal solution. Using “gridsearchCV” in Python allows us to set the parameters for limiting tree growth by assessing error on the validation data. We did not use “gridsearchCV” in this case, opting to keep the problem simple by specifying the limiting parameters.\n9.2.c.vi. Compare this general result to that from logistic regression in the example in Chapter 10. What are possible reasons for the classification tree’s failure to find a good predictive model?\n\n# predictive power of tree\n# predicted values for validation set\npred_v = ClassTree.predict(valid_X1)\n# confusion matrix for validation set\nclassificationSummary(valid_y1, pred_v)\n\nConfusion Matrix (Accuracy 0.8104)\n\n       Prediction\nActual   0   1\n     0   0 167\n     1   0 714\n\n\nThe logistic regression improves only marginally on the naive rule, and the simple tree we ended up with improves on it not at all, so it is likely that the predictor variables offer little predictive power. With poor predictive power and a relatively small dataset, the model-based logistic regression may do a bit better by virtue of imposing structure, as opposed to the tree, which is more at the mercy of the data and can suffer from instability."
  },
  {
    "objectID": "Course/MGS3001/07_Script/python-analytics/test.html",
    "href": "Course/MGS3001/07_Script/python-analytics/test.html",
    "title": "Quarto Test",
    "section": "",
    "text": "::: {.cell}\n\nCode\nprint(\"Python test\")\n\n::: {.cell-output .cell-output-stdout}\nPython test\n::: :::\n## This is R test\n::: {.cell}\n\nCode\ndata(mtcars)\nsummary(mtcars)\n\n::: {.cell-output .cell-output-stdout}\n      mpg             cyl             disp             hp       \n Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n 1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \n Median :19.20   Median :6.000   Median :196.3   Median :123.0  \n Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n 3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \n Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n      drat             wt             qsec             vs        \n Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n 1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \n Median :3.695   Median :3.325   Median :17.71   Median :0.0000  \n Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n 3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \n Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n       am              gear            carb      \n Min.   :0.0000   Min.   :3.000   Min.   :1.000  \n 1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000  \n Median :0.0000   Median :4.000   Median :2.000  \n Mean   :0.4062   Mean   :3.688   Mean   :2.812  \n 3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000  \n Max.   :1.0000   Max.   :5.000   Max.   :8.000  \n:::\n\nCode\nhist(mtcars$mpg)\n\n::: {.cell-output-display}  ::: :::"
  },
  {
    "objectID": "Course/MGS3001/07_Script/python-analytics/test.html#this-is-a-test",
    "href": "Course/MGS3001/07_Script/python-analytics/test.html#this-is-a-test",
    "title": "Quarto Test",
    "section": "",
    "text": "::: {.cell}\n\nCode\nprint(\"Python test\")\n\n::: {.cell-output .cell-output-stdout}\nPython test\n::: :::\n## This is R test\n::: {.cell}\n\nCode\ndata(mtcars)\nsummary(mtcars)\n\n::: {.cell-output .cell-output-stdout}\n      mpg             cyl             disp             hp       \n Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n 1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \n Median :19.20   Median :6.000   Median :196.3   Median :123.0  \n Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n 3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \n Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n      drat             wt             qsec             vs        \n Min.   :2.760   Min.   :1.513   Min.   :14.50   Min.   :0.0000  \n 1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1st Qu.:0.0000  \n Median :3.695   Median :3.325   Median :17.71   Median :0.0000  \n Mean   :3.597   Mean   :3.217   Mean   :17.85   Mean   :0.4375  \n 3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90   3rd Qu.:1.0000  \n Max.   :4.930   Max.   :5.424   Max.   :22.90   Max.   :1.0000  \n       am              gear            carb      \n Min.   :0.0000   Min.   :3.000   Min.   :1.000  \n 1st Qu.:0.0000   1st Qu.:3.000   1st Qu.:2.000  \n Median :0.0000   Median :4.000   Median :2.000  \n Mean   :0.4062   Mean   :3.688   Mean   :2.812  \n 3rd Qu.:1.0000   3rd Qu.:4.000   3rd Qu.:4.000  \n Max.   :1.0000   Max.   :5.000   Max.   :8.000  \n:::\n\nCode\nhist(mtcars$mpg)\n\n::: {.cell-output-display}  ::: :::"
  },
  {
    "objectID": "Course/MGS3001/07_Script/python-analytics/analysis2.html",
    "href": "Course/MGS3001/07_Script/python-analytics/analysis2.html",
    "title": "Read Data",
    "section": "",
    "text": "# Import Modules\n\n\nimport pandas as pd\nimport numpy as np\nfrom dfply import *\nimport statsmodels.formula.api as smf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n\n\nairports = pd.read_csv('airports.csv')\nflight = pd.read_csv('flights.csv')\n\n\nSummary\n\nflight.head\n\n&lt;bound method NDFrame.head of         Unnamed: 0  year  month  day  dep_time  sched_dep_time  dep_delay  \\\n0                1  2013      1    1     517.0             515        2.0   \n1                2  2013      1    1     533.0             529        4.0   \n2                3  2013      1    1     542.0             540        2.0   \n3                4  2013      1    1     544.0             545       -1.0   \n4                5  2013      1    1     554.0             600       -6.0   \n...            ...   ...    ...  ...       ...             ...        ...   \n336771      336772  2013      9   30       NaN            1455        NaN   \n336772      336773  2013      9   30       NaN            2200        NaN   \n336773      336774  2013      9   30       NaN            1210        NaN   \n336774      336775  2013      9   30       NaN            1159        NaN   \n336775      336776  2013      9   30       NaN             840        NaN   \n\n        arr_time  sched_arr_time  arr_delay carrier  flight tailnum origin  \\\n0          830.0             819       11.0      UA    1545  N14228    EWR   \n1          850.0             830       20.0      UA    1714  N24211    LGA   \n2          923.0             850       33.0      AA    1141  N619AA    JFK   \n3         1004.0            1022      -18.0      B6     725  N804JB    JFK   \n4          812.0             837      -25.0      DL     461  N668DN    LGA   \n...          ...             ...        ...     ...     ...     ...    ...   \n336771       NaN            1634        NaN      9E    3393     NaN    JFK   \n336772       NaN            2312        NaN      9E    3525     NaN    LGA   \n336773       NaN            1330        NaN      MQ    3461  N535MQ    LGA   \n336774       NaN            1344        NaN      MQ    3572  N511MQ    LGA   \n336775       NaN            1020        NaN      MQ    3531  N839MQ    LGA   \n\n       dest  air_time  distance  hour  minute            time_hour  \n0       IAH     227.0      1400     5      15  2013-01-01 05:00:00  \n1       IAH     227.0      1416     5      29  2013-01-01 05:00:00  \n2       MIA     160.0      1089     5      40  2013-01-01 05:00:00  \n3       BQN     183.0      1576     5      45  2013-01-01 05:00:00  \n4       ATL     116.0       762     6       0  2013-01-01 06:00:00  \n...     ...       ...       ...   ...     ...                  ...  \n336771  DCA       NaN       213    14      55  2013-09-30 14:00:00  \n336772  SYR       NaN       198    22       0  2013-09-30 22:00:00  \n336773  BNA       NaN       764    12      10  2013-09-30 12:00:00  \n336774  CLE       NaN       419    11      59  2013-09-30 11:00:00  \n336775  RDU       NaN       431     8      40  2013-09-30 08:00:00  \n\n[336776 rows x 20 columns]&gt;\n\n\n\nsummary = flight.describe()\nsummary = summary.transpose()\nsummary.head\n\n&lt;bound method NDFrame.head of                    count           mean           std     min       25%  \\\nUnnamed: 0      336776.0  168388.500000  97219.001466     1.0  84194.75   \nyear            336776.0    2013.000000      0.000000  2013.0   2013.00   \nmonth           336776.0       6.548510      3.414457     1.0      4.00   \nday             336776.0      15.710787      8.768607     1.0      8.00   \ndep_time        328521.0    1349.109947    488.281791     1.0    907.00   \nsched_dep_time  336776.0    1344.254840    467.335756   106.0    906.00   \ndep_delay       328521.0      12.639070     40.210061   -43.0     -5.00   \narr_time        328063.0    1502.054999    533.264132     1.0   1104.00   \nsched_arr_time  336776.0    1536.380220    497.457142     1.0   1124.00   \narr_delay       327346.0       6.895377     44.633292   -86.0    -17.00   \nflight          336776.0    1971.923620   1632.471938     1.0    553.00   \nair_time        327346.0     150.686460     93.688305    20.0     82.00   \ndistance        336776.0    1039.912604    733.233033    17.0    502.00   \nhour            336776.0      13.180247      4.661316     1.0      9.00   \nminute          336776.0      26.230100     19.300846     0.0      8.00   \n\n                     50%        75%       max  \nUnnamed: 0      168388.5  252582.25  336776.0  \nyear              2013.0    2013.00    2013.0  \nmonth                7.0      10.00      12.0  \nday                 16.0      23.00      31.0  \ndep_time          1401.0    1744.00    2400.0  \nsched_dep_time    1359.0    1729.00    2359.0  \ndep_delay           -2.0      11.00    1301.0  \narr_time          1535.0    1940.00    2400.0  \nsched_arr_time    1556.0    1945.00    2359.0  \narr_delay           -5.0      14.00    1272.0  \nflight            1496.0    3465.00    8500.0  \nair_time           129.0     192.00     695.0  \ndistance           872.0    1389.00    4983.0  \nhour                13.0      17.00      23.0  \nminute              29.0      44.00      59.0  &gt;\n\n\n\n\nData Handeling\n\n# Select\n(flight &gt;&gt;\n select(X.origin, X.dest, X.hour) &gt;&gt;\n head()\n)\n\n\n\n\n\n\n\n\norigin\ndest\nhour\n\n\n\n\n0\nEWR\nIAH\n5\n\n\n1\nLGA\nIAH\n5\n\n\n2\nJFK\nMIA\n5\n\n\n3\nJFK\nBQN\n5\n\n\n4\nLGA\nATL\n6\n\n\n\n\n\n\n\n\n# Drop\n(flight &gt;&gt;\n drop(X.year, X.month, X.day) &gt;&gt;\n head()\n)\n\n\n\n\n\n\n\n\nUnnamed: 0\ndep_time\nsched_dep_time\ndep_delay\narr_time\nsched_arr_time\narr_delay\ncarrier\nflight\ntailnum\norigin\ndest\nair_time\ndistance\nhour\nminute\ntime_hour\n\n\n\n\n0\n1\n517.0\n515\n2.0\n830.0\n819\n11.0\nUA\n1545\nN14228\nEWR\nIAH\n227.0\n1400\n5\n15\n2013-01-01 05:00:00\n\n\n1\n2\n533.0\n529\n4.0\n850.0\n830\n20.0\nUA\n1714\nN24211\nLGA\nIAH\n227.0\n1416\n5\n29\n2013-01-01 05:00:00\n\n\n2\n3\n542.0\n540\n2.0\n923.0\n850\n33.0\nAA\n1141\nN619AA\nJFK\nMIA\n160.0\n1089\n5\n40\n2013-01-01 05:00:00\n\n\n3\n4\n544.0\n545\n-1.0\n1004.0\n1022\n-18.0\nB6\n725\nN804JB\nJFK\nBQN\n183.0\n1576\n5\n45\n2013-01-01 05:00:00\n\n\n4\n5\n554.0\n600\n-6.0\n812.0\n837\n-25.0\nDL\n461\nN668DN\nLGA\nATL\n116.0\n762\n6\n0\n2013-01-01 06:00:00\n\n\n\n\n\n\n\n\n# Select\n(flight &gt;&gt;\n select(~X.hour, ~X.minute)\n)\n\n\n\n\n\n\n\n\nUnnamed: 0\nyear\nmonth\nday\ndep_time\nsched_dep_time\ndep_delay\narr_time\nsched_arr_time\narr_delay\ncarrier\nflight\ntailnum\norigin\ndest\nair_time\ndistance\ntime_hour\n\n\n\n\n0\n1\n2013\n1\n1\n517.0\n515\n2.0\n830.0\n819\n11.0\nUA\n1545\nN14228\nEWR\nIAH\n227.0\n1400\n2013-01-01 05:00:00\n\n\n1\n2\n2013\n1\n1\n533.0\n529\n4.0\n850.0\n830\n20.0\nUA\n1714\nN24211\nLGA\nIAH\n227.0\n1416\n2013-01-01 05:00:00\n\n\n2\n3\n2013\n1\n1\n542.0\n540\n2.0\n923.0\n850\n33.0\nAA\n1141\nN619AA\nJFK\nMIA\n160.0\n1089\n2013-01-01 05:00:00\n\n\n3\n4\n2013\n1\n1\n544.0\n545\n-1.0\n1004.0\n1022\n-18.0\nB6\n725\nN804JB\nJFK\nBQN\n183.0\n1576\n2013-01-01 05:00:00\n\n\n4\n5\n2013\n1\n1\n554.0\n600\n-6.0\n812.0\n837\n-25.0\nDL\n461\nN668DN\nLGA\nATL\n116.0\n762\n2013-01-01 06:00:00\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n336771\n336772\n2013\n9\n30\nNaN\n1455\nNaN\nNaN\n1634\nNaN\n9E\n3393\nNaN\nJFK\nDCA\nNaN\n213\n2013-09-30 14:00:00\n\n\n336772\n336773\n2013\n9\n30\nNaN\n2200\nNaN\nNaN\n2312\nNaN\n9E\n3525\nNaN\nLGA\nSYR\nNaN\n198\n2013-09-30 22:00:00\n\n\n336773\n336774\n2013\n9\n30\nNaN\n1210\nNaN\nNaN\n1330\nNaN\nMQ\n3461\nN535MQ\nLGA\nBNA\nNaN\n764\n2013-09-30 12:00:00\n\n\n336774\n336775\n2013\n9\n30\nNaN\n1159\nNaN\nNaN\n1344\nNaN\nMQ\n3572\nN511MQ\nLGA\nCLE\nNaN\n419\n2013-09-30 11:00:00\n\n\n336775\n336776\n2013\n9\n30\nNaN\n840\nNaN\nNaN\n1020\nNaN\nMQ\n3531\nN839MQ\nLGA\nRDU\nNaN\n431\n2013-09-30 08:00:00\n\n\n\n\n336776 rows × 18 columns\n\n\n\n\n# Filtering\n(flight &gt;&gt; \n     mask(X.month == 1, X.day == 1, X.origin == 'JFK', X.hour &gt; 10) &gt;&gt;\n     head())\n\n\n\n\n\n\n\n\nUnnamed: 0\nyear\nmonth\nday\ndep_time\nsched_dep_time\ndep_delay\narr_time\nsched_arr_time\narr_delay\ncarrier\nflight\ntailnum\norigin\ndest\nair_time\ndistance\nhour\nminute\ntime_hour\n\n\n\n\n151\n152\n2013\n1\n1\n848.0\n1835\n853.0\n1001.0\n1950\n851.0\nMQ\n3944\nN942MQ\nJFK\nBWI\n41.0\n184\n18\n35\n2013-01-01 18:00:00\n\n\n258\n259\n2013\n1\n1\n1059.0\n1100\n-1.0\n1210.0\n1215\n-5.0\nMQ\n3792\nN509MQ\nJFK\nDCA\n50.0\n213\n11\n0\n2013-01-01 11:00:00\n\n\n265\n266\n2013\n1\n1\n1111.0\n1115\n-4.0\n1222.0\n1226\n-4.0\nB6\n24\nN279JB\nJFK\nBTV\n52.0\n266\n11\n15\n2013-01-01 11:00:00\n\n\n266\n267\n2013\n1\n1\n1112.0\n1100\n12.0\n1440.0\n1438\n2.0\nUA\n285\nN517UA\nJFK\nSFO\n364.0\n2586\n11\n0\n2013-01-01 11:00:00\n\n\n272\n273\n2013\n1\n1\n1124.0\n1100\n24.0\n1435.0\n1431\n4.0\nB6\n641\nN590JB\nJFK\nSFO\n349.0\n2586\n11\n0\n2013-01-01 11:00:00\n\n\n\n\n\n\n\n\n# Arrange (ascending)\n(flight &gt;&gt;\n arrange(X.distance, X.hour) &gt;&gt;\n head()\n)\n\n\n\n\n\n\n\n\nUnnamed: 0\nyear\nmonth\nday\ndep_time\nsched_dep_time\ndep_delay\narr_time\nsched_arr_time\narr_delay\ncarrier\nflight\ntailnum\norigin\ndest\nair_time\ndistance\nhour\nminute\ntime_hour\n\n\n\n\n275945\n275946\n2013\n7\n27\nNaN\n106\nNaN\nNaN\n245\nNaN\nUS\n1632\nNaN\nEWR\nLGA\nNaN\n17\n1\n6\n2013-07-27 01:00:00\n\n\n3083\n3084\n2013\n1\n4\n1240.0\n1200\n40.0\n1333.0\n1306\n27.0\nEV\n4193\nN14972\nEWR\nPHL\n30.0\n80\n12\n0\n2013-01-04 12:00:00\n\n\n3901\n3902\n2013\n1\n5\n1155.0\n1200\n-5.0\n1241.0\n1306\n-25.0\nEV\n4193\nN14902\nEWR\nPHL\n29.0\n80\n12\n0\n2013-01-05 12:00:00\n\n\n3426\n3427\n2013\n1\n4\n1829.0\n1615\n134.0\n1937.0\n1721\n136.0\nEV\n4502\nN15983\nEWR\nPHL\n28.0\n80\n16\n15\n2013-01-04 16:00:00\n\n\n10235\n10236\n2013\n1\n12\n1613.0\n1617\n-4.0\n1708.0\n1722\n-14.0\nEV\n4616\nN11150\nEWR\nPHL\n36.0\n80\n16\n17\n2013-01-12 16:00:00\n\n\n\n\n\n\n\n\n# Arrange (Decending)\n(flight &gt;&gt;\n arrange(X.distance, X.hour, ascending=False) &gt;&gt;\n head()\n)\n\n\n\n\n\n\n\n\nUnnamed: 0\nyear\nmonth\nday\ndep_time\nsched_dep_time\ndep_delay\narr_time\nsched_arr_time\narr_delay\ncarrier\nflight\ntailnum\norigin\ndest\nair_time\ndistance\nhour\nminute\ntime_hour\n\n\n\n\n28259\n28260\n2013\n10\n2\n951.0\n1000\n-9.0\n1438.0\n1450\n-12.0\nHA\n51\nN381HA\nJFK\nHNL\n623.0\n4983\n10\n0\n2013-10-02 10:00:00\n\n\n30229\n30230\n2013\n10\n4\n954.0\n1000\n-6.0\n1438.0\n1450\n-12.0\nHA\n51\nN380HA\nJFK\nHNL\n618.0\n4983\n10\n0\n2013-10-04 10:00:00\n\n\n31157\n31158\n2013\n10\n5\n1002.0\n1000\n2.0\n1418.0\n1450\n-32.0\nHA\n51\nN384HA\nJFK\nHNL\n593.0\n4983\n10\n0\n2013-10-05 10:00:00\n\n\n31850\n31851\n2013\n10\n6\n958.0\n1000\n-2.0\n1415.0\n1450\n-35.0\nHA\n51\nN389HA\nJFK\nHNL\n601.0\n4983\n10\n0\n2013-10-06 10:00:00\n\n\n32842\n32843\n2013\n10\n7\n957.0\n1000\n-3.0\n1504.0\n1450\n14.0\nHA\n51\nN390HA\nJFK\nHNL\n642.0\n4983\n10\n0\n2013-10-07 10:00:00\n\n\n\n\n\n\n\n\n# Mutate\n(flight &gt;&gt;\n mutate(\n  new_distance = X.distance / 1000,\n  carrier_origin = X.carrier + X.origin) &gt;&gt;\n head())\n\n\n\n\n\n\n\n\nUnnamed: 0\nyear\nmonth\nday\ndep_time\nsched_dep_time\ndep_delay\narr_time\nsched_arr_time\narr_delay\n...\ntailnum\norigin\ndest\nair_time\ndistance\nhour\nminute\ntime_hour\nnew_distance\ncarrier_origin\n\n\n\n\n0\n1\n2013\n1\n1\n517.0\n515\n2.0\n830.0\n819\n11.0\n...\nN14228\nEWR\nIAH\n227.0\n1400\n5\n15\n2013-01-01 05:00:00\n1.400\nUAEWR\n\n\n1\n2\n2013\n1\n1\n533.0\n529\n4.0\n850.0\n830\n20.0\n...\nN24211\nLGA\nIAH\n227.0\n1416\n5\n29\n2013-01-01 05:00:00\n1.416\nUALGA\n\n\n2\n3\n2013\n1\n1\n542.0\n540\n2.0\n923.0\n850\n33.0\n...\nN619AA\nJFK\nMIA\n160.0\n1089\n5\n40\n2013-01-01 05:00:00\n1.089\nAAJFK\n\n\n3\n4\n2013\n1\n1\n544.0\n545\n-1.0\n1004.0\n1022\n-18.0\n...\nN804JB\nJFK\nBQN\n183.0\n1576\n5\n45\n2013-01-01 05:00:00\n1.576\nB6JFK\n\n\n4\n5\n2013\n1\n1\n554.0\n600\n-6.0\n812.0\n837\n-25.0\n...\nN668DN\nLGA\nATL\n116.0\n762\n6\n0\n2013-01-01 06:00:00\n0.762\nDLLGA\n\n\n\n\n5 rows × 22 columns\n\n\n\n\n# Group by\n(flight &gt;&gt;\n group_by(X.origin)\n)\n\n\n\n\n\n\n\n\nUnnamed: 0\nyear\nmonth\nday\ndep_time\nsched_dep_time\ndep_delay\narr_time\nsched_arr_time\narr_delay\ncarrier\nflight\ntailnum\norigin\ndest\nair_time\ndistance\nhour\nminute\ntime_hour\n\n\n\n\n0\n1\n2013\n1\n1\n517.0\n515\n2.0\n830.0\n819\n11.0\nUA\n1545\nN14228\nEWR\nIAH\n227.0\n1400\n5\n15\n2013-01-01 05:00:00\n\n\n1\n2\n2013\n1\n1\n533.0\n529\n4.0\n850.0\n830\n20.0\nUA\n1714\nN24211\nLGA\nIAH\n227.0\n1416\n5\n29\n2013-01-01 05:00:00\n\n\n2\n3\n2013\n1\n1\n542.0\n540\n2.0\n923.0\n850\n33.0\nAA\n1141\nN619AA\nJFK\nMIA\n160.0\n1089\n5\n40\n2013-01-01 05:00:00\n\n\n3\n4\n2013\n1\n1\n544.0\n545\n-1.0\n1004.0\n1022\n-18.0\nB6\n725\nN804JB\nJFK\nBQN\n183.0\n1576\n5\n45\n2013-01-01 05:00:00\n\n\n4\n5\n2013\n1\n1\n554.0\n600\n-6.0\n812.0\n837\n-25.0\nDL\n461\nN668DN\nLGA\nATL\n116.0\n762\n6\n0\n2013-01-01 06:00:00\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n336771\n336772\n2013\n9\n30\nNaN\n1455\nNaN\nNaN\n1634\nNaN\n9E\n3393\nNaN\nJFK\nDCA\nNaN\n213\n14\n55\n2013-09-30 14:00:00\n\n\n336772\n336773\n2013\n9\n30\nNaN\n2200\nNaN\nNaN\n2312\nNaN\n9E\n3525\nNaN\nLGA\nSYR\nNaN\n198\n22\n0\n2013-09-30 22:00:00\n\n\n336773\n336774\n2013\n9\n30\nNaN\n1210\nNaN\nNaN\n1330\nNaN\nMQ\n3461\nN535MQ\nLGA\nBNA\nNaN\n764\n12\n10\n2013-09-30 12:00:00\n\n\n336774\n336775\n2013\n9\n30\nNaN\n1159\nNaN\nNaN\n1344\nNaN\nMQ\n3572\nN511MQ\nLGA\nCLE\nNaN\n419\n11\n59\n2013-09-30 11:00:00\n\n\n336775\n336776\n2013\n9\n30\nNaN\n840\nNaN\nNaN\n1020\nNaN\nMQ\n3531\nN839MQ\nLGA\nRDU\nNaN\n431\n8\n40\n2013-09-30 08:00:00\n\n\n\n\n336776 rows × 20 columns\n\n\n\n\n# Group by and summarize\n(flight &gt;&gt;\n group_by(X.origin) &gt;&gt;\n summarize(mean_distance = X.distance.mean())\n)\n\n\n\n\n\n\n\n\norigin\nmean_distance\n\n\n\n\n0\nEWR\n1056.742790\n\n\n1\nJFK\n1266.249077\n\n\n2\nLGA\n779.835671\n\n\n\n\n\n\n\n\n\n#Bringing it together with pipes\n#[Step 1]: Filter out all flights less than 10 hours\n#[Step 2]: Create a new column, speed, using the formula [distance / (air time * 60)]\n#[Step 3]: Calculate the mean speed for flights originating from each airport\n#[Step 4]: Sort the result by mean speed in descending order\n(flight &gt;&gt;\n  mask(X.hour &gt; 10) &gt;&gt; # step 1\n  mutate(speed = X.distance / (X.air_time * 60)) &gt;&gt; # step 2\n  group_by(X.origin) &gt;&gt; # step 3a\n  summarize(mean_speed = X.speed.mean()) &gt;&gt; # step 3b\n  arrange(X.mean_speed, ascending=False) # step 4\n)\n\n\n\n\n\n\n\n\norigin\nmean_speed\n\n\n\n\n0\nEWR\n0.109777\n\n\n1\nJFK\n0.109427\n\n\n2\nLGA\n0.107362\n\n\n\n\n\n\n\n\nflight.loc[flight['hour'] &gt; 10, 'speed'] = flight['distance'] / (flight['air_time'] * 60)\nresult = flight.groupby('origin', as_index=False)['speed'].mean()\nresult.sort_values('speed', ascending=False)\n\n\n\n\n\n\n\n\norigin\nspeed\n\n\n\n\n0\nEWR\n0.109777\n\n\n1\nJFK\n0.109427\n\n\n2\nLGA\n0.107362\n\n\n\n\n\n\n\n\n\nBasic Stat\n\ndf = pd.read_csv(\"MELBOURNE_HOUSE_PRICES_LESS.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nSuburb\nAddress\nRooms\nType\nPrice\nMethod\nSellerG\nDate\nPostcode\nRegionname\nPropertycount\nDistance\nCouncilArea\n\n\n\n\n0\nAbbotsford\n49 Lithgow St\n3\nh\n1490000.0\nS\nJellis\n1/04/2017\n3067\nNorthern Metropolitan\n4019\n3.0\nYarra City Council\n\n\n1\nAbbotsford\n59A Turner St\n3\nh\n1220000.0\nS\nMarshall\n1/04/2017\n3067\nNorthern Metropolitan\n4019\n3.0\nYarra City Council\n\n\n2\nAbbotsford\n119B Yarra St\n3\nh\n1420000.0\nS\nNelson\n1/04/2017\n3067\nNorthern Metropolitan\n4019\n3.0\nYarra City Council\n\n\n3\nAberfeldie\n68 Vida St\n3\nh\n1515000.0\nS\nBarry\n1/04/2017\n3040\nWestern Metropolitan\n1543\n7.5\nMoonee Valley City Council\n\n\n4\nAirport West\n92 Clydesdale Rd\n2\nh\n670000.0\nS\nNelson\n1/04/2017\n3042\nWestern Metropolitan\n3464\n10.4\nMoonee Valley City Council\n\n\n\n\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 63023 entries, 0 to 63022\nData columns (total 13 columns):\n #   Column         Non-Null Count  Dtype  \n---  ------         --------------  -----  \n 0   Suburb         63023 non-null  object \n 1   Address        63023 non-null  object \n 2   Rooms          63023 non-null  int64  \n 3   Type           63023 non-null  object \n 4   Price          48433 non-null  float64\n 5   Method         63023 non-null  object \n 6   SellerG        63023 non-null  object \n 7   Date           63023 non-null  object \n 8   Postcode       63023 non-null  int64  \n 9   Regionname     63023 non-null  object \n 10  Propertycount  63023 non-null  int64  \n 11  Distance       63023 non-null  float64\n 12  CouncilArea    63023 non-null  object \ndtypes: float64(2), int64(3), object(8)\nmemory usage: 6.3+ MB\n\n\n\naverage = df['Price'].mean()\nprint(average)\n\nmed = df['Price'].median()\nprint(med)\n\nstandard_deviation = df['Price'].std()\nprint(standard_deviation)\n\n997898.2414882415\n830000.0\n593498.9190372769\n\n\n\n%matplotlib inline\n\nsns.set(style=\"whitegrid\")\nplt.figure(figsize=(10,8))\nax = sns.boxplot(y='Price', data=df, orient=\"v\")\n\nax = sns.boxplot(x='Type', y='Price', data=df, orient=\"v\")\n\nfilter_data = df.dropna(subset=['Price'])\nplt.figure(figsize=(14,8))\nsns.distplot(filter_data['Price'], kde=False)\n\ntype_counts = df['Type'].value_counts()\ndf2 = pd.DataFrame({'house_type': type_counts}, \n                     index = ['t', 'h', 'u']\n                   )\n\ndf2.plot.pie(y='house_type', figsize=(10,10), autopct='%1.1f%%')\n\nsns.set(style='darkgrid')\nplt.figure(figsize=(20,10))\nax = sns.countplot(x='Regionname', data=df)\n\n/usr/local/lib/python3.9/site-packages/seaborn/distributions.py:2557: FutureWarning:\n\n`distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCorrelation\n\ndf.corr()\npd.plotting.scatter_matrix(df, figsize=(6,6))\nplt.show()\n\n\n\n\n\n\n\n\n\n\nRegression\n\n# get data\nurl = \"http://peopleanalytics-regression-book.org/data/ugtests.csv\"\nugtests = pd.read_csv(url)\nsns.scatterplot(data=ugtests, x=\"Yr1\", y=\"Final\")\nfig = px.scatter_3d(ugtests, x='Yr3', y='Final', z='Yr1')\nfig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\n\n\n\n\n\n\nsns.histplot(data=ugtests, x = 'Yr1')\n\n\n\n\n\n\n\n\n\nugtests.corr()\n\n\n\n\n\n\n\n\nYr1\nYr2\nYr3\nFinal\n\n\n\n\nYr1\n1.000000\n0.027509\n-0.020090\n0.020484\n\n\nYr2\n0.027509\n1.000000\n0.043435\n0.321298\n\n\nYr3\n-0.020090\n0.043435\n1.000000\n0.666364\n\n\nFinal\n0.020484\n0.321298\n0.666364\n1.000000\n\n\n\n\n\n\n\n\nplt.matshow(ugtests.corr())\nplt.xticks(range(len(ugtests.columns)), ugtests.columns)\nplt.yticks(range(len(ugtests.columns)), ugtests.columns)\nplt.colorbar()\nplt.show()\n\n\n\n\n\n\n\n\n\n# define model\nmodel = smf.ols(formula = \"Final ~ Yr3 + Yr2 + Yr1\", data = ugtests)\n# fit model\nugtests_model = model.fit()\n# see results summary\nprint(ugtests_model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  Final   R-squared:                       0.530\nModel:                            OLS   Adj. R-squared:                  0.529\nMethod:                 Least Squares   F-statistic:                     365.5\nDate:                Thu, 08 Jul 2021   Prob (F-statistic):          8.22e-159\nTime:                        08:20:25   Log-Likelihood:                -4711.6\nNo. Observations:                 975   AIC:                             9431.\nDf Residuals:                     971   BIC:                             9451.\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     14.1460      5.480      2.581      0.010       3.392      24.900\nYr3            0.8657      0.029     29.710      0.000       0.809       0.923\nYr2            0.4313      0.033     13.267      0.000       0.367       0.495\nYr1            0.0760      0.065      1.163      0.245      -0.052       0.204\n==============================================================================\nOmnibus:                        0.762   Durbin-Watson:                   2.006\nProb(Omnibus):                  0.683   Jarque-Bera (JB):                0.795\nSkew:                           0.067   Prob(JB):                        0.672\nKurtosis:                       2.961   Cond. No.                         858.\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nsns.set_theme(color_codes=True)\ntips = sns.load_dataset(\"tips\")\nsns.regplot(x=\"total_bill\", y=\"tip\",  data=tips);\nsns.lmplot(x=\"total_bill\", y='tip', hue='smoker', data = tips);\nsns.lmplot(x=\"total_bill\", y='tip', col='sex', row='smoker', data = tips);\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLogistic Regression\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n# obtain salespeople data\nurl = \"http://peopleanalytics-regression-book.org/data/salespeople.csv\"\nsalespeople = pd.read_csv(url)\n\n# define model\nmodel = smf.glm(formula = \"promoted ~ sales + customer_rate\", \n                data = salespeople, \n                family = sm.families.Binomial())\n\n\n# fit model\npromotion_model = model.fit()\n\n\n# see results summary\nprint(promotion_model.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:               promoted   No. Observations:                  350\nModel:                            GLM   Df Residuals:                      347\nModel Family:                Binomial   Df Model:                            2\nLink Function:                  logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -32.566\nDate:                Thu, 08 Jul 2021   Deviance:                       65.131\nTime:                        08:20:29   Pearson chi2:                     198.\nNo. Iterations:                     9                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          z      P&gt;|z|      [0.025      0.975]\n---------------------------------------------------------------------------------\nIntercept       -19.5177      3.347     -5.831      0.000     -26.078     -12.958\nsales             0.0404      0.007      6.189      0.000       0.028       0.053\ncustomer_rate    -1.1221      0.467     -2.403      0.016      -2.037      -0.207\n=================================================================================\n\n\n\n\nTime series\n\n# Using graph_objects\nimport plotly.graph_objects as go\nimport pandas as pd\ndf = pd.read_csv('https://raw.githubusercontent.com/plotly/datasets/master/finance-charts-apple.csv')\ndf\n\n\n\n\n\n\n\n\nDate\nAAPL.Open\nAAPL.High\nAAPL.Low\nAAPL.Close\nAAPL.Volume\nAAPL.Adjusted\ndn\nmavg\nup\ndirection\n\n\n\n\n0\n2015-02-17\n127.489998\n128.880005\n126.919998\n127.830002\n63152400\n122.905254\n106.741052\n117.927667\n129.114281\nIncreasing\n\n\n1\n2015-02-18\n127.629997\n128.779999\n127.449997\n128.720001\n44891700\n123.760965\n107.842423\n118.940333\n130.038244\nIncreasing\n\n\n2\n2015-02-19\n128.479996\n129.029999\n128.330002\n128.449997\n37362400\n123.501363\n108.894245\n119.889167\n130.884089\nDecreasing\n\n\n3\n2015-02-20\n128.619995\n129.500000\n128.050003\n129.500000\n48948400\n124.510914\n109.785449\n120.763500\n131.741551\nIncreasing\n\n\n4\n2015-02-23\n130.020004\n133.000000\n129.660004\n133.000000\n70974100\n127.876074\n110.372516\n121.720167\n133.067817\nIncreasing\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n501\n2017-02-10\n132.460007\n132.940002\n132.050003\n132.119995\n20065500\n132.119995\n114.494004\n124.498666\n134.503328\nDecreasing\n\n\n502\n2017-02-13\n133.080002\n133.820007\n132.750000\n133.289993\n23035400\n133.289993\n114.820798\n125.205166\n135.589534\nIncreasing\n\n\n503\n2017-02-14\n133.470001\n135.089996\n133.250000\n135.020004\n32815500\n135.020004\n115.175718\n125.953499\n136.731280\nIncreasing\n\n\n504\n2017-02-15\n135.520004\n136.270004\n134.619995\n135.509995\n35501600\n135.509995\n115.545035\n126.723499\n137.901963\nDecreasing\n\n\n505\n2017-02-16\n135.669998\n135.899994\n134.839996\n135.350006\n22118000\n135.350006\n116.203299\n127.504333\n138.805366\nDecreasing\n\n\n\n\n506 rows × 11 columns\n\n\n\n\nfig = go.Figure([go.Scatter(x=df['Date'], y=df['AAPL.High'])])\nfig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\nimport plotly.express as px\ndf = px.data.stocks()\nfig = px.line(df, x=\"date\", y=df.columns,\n              hover_data={\"date\": \"|%B %d, %Y\"},\n              title='custom tick labels')\nfig.update_xaxes(\n    dtick=\"M1\",\n    tickformat=\"%b\\n%Y\")\nfig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json"
  },
  {
    "objectID": "Course/MGS3001/07_Script/python-analytics/barchart_race/barchart1.html",
    "href": "Course/MGS3001/07_Script/python-analytics/barchart_race/barchart1.html",
    "title": "Chad's Course Portfolio",
    "section": "",
    "text": "import bar_chart_race as bcr\ndf = bcr.load_dataset('covid19_tutorial')\nbcr.bar_chart_race(\n    df=df,\n    filename='covid19_horiz.mp4',\n    orientation='h',\n    sort='desc',\n    n_bars=6,\n    fixed_order=False,\n    fixed_max=True,\n    steps_per_period=10,\n    interpolate_period=False,\n    label_bars=True,\n    bar_size=.95,\n    period_label={'x': .99, 'y': .25, 'ha': 'right', 'va': 'center'},\n    period_fmt='%B %d, %Y',\n    period_summary_func=lambda v, r: {'x': .99, 'y': .18,\n                                      's': f'Total deaths: {v.nlargest(6).sum():,.0f}',\n                                      'ha': 'right', 'size': 8, 'family': 'Courier New'},\n    perpendicular_bar_func='median',\n    period_length=500,\n    figsize=(5, 3),\n    dpi=144,\n    cmap='dark12',\n    title='COVID-19 Deaths by Country',\n    title_size='',\n    bar_label_size=7,\n    tick_label_size=7,\n    shared_fontdict={'family' : 'Helvetica', 'color' : '.1'},\n    scale='linear',\n    writer=None,\n    fig=None,\n    bar_kwargs={'alpha': .7},\n    filter_column_colors=False)  \n\n/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/bar_chart_race/_make_chart.py:286: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax.set_yticklabels(self.df_values.columns)\n/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/bar_chart_race/_make_chart.py:287: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax.set_xticklabels([max_val] * len(ax.get_xticks()))\n\n\n\nbcr.bar_chart_race(df=df, filename=None)\n\n/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/bar_chart_race/_make_chart.py:286: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax.set_yticklabels(self.df_values.columns)\n/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/bar_chart_race/_make_chart.py:287: UserWarning: FixedFormatter should only be used together with FixedLocator\n  ax.set_xticklabels([max_val] * len(ax.get_xticks()))\n\n\n\n  \n  Your browser does not support the video tag."
  },
  {
    "objectID": "Course/MGS3001/07_Script/python-analytics/code_in_class_visualization.html",
    "href": "Course/MGS3001/07_Script/python-analytics/code_in_class_visualization.html",
    "title": "Box plot",
    "section": "",
    "text": "## Histogram\n\n\nimport seaborn as sns\ndf = sns.load_dataset('iris')\ndf.head()\n\n\n  \n    \n      \n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa\n\n\n\n\n\n      \n        \n  \n    \n    \n  \n      \n      \n  \n\n      \n    \n  \n  \n\n\n\nsns.distplot(a=df['sepal_length'], hist=True, kde = False, rug = False)\n\n/usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n  warnings.warn(msg, FutureWarning)\n\n\n\n\n\n\n\n\n\n\nsns.kdeplot(df['sepal_length'])\n\n\n\n\n\n\n\n\n\nsns.distplot(a=df['sepal_length'], hist=True, kde = False, rug = False)\nsns.kdeplot(df['sepal_length'])\n\n/usr/local/lib/python3.7/dist-packages/seaborn/distributions.py:2619: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n  warnings.warn(msg, FutureWarning)\n\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nhours = [17,20,22,25,26,35,35,35,36,32,31,26,40,25,45,55]\nplt.hist(hours, bins = 5, edgecolor = 'black');\n\n\n\n\n\n\n\n\n\nsns.boxplot(x = df['species'], y = df['sepal_length'])\n\n\n\n\n\n\n\n\n\nsns.boxplot(x = df['species'], y = df['sepal_width'])\n\n\n\n\n\n\n\n\n\nsns.boxplot(x = df['species'], y = df['petal_length'])\n\n\n\n\n\n\n\n\n\nsns.boxplot(x = df['species'], y = df['petal_width'])\n\n\n\n\n\n\n\n\n\nsns.regplot(x = df['sepal_length'], y = df['sepal_width'])\n\n\n\n\n\n\n\n\n\nsns.regplot(x = df['sepal_length'], y = df['sepal_width'], line_kws = {'color':\"r\", 'lw':10, 'alpha':0.3})\n\n\n\n\n\n\n\n\n\nsns.pairplot(df)\n\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n\nheight = [2,12,5,18,45]\nbars = ('chad', 'jerry', 'lee', 'kim', 'park')\n\n\ny_pos = bars\nplt.bar(y_pos, height)\n\n\n\n\n\n\n\n\n\nsize_of_group = [11,12,4,29]\nplt.pie(size_of_group)\nplt.show()\n\n\n\n\n\n\n\n\n\nimport numpy as np\nvalues = np.cumsum(np.random.randn(1000,1))\nplt.plot(values)\n\n\n\n\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\nimport networkx as nx\nimport matplotlib.pyplot as plt\n\n\ndf = pd.DataFrame({'from':['Chad', 'Jerry', 'Lee', 'Park', 'Park', 'Lee'], 'to':['Park','Chad','Kim','Lee', 'Jerry', 'Chad']})\n\n\nG = nx.from_pandas_edgelist(df, 'from', 'to')\nnx.draw(G, with_labels = True, node_size = 1500, font_size = 25, font_color ='yellow', font_weight = 'bold')\nplt.show()\n\n\n\n\n\n\n\n\n\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\ntext = (\"Chad is Awesome\")\nwordcloud = WordCloud(width = 480, height = 480, margin =0).generate(text)\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.margins(x=0, y=0)\nplt.show()\n\n\n\n\n\n\n\n\n\n!pip install basemap\n\nLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nCollecting basemap\n  Downloading basemap-1.3.3-cp37-cp37m-manylinux1_x86_64.whl (863 kB)\n     |████████████████████████████████| 863 kB 4.3 MB/s \nRequirement already satisfied: numpy&lt;1.23,&gt;=1.21 in /usr/local/lib/python3.7/dist-packages (from basemap) (1.21.6)\nCollecting pyproj&lt;3.4.0,&gt;=1.9.3\n  Downloading pyproj-3.2.1-cp37-cp37m-manylinux2010_x86_64.whl (6.3 MB)\n     |████████████████████████████████| 6.3 MB 24.8 MB/s \nRequirement already satisfied: matplotlib&lt;3.6,&gt;=1.5 in /usr/local/lib/python3.7/dist-packages (from basemap) (3.2.2)\nCollecting basemap-data&lt;1.4,&gt;=1.3.2\n  Downloading basemap_data-1.3.2-py2.py3-none-any.whl (30.5 MB)\n     |████████████████████████████████| 30.5 MB 2.9 MB/s \nCollecting pyshp&lt;2.2,&gt;=1.2\n  Downloading pyshp-2.1.3.tar.gz (219 kB)\n     |████████████████████████████████| 219 kB 46.0 MB/s \nRequirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib&lt;3.6,&gt;=1.5-&gt;basemap) (0.11.0)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib&lt;3.6,&gt;=1.5-&gt;basemap) (1.4.3)\nRequirement already satisfied: python-dateutil&gt;=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib&lt;3.6,&gt;=1.5-&gt;basemap) (2.8.2)\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,&gt;=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib&lt;3.6,&gt;=1.5-&gt;basemap) (3.0.9)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver&gt;=1.0.1-&gt;matplotlib&lt;3.6,&gt;=1.5-&gt;basemap) (4.1.1)\nRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from pyproj&lt;3.4.0,&gt;=1.9.3-&gt;basemap) (2022.6.15)\nRequirement already satisfied: six&gt;=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil&gt;=2.1-&gt;matplotlib&lt;3.6,&gt;=1.5-&gt;basemap) (1.15.0)\nBuilding wheels for collected packages: pyshp\n  Building wheel for pyshp (setup.py) ... done\n  Created wheel for pyshp: filename=pyshp-2.1.3-py3-none-any.whl size=37324 sha256=87ecc98924c5fd67bdb9c67675d1690a6292ba4e3c7cab49a8fa3981b7ed4a67\n  Stored in directory: /root/.cache/pip/wheels/43/f8/87/53c8cd41545ba20e536ea29a8fcb5431b5f477ca50d5dffbbe\nSuccessfully built pyshp\nInstalling collected packages: pyshp, pyproj, basemap-data, basemap\nSuccessfully installed basemap-1.3.3 basemap-data-1.3.2 pyproj-3.2.1 pyshp-2.1.3\n\n\nUnable to display output for mime type(s): application/vnd.colab-display-data+json\n\n\n\n# Libraries\nimport pandas as pd\n\n# read the data (on the web)\ndata = pd.read_csv('https://raw.githubusercontent.com/holtzy/The-Python-Graph-Gallery/master/static/data/TweetSurfData.csv', sep=\";\")\n\n# Check the first 2 rows\ndata.head(2)\n\n# Basemap library\nfrom mpl_toolkits.basemap import Basemap\nimport matplotlib.pyplot as plt\n \n# Set the dimension of the figure\nplt.rcParams[\"figure.figsize\"]=15,10;\n\n# Make the background map\nm=Basemap(llcrnrlon=-180, llcrnrlat=-65, urcrnrlon=180, urcrnrlat=80, projection='merc');\nm.drawmapboundary(fill_color='#A6CAE0', linewidth=0);\nm.fillcontinents(color='grey', alpha=0.3);\nm.drawcoastlines(linewidth=0.1, color=\"white\");\n\n# Make the background map\nm=Basemap(llcrnrlon=-180, llcrnrlat=-65, urcrnrlon=180, urcrnrlat=80)\nm.drawmapboundary(fill_color='#A6CAE0', linewidth=0)\nm.fillcontinents(color='grey', alpha=0.3)\nm.drawcoastlines(linewidth=0.1, color=\"white\")\n\n# prepare a color for each point depending on the continent.\ndata['labels_enc'] = pd.factorize(data['homecontinent'])[0]\n \n# Add a point per position\nm.scatter(\n    x=data['homelon'], \n    y=data['homelat'], \n    s=data['n']/6, \n    alpha=0.4, \n    c=data['labels_enc'], \n    cmap=\"Set1\"\n)\n \n# copyright and source data info\nplt.text( -175, -62,'Where people talk about #Surf\\n\\nData collected on twitter by @R_Graph_Gallery during 300 days\\nPlot realized with Python and the Basemap library', ha='left', va='bottom', size=9, color='#555555' );"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter14/chapter14.html#short-video-introduction",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter14/chapter14.html#short-video-introduction",
    "title": "Database Programming",
    "section": "SHORT VIDEO INTRODUCTION",
    "text": "SHORT VIDEO INTRODUCTION"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter14/chapter14.html#section",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter14/chapter14.html#section",
    "title": "Database Programming",
    "section": "#",
    "text": "#"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter14/chapter14.html#section-1",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter14/chapter14.html#section-1",
    "title": "Database Programming",
    "section": "##",
    "text": "##"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter14/chapter14.html#assignment",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter14/chapter14.html#assignment",
    "title": "Database Programming",
    "section": "Assignment",
    "text": "Assignment\n\n\n\nWhat to do\n\n\n\nRequirement\n\nPDF format\nfile name should be include your student id and name\n\nstuID_name_title.pdf (e.g. 1111111_ChungilChae_SelfIntroduction.pdf)\n\n\nDue date\n\nby Feb23(Sun) 11:59PM\nNO LATE SUBMISSION ALLOWED!!!!"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter4/chapter4.html#short-video-introduction",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter4/chapter4.html#short-video-introduction",
    "title": "Repetition Structures",
    "section": "SHORT VIDEO INTRODUCTION",
    "text": "SHORT VIDEO INTRODUCTION"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter4/chapter4.html#section",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter4/chapter4.html#section",
    "title": "Repetition Structures",
    "section": "#",
    "text": "#"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter4/chapter4.html#section-1",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter4/chapter4.html#section-1",
    "title": "Repetition Structures",
    "section": "##",
    "text": "##"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter4/chapter4.html#assignment",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter4/chapter4.html#assignment",
    "title": "Repetition Structures",
    "section": "Assignment",
    "text": "Assignment\n\n\n\nWhat to do\n\n\n\nRequirement\n\nPDF format\nfile name should be include your student id and name\n\nstuID_name_title.pdf (e.g. 1111111_ChungilChae_SelfIntroduction.pdf)\n\n\nDue date\n\nby Feb23(Sun) 11:59PM\nNO LATE SUBMISSION ALLOWED!!!!"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#short-video-introduction",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#short-video-introduction",
    "title": "Input, Processing, and Output",
    "section": "SHORT VIDEO INTRODUCTION",
    "text": "SHORT VIDEO INTRODUCTION"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#the-program-development-cycle",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#the-program-development-cycle",
    "title": "Input, Processing, and Output",
    "section": "The Program Development Cycle",
    "text": "The Program Development Cycle\nThe process of creating a program that works correctly typically requires the five phases known as the program development cycle.\n\nDesign the Program\nWrite the Code\nCorrect Syntax Errors\nTest the Program\nCorrect Logic Errors\n\n\n\n\n\n\n\n\nDesign the Program. All professional programmers will tell you that a program should be carefully designed before the code is actually written. When programmers begin a new project, they should never jump right in and start writing code as the first step. They start by creating a design of the program. There are several ways to design a program, and later in this section, we will discuss some techniques that you can use to design your Python programs.\nWrite the Code. After designing the program, the programmer begins writing code in a high-level language such as Python. Recall from Chapter 1 that each language has its own rules, known as syntax, that must be followed when writing a program. A language’s syntax rules dictate things such as how keywords, operators, and punctuation characters can be used. A syntax error occurs if the programmer violates any of these rules.\nCorrect Syntax Errors. If the program contains a syntax error, or even a simple mistake such as a misspelled keyword, the compiler or interpreter will display an error message indicating what the error is. Virtually all code contains syntax errors when it is first written, so the programmer will typically spend some time correcting these. Once all of the syntax errors and simple typing mistakes have been corrected, the program can be compiled and translated into a machine language program (or executed by an interpreter, depending on the language being used).\nTest the Program. Once the code is in an executable form, it is then tested to determine whether any logic errors exist. A logic error is a mistake that does not prevent the program from running, but causes it to produce incorrect results. (Mathematical mistakes are common causes of logic errors.)\nCorrect Logic Errors. If the program produces incorrect results, the programmer debugs the code. This means that the programmer finds and corrects logic errors in the program. Sometimes during this process, the programmer discovers that the program’s original design must be changed. In this event, the program development cycle starts over and continues until no errors can be found."
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#pseudocode",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#pseudocode",
    "title": "Input, Processing, and Output",
    "section": "Pseudocode",
    "text": "Pseudocode\n\nPseudocode: fake code read more\n\nInformal language that has no syntax rule\nNot meant to be compiled or executed\nUsed to create model program\n\nNo need to worry about syntax errors, can focus on program’s design\nCan be translated directly into actual code in any programming language."
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#flowchart",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#flowchart",
    "title": "Input, Processing, and Output",
    "section": "Flowchart",
    "text": "Flowchart\n\nFlowchart: diagram that graphically depicts the steps in a program read more\n\nOvals are terminal symbols\nParallelograms are input and output symbols\nRectangles are processing symbols\nSymbols are connected by arrows that represent the flow of the program"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#ipo",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#ipo",
    "title": "Input, Processing, and Output",
    "section": "IPO",
    "text": "IPO\n\nTypically, computer performs three-step process\n\nReceive input\n\nInput: any data that the program receives while it is running\n\nPerform some process on the input\n\nExample: mathematical calculation\n\nProduce output"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#related-concept",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#related-concept",
    "title": "Input, Processing, and Output",
    "section": "Related Concept",
    "text": "Related Concept\n\nFunction: piece of prewritten code that performs an operation\nprint function: displays output on the screen\nArgument: data given to a function\n\nExample: data that is printed to screen\n\nStatements in a program execute in the order that they appear\nFrom top to bottom"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#strings-and-string-literals",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#strings-and-string-literals",
    "title": "Input, Processing, and Output",
    "section": "Strings and String Literals",
    "text": "Strings and String Literals\n\nString: sequence of characters that is used as data\nString literal: string that appears in actual code of a program\n\nMust be enclosed in single (’) or double (“) quote marks\nString literal can be enclosed in triple quotes (’’’ or “““)\n\nEnclosed string can contain both single and double quotes and can have multiple lines"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#comments",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#comments",
    "title": "Input, Processing, and Output",
    "section": "Comments",
    "text": "Comments\n\nComments: notes of explanation within a program\n\nIgnored by Python interpreter\n\nIntended for a person reading the program’s code\n\nBegin with a # character\n\nEnd-line comment: appears at the end of a line of code\n\nTypically explains the purpose of that line"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#overview",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#overview",
    "title": "Input, Processing, and Output",
    "section": "Overview",
    "text": "Overview\n\n\nVariable: name that represents a value stored in the computer memory\n\nUsed to access and manipulate data stored in memory\nA variable references the value it represents\n\nAssignment statement: used to create a variable and make it reference data\n\nGeneral format is variable = expression\n\nExample: age = 29\nAssignment operator: the equal sign (=)\n\n\n\n\n\nname = \"chad\"\nage = 20\nprint(age, name)\n\n20 chad"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#variable-naming-rules",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#variable-naming-rules",
    "title": "Input, Processing, and Output",
    "section": "Variable Naming Rules",
    "text": "Variable Naming Rules\n\nRules for naming variables in Python:\n\nVariable name cannot be a Python keyword\nVariable name cannot contain spaces\nFirst character must be a letter or an underscore\nAfter first character may use letters, digits, or underscores\nVariable names are case sensitive\nVariable name should reflect its use"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#displaying-multiple-items-with-the-print-function",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#displaying-multiple-items-with-the-print-function",
    "title": "Input, Processing, and Output",
    "section": "Displaying Multiple Items with the print Function",
    "text": "Displaying Multiple Items with the print Function\n\nPython allows one to display multiple items with a single call to print\n\nItems are separated by commas when passed as arguments\nArguments displayed in the order they are passed to the function\nItems are automatically separated by a space when displayed on screen\n\n\n\nname = \"chad\"\nage = 20\nprint(\"Hello\", name+\".\", \"I heard that you got\", str(age)+\"th\", \"birthday\")\n\nHello chad. I heard that you got 20th birthday"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#variable-reassignment",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#variable-reassignment",
    "title": "Input, Processing, and Output",
    "section": "Variable Reassignment",
    "text": "Variable Reassignment\n\n\nVariables can reference different values while program is running\nGarbage collection: removal of values that are no longer referenced by variables\n\nCarried out by Python interpreter\n\nA variable can refer to item of any type\n\nVariable that has been assigned to one type can be reassigned to another type\n\n\n\n\nname = \"chad\"\nprint(name)\n\nname = \"Joe\"\nprint(name)\n\nname = 20\nprint(name)\n\nchad\nJoe\n20"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#numeric-data-types-literals-and-the-str-data-type",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#numeric-data-types-literals-and-the-str-data-type",
    "title": "Input, Processing, and Output",
    "section": "Numeric Data Types, Literals, and the str Data Type",
    "text": "Numeric Data Types, Literals, and the str Data Type\n\n\nData types: categorize value in memory\n\ne.g., int for integer, float for real number, str used for storing strings in memory\n\nNumeric literal: number written in a program\n\nNo decimal point considered int, otherwise, considered float\n\nSome operations behave differently depending on data type\n\n\n\nnum1 = 20\nnum2 = 20.54\nnum3 = \"20.68\"\nprint(num1)\nprint(num2)\nprint(num3)\n\n\n20\n20.54\n20.68"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#reassigning-a-variable-to-a-different-type",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#reassigning-a-variable-to-a-different-type",
    "title": "Input, Processing, and Output",
    "section": "Reassigning a Variable to a Different Type",
    "text": "Reassigning a Variable to a Different Type\n\nA variable in Python can refer to items of any type"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#overview-1",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#overview-1",
    "title": "Input, Processing, and Output",
    "section": "Overview",
    "text": "Overview\nMost programs need to read input from the user\n\nBuilt-in inputfunction reads input from keyboard\n\nReturns the data as a string\nFormat: variable = input(prompt)\n\nprompt is typically a string instructing user to enter a value\n\nDoes not automatically display a space after the prompt"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#overview-2",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#overview-2",
    "title": "Input, Processing, and Output",
    "section": "Overview",
    "text": "Overview\n\n\nMath expression: performs calculation and gives a value\n\nMath operator: tool for performing calculation\nOperands: values surrounding operator\nVariables can be used as operands, resulting value typically assigned to variable\n\nTwo types of division:\n\n/ operator performs floating point division, // operator performs integer division\nPositive results truncated, negative rounded away from zero\n\n\n\n\nprint(1+2)\nprint(58-782)\nprint(35*72)\nprint(4/2)\nprint(4//2)\nprint(5//2)\nprint(((1+2) * (58-782)) / 35*72)\n\n\n3\n-724\n2520\n2.0\n2\n2\n-4468.114285714286"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#operator-precedence-and-grouping-with-parentheses",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#operator-precedence-and-grouping-with-parentheses",
    "title": "Input, Processing, and Output",
    "section": "Operator Precedence and Grouping with Parentheses",
    "text": "Operator Precedence and Grouping with Parentheses\nPython operator precedence:\n\nOperations enclosed in parentheses\n\nForces operations to be performed before others\n\nExponentiation (**)\nMultiplication (*), division (/ and //), and remainder (%)\nAddition (+) and subtraction (-)\n\nHigher precedence performed first\nSame precedence operators execute from left to right"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#the-exponent-operator-and-the",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#the-exponent-operator-and-the",
    "title": "Input, Processing, and Output",
    "section": "The Exponent Operator and the",
    "text": "The Exponent Operator and the\n\nRemainder Operator:\n\nExponent operator (**): Raises a number to a power\n\nx ** y = xy\n\nRemainder operator (%): Performs division and returns the remainder\n\na.k.a. modulus operator\ne.g., 4%2=0, 5%2=1\nTypically used to convert times and distances, and to detect odd or even numbers\n\n\n\n\nprint(4**2)\nprint(4^2)\nprint(4%2)\nprint(5%2)\n\n\n16\n6\n0\n1"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#converting-math-formulas-to-programming-statements",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#converting-math-formulas-to-programming-statements",
    "title": "Input, Processing, and Output",
    "section": "Converting Math Formulas to Programming Statements",
    "text": "Converting Math Formulas to Programming Statements\n\nOperator required for any mathematical operation\nWhen converting mathematical expression to programming statement:\n\nMay need to add multiplication operators\nMay need to insert parentheses"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#mixed-type-expressions-and-data-type-conversion",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#mixed-type-expressions-and-data-type-conversion",
    "title": "Input, Processing, and Output",
    "section": "Mixed-Type Expressions and Data Type Conversion",
    "text": "Mixed-Type Expressions and Data Type Conversion\n\n\nData type resulting from math operation depends on data types of operands\n\nTwo intvalues: result is an int\nTwo floatvalues: result is a float\nint and float: int temporarily converted to float, result of the operation is a float\nMixed-type expression\n\nType conversion of float to int causes truncation of fractional part\n\n\n\n\n\nprint(10 + 11)\nprint(10.1 + 11.3)\nprint(10.1 + 11)\nprint(float(10)+11)\nprint(int(12.1235845865))\n\n\n21\n21.4\n21.1\n21.0\n12"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#breaking-long-statements-into-multiple-lines",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#breaking-long-statements-into-multiple-lines",
    "title": "Input, Processing, and Output",
    "section": "Breaking Long Statements into Multiple Lines",
    "text": "Breaking Long Statements into Multiple Lines\n\nLong statements cannot be viewed on screen without scrolling and cannot be printed without cutting off\nMultiline continuation character (\\): Allows to break a statement into multiple lines\n\n\nresult = var1 * 2 + var2 * 3 + \\\nvar3 * 4 + var4 * 5"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#syntex",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#syntex",
    "title": "Input, Processing, and Output",
    "section": "Syntex",
    "text": "Syntex\n\nTo append one string to the end of another string\nUse the + operator to concatenate strings\n\n\n\nmessage = 'Hello' + 'world'\nprint(message)\n\n\nHelloworld\n\n\n\nYou can use string concatenation to break up a long string literal\n\n\n\nprint('Enter the amount of ' + 'sales for each day and ' + 'press Enter.')\n\n\nEnter the amount of sales for each day and press Enter."
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#implicit-string-literal-concatenation",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#implicit-string-literal-concatenation",
    "title": "Input, Processing, and Output",
    "section": "Implicit String Literal Concatenation",
    "text": "Implicit String Literal Concatenation\n\nTwo or more string literals written adjacent to each other are implicitly concatenated into a single string\n\n\n\nmy_str = 'one' 'two' 'three'\nprint(my_str)\n\n\nonetwothree\n\n\n\n\nprint('Enter the amount of ' 'sales for each day and ' 'press Enter.')\n\n\nEnter the amount of sales for each day and press Enter."
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#overview-3",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#overview-3",
    "title": "Input, Processing, and Output",
    "section": "Overview",
    "text": "Overview\n\n\nprint function displays line of output\n\nNewline character at end of printed data\nSpecial argument end=’delimiter’causes print to place delimiter at end of data instead of newline character\n\nprintfunction uses space as item separator\n\nSpecial argument sep=’delimiter’causes print to use delimiter as item separator Special characters appearing in string literal\nPreceded by backslash (\\)\n\nExamples: newline (\\n), horizontal tab (\\t)\n\nTreated as commands embedded in string"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#overview-4",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#overview-4",
    "title": "Input, Processing, and Output",
    "section": "Overview",
    "text": "Overview\n\nBasicPlaceholders\n\n\n\nAn f-string is a special type of string literal that is prefixed with the letter f\n\n\n\nprint(f'Hello world')\n\n\nHello world\n\n\n\n\n\nF-strings support placeholders for variables\n\n\n\nname = 'Johnny'\nprint(f'Hello {name}.')\n\n\nHello Johnny."
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#magic-numbers",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#magic-numbers",
    "title": "Input, Processing, and Output",
    "section": "Magic Numbers",
    "text": "Magic Numbers\n\nA magic number is an unexplained numeric value that appears in a program’s code.\nExample:\n\n\namount = balance * 0.069\n\n\nWhat is the value 0.069? An interest rate? A fee percentage? Only the person who wrote the code knows for sure."
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#the-problem-with-magic-numbers",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#the-problem-with-magic-numbers",
    "title": "Input, Processing, and Output",
    "section": "The Problem with Magic Numbers",
    "text": "The Problem with Magic Numbers\n\nIt can be difficult to determine the purpose of the number.\nIf the magic number is used in multiple places in the program, it can take a lot of effort to change the number in each location, should the need arise.\nYou take the risk of making a mistake each time you type the magic number in the program’s code.\n\nFor example, suppose you intend to type 0.069, but you accidentally type .0069. This mistake will cause mathematical errors that can be difficult to find."
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#named-constants-1",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#named-constants-1",
    "title": "Input, Processing, and Output",
    "section": "Named Constants",
    "text": "Named Constants\n\nYou should use named constants instead of magic numbers.\nA named constant is a name that represents a value that does not change during the program’s execution.\nExample:\n\n\nINTEREST_RATE = 0.069\n\n\nThis creates a named constant named INTEREST_RATE, assigned the value 0.069. It can be used instead of the magic number:\n\n\namount = balance * INTEREST_RATE"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#advantages-of-using-named-constants",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#advantages-of-using-named-constants",
    "title": "Input, Processing, and Output",
    "section": "Advantages of Using Named Constants",
    "text": "Advantages of Using Named Constants\n\nNamed constants make code self-explanatory (self-documenting)\nNamed constants make code easier to maintain (change the value assigned to the constant, and the new value takes effect everywhere the constant is used)\nNamed constants help prevent typographical errors that are common when using magic numbers"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#overview-5",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#overview-5",
    "title": "Input, Processing, and Output",
    "section": "Overview",
    "text": "Overview\n\nPython’s turtle graphics system displays a small cursor known as a turtle.\nYou can use Python statements to move the turtle around the screen, drawing lines and shapes.\nTo use the turtle graphics system, you must import the turtle module with this statement:\n\n\nimport turtle\n\nThis loads the turtle module into memory"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#moving-the-turtle-forward",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#moving-the-turtle-forward",
    "title": "Input, Processing, and Output",
    "section": "Moving the Turtle Forward",
    "text": "Moving the Turtle Forward\n\nUse the turtle.forward(n)statement to move the turtle forward n pixels.\n\n\nimport turtle\nturtle.forward(100)"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#turning-the-turtle",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#turning-the-turtle",
    "title": "Input, Processing, and Output",
    "section": "Turning the Turtle",
    "text": "Turning the Turtle\n\nThe turtle’s initial heading is 0 degrees (east)\nUse the turtle.right(angle)statement to turn the turtle right by angle degrees.\nUse the turtle.left(angle)statement to turn the turtle left by angle degrees.\n\n\nimport turtle\nturtle.forward(100)\nturtle.left(90)\nturtle.forward(100)\n\n\nimport turtle\nturtle.forward(100)\nturtle.right(45)\nturtle.forward(100)"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#setting-the-turtles-heading",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#setting-the-turtles-heading",
    "title": "Input, Processing, and Output",
    "section": "Setting the Turtle’s Heading",
    "text": "Setting the Turtle’s Heading\n\nUse the turtle.setheading(angle)statement toset the turtle’s heading to a specific angle.\n\n\nimport turtle\nturtle.forward(50)\nturtle.setheading(90)\nturtle.forward(100)\nturtle.setheading(180)\nturtle.forward(50)\nturtle.setheading(270)\nturtle.forward(100)"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#setting-the-pen-up-or-down",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#setting-the-pen-up-or-down",
    "title": "Input, Processing, and Output",
    "section": "Setting the Pen Up or Down",
    "text": "Setting the Pen Up or Down\n\nWhen the turtle’s pen is down, the turtle draws a line as it moves. By default, the pen is down.\nWhen the turtle’s pen is up, the turtle does not draw as it moves.\nUse the turtle.penup()statement to raise the pen.\nUse the turtle.pendown()statement to lower the pen.\n\n\nimport turtle\nturtle.forward(50)\nturtle.penup()\nturtle.forward(25)\nturtle.pendown()\nturtle.forward(50)\nturtle.penup()\nturtle.forward(25)\nturtle.pendown()\nturtle.forward(50)"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#drawing-circles",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#drawing-circles",
    "title": "Input, Processing, and Output",
    "section": "Drawing Circles",
    "text": "Drawing Circles\n\nUse the turtle.circle(radius)statement to draw a circle with a specified radius.\n\n\nimport turtle\nturtle.circle(100)"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#drawing-dots",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#drawing-dots",
    "title": "Input, Processing, and Output",
    "section": "Drawing Dots",
    "text": "Drawing Dots\n\nUse the turtle.dot()statement to draw a simple dot at the turtle’s current location.\n\n\nimport turtle\nturtle.dot()\nturtle.forward(50)\nturtle.dot()\nturtle.forward(50)\nturtle.dot()\nturtle.forward(50)"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#changing-the-pen-size-and-drawing-color",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#changing-the-pen-size-and-drawing-color",
    "title": "Input, Processing, and Output",
    "section": "Changing the Pen Size and Drawing Color",
    "text": "Changing the Pen Size and Drawing Color\n\nUse the turtle.pensize(width)statement to change the width of the turtle’s pen, in pixels.\nUse the turtle.pencolor(color)statement to change the turtle’s drawing color.\n\nSee Appendix D in your textbook for a complete list of colors.\n\n\n\nimport turtle\nturtle.pensize(5)\nturtle.pencolor('red')\nturtle.circle(100)"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#working-with-the-turtles-window",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#working-with-the-turtles-window",
    "title": "Input, Processing, and Output",
    "section": "Working with the Turtle’s Window",
    "text": "Working with the Turtle’s Window\n\nUse the turtle.bgcolor(color)statement to set the window’s background color.\n\nSee Appendix D in your textbook for a complete list of colors.\n\nUse the turtle.setup(width,height)statement to set the size of the turtle’s window, in pixels.\n\nThe width and height arguments are the width and height, in pixels.\nFor example, the following interactive session creates a graphics window that is 640 pixels wide and 480 pixels high:\n\n\n\nimport turtle\nturtle.setup(640, 480)"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#resetting-the-turtles-window",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#resetting-the-turtles-window",
    "title": "Input, Processing, and Output",
    "section": "Resetting the Turtle’s Window",
    "text": "Resetting the Turtle’s Window\n\nThe turtle.reset()statement:\n\nErases all drawings that currently appear in the graphics window.\nResets the drawing color to black.\nResets the turtle to its original position in the center of the screen.\nDoes not reset the graphics window’s background color.\n\nThe turtle.clear()statement:\n\nErases all drawings that currently appear in the graphics window.\nDoes not change the turtle’s position.\nDoes not change the drawing color.\nDoes not change the graphics window’s background color."
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#working-with-coordinates",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#working-with-coordinates",
    "title": "Input, Processing, and Output",
    "section": "Working with Coordinates",
    "text": "Working with Coordinates\n\nThe turtle uses Cartesian Coordinates"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#moving-the-turtle-to-a-specific-location",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#moving-the-turtle-to-a-specific-location",
    "title": "Input, Processing, and Output",
    "section": "Moving the Turtle to a Specific Location",
    "text": "Moving the Turtle to a Specific Location\n\nUse the turtle.goto(x, y)statement to move the turtle to a specific location.\n\n\nimport turtle\nturtle.goto(0, 100)\nturtle.goto(−100, 0)\nturtle.goto(0, 0)\n\n\nThe turtle.pos() statement displays the turtle’s current X,Y coordinates.\nThe turtle.xcor() statement displays the turtle’s current X coordinate and the turtle.ycor() statement displays the turtle’s current Y coordinate."
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#animation-speed",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#animation-speed",
    "title": "Input, Processing, and Output",
    "section": "Animation Speed",
    "text": "Animation Speed\n\nUse the turtle.speed(speed)command to change the speed at which the turtle moves.\n\nThe speed argument is a number in the range of 0 through 10.\nIf you specify 0, then the turtle will make all of its moves instantly (animation is disabled)."
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#hiding-and-displaying-the-turtle",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#hiding-and-displaying-the-turtle",
    "title": "Input, Processing, and Output",
    "section": "Hiding and Displaying the Turtle",
    "text": "Hiding and Displaying the Turtle\n\nUse the turtle.hideturtle()command to hide the turtle.\n\nThis command does not change the way graphics are drawn, it simply hides the turtle icon.\n\nUse the turtle.showturtle()command to display the turtle."
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#displaying-text",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#displaying-text",
    "title": "Input, Processing, and Output",
    "section": "Displaying Text",
    "text": "Displaying Text\n\nUse the turtle.write(text)statement to display text in the turtle’s graphics window.\n\nThe text argument is a string that you want to display.\nThe lower-left corner of the first character will be positioned at the turtle’s X and Y coordinates.\n\n\n\nimport turtle\nturtle.write('Hello World')"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#filling-shapes",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#filling-shapes",
    "title": "Input, Processing, and Output",
    "section": "Filling Shapes",
    "text": "Filling Shapes\n\nTo fill a shape with a color:\n\nUse the turtle.begin_fill()command before drawing the shape\nThen use the turtle.end_fill()command after the shape is drawn.\nWhen the turtle.end_fill()command executes, the shape will be filled with the current fill color\n\n\n\nimport turtle\nturtle.hideturtle()\nturtle.fillcolor('red')\nturtle.begin_fill()\nturtle.circle(100)\nturtle.end_fill()"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#getting-input-with-a-dialog-box",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#getting-input-with-a-dialog-box",
    "title": "Input, Processing, and Output",
    "section": "Getting Input With a Dialog Box",
    "text": "Getting Input With a Dialog Box\n\nimport turtle\nage = turtle.numinput('Input', 'Enter your age')\n\n\nimport turtle\nname = turtle.textinput('Input', 'Enter your name')\n\n\nSpecifying a default value, minimum value, and maximum value with turtle.numinput:\n\n\nimport turtle\nnum = turtle.numinput('Input', 'Enter a number', default=10, minval=0, maxval=100)\n\n\nAn error message will be displayed if the input is less than minvalor greater than maxval"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#keeping-the-graphics-window-open",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#keeping-the-graphics-window-open",
    "title": "Input, Processing, and Output",
    "section": "Keeping the Graphics Window Open",
    "text": "Keeping the Graphics Window Open\n\nWhen running a turtle graphics program outside IDLE, the graphics window closes immediately when the program is done.\nTo prevent this, add the turtle.done()statementto the very end of your turtle graphics programs.\n\nThis will cause the graphics window to remain open, so you can see its contents after the program finishes executing."
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#assignment",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#assignment",
    "title": "Input, Processing, and Output",
    "section": "Assignment",
    "text": "Assignment\n\n\n\nWhat to do\n\n\n\nRequirement\n\nPDF format\nfile name should be include your student id and name\n\nstuID_name_title.pdf (e.g. 1111111_ChungilChae_SelfIntroduction.pdf)\n\n\nDue date\n\nby Feb23(Sun) 11:59PM\nNO LATE SUBMISSION ALLOWED!!!!"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter11/chapter11.html#short-video-introduction",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter11/chapter11.html#short-video-introduction",
    "title": "Inheritance",
    "section": "SHORT VIDEO INTRODUCTION",
    "text": "SHORT VIDEO INTRODUCTION"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter11/chapter11.html#section",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter11/chapter11.html#section",
    "title": "Inheritance",
    "section": "#",
    "text": "#"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter11/chapter11.html#section-1",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter11/chapter11.html#section-1",
    "title": "Inheritance",
    "section": "##",
    "text": "##"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter11/chapter11.html#assignment",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter11/chapter11.html#assignment",
    "title": "Inheritance",
    "section": "Assignment",
    "text": "Assignment\n\n\n\nWhat to do\n\n\n\nRequirement\n\nPDF format\nfile name should be include your student id and name\n\nstuID_name_title.pdf (e.g. 1111111_ChungilChae_SelfIntroduction.pdf)\n\n\nDue date\n\nby Feb23(Sun) 11:59PM\nNO LATE SUBMISSION ALLOWED!!!!"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter9/chapter9.html#short-video-introduction",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter9/chapter9.html#short-video-introduction",
    "title": "Dictionaries and Sets",
    "section": "SHORT VIDEO INTRODUCTION",
    "text": "SHORT VIDEO INTRODUCTION"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter9/chapter9.html#section",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter9/chapter9.html#section",
    "title": "Dictionaries and Sets",
    "section": "#",
    "text": "#"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter9/chapter9.html#section-1",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter9/chapter9.html#section-1",
    "title": "Dictionaries and Sets",
    "section": "##",
    "text": "##"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter9/chapter9.html#assignment",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter9/chapter9.html#assignment",
    "title": "Dictionaries and Sets",
    "section": "Assignment",
    "text": "Assignment\n\n\n\nWhat to do\n\n\n\nRequirement\n\nPDF format\nfile name should be include your student id and name\n\nstuID_name_title.pdf (e.g. 1111111_ChungilChae_SelfIntroduction.pdf)\n\n\nDue date\n\nby Feb23(Sun) 11:59PM\nNO LATE SUBMISSION ALLOWED!!!!"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter7/chapter7.html#short-video-introduction",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter7/chapter7.html#short-video-introduction",
    "title": "Lists and Tuples",
    "section": "SHORT VIDEO INTRODUCTION",
    "text": "SHORT VIDEO INTRODUCTION"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter7/chapter7.html#section",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter7/chapter7.html#section",
    "title": "Lists and Tuples",
    "section": "#",
    "text": "#"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter7/chapter7.html#section-1",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter7/chapter7.html#section-1",
    "title": "Lists and Tuples",
    "section": "##",
    "text": "##"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter7/chapter7.html#assignment",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter7/chapter7.html#assignment",
    "title": "Lists and Tuples",
    "section": "Assignment",
    "text": "Assignment\n\n\n\nWhat to do\n\n\n\nRequirement\n\nPDF format\nfile name should be include your student id and name\n\nstuID_name_title.pdf (e.g. 1111111_ChungilChae_SelfIntroduction.pdf)\n\n\nDue date\n\nby Feb23(Sun) 11:59PM\nNO LATE SUBMISSION ALLOWED!!!!"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#short-video-introduction",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#short-video-introduction",
    "title": "Introduction to Computers and Programming",
    "section": "SHORT VIDEO INTRODUCTION",
    "text": "SHORT VIDEO INTRODUCTION"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#program-and-programmer",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#program-and-programmer",
    "title": "Introduction to Computers and Programming",
    "section": "Program and Programmer",
    "text": "Program and Programmer\n\nProgram: set of instructions that a computer follows to perform a task\n\nCommonly referred to as Software\n\nProgrammer: person who can design, create, and test computer programs\n\nAlso known as software developer"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#central-processing-unit",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#central-processing-unit",
    "title": "Introduction to Computers and Programming",
    "section": "Central processing unit",
    "text": "Central processing unit\n\nCentral processing unit (CPU): the part of the computer that actually runs programs\n\nMost important component\nWithout it, cannot run software\nUsed to be a huge device\n\nMicroprocessors: CPUs located on small chips"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#main-memory",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#main-memory",
    "title": "Introduction to Computers and Programming",
    "section": "Main memory",
    "text": "Main memory\n\nMain memory: where computer stores a program while program is running, and data used by the program\nKnown as Random Access Memory or RAM\n\nCPU is able to quickly access data in RAM\nVolatile memory used for temporary storage while program is running\nContents are erased when computer is off"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#secondary-storage-devices",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#secondary-storage-devices",
    "title": "Introduction to Computers and Programming",
    "section": "Secondary storage devices",
    "text": "Secondary storage devices\n\nSecondary storage: can hold data for long periods of time\n\nPrograms normally stored here and loaded to main memory when needed\n\nTypes of secondary memory\n\nDisk drive: magnetically encodes data onto a spinning circular disk\nSolid state drive: faster than disk drive, no moving parts, stores data in solid state memory\nFlash memory: portable, no physical disk"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#input-devices",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#input-devices",
    "title": "Introduction to Computers and Programming",
    "section": "Input devices",
    "text": "Input devices\n\nInput: data the computer collects from people and other devices\nInput device: component that collects the data\n\nExamples: keyboard, mouse, touchscreen, scanner,camera\nDisk drives can be considered input devices because they load programs into the main memory"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#output-devices",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#output-devices",
    "title": "Introduction to Computers and Programming",
    "section": "Output devices",
    "text": "Output devices\n\nOutput: data produced by the computer for other people or devices\n\nCan be text, image, audio, or bit stream\n\nOutput device: formats and presents output\n\nExamples: video display, printer -Disk drives and USB drives can be considered output devices because data is sent to them to be saved"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#software",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#software",
    "title": "Introduction to Computers and Programming",
    "section": "Software",
    "text": "Software\n\nEverything the computer does is controlled by software\n\nGeneral categories:\n\nApplication software\nSystem software\n\n\nApplication software: programs that make computer useful for every day tasks\nExamples: word processing, email, games, and Web browsers"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#principal",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#principal",
    "title": "Introduction to Computers and Programming",
    "section": "Principal",
    "text": "Principal\n\nAll data in a computer is stored in sequences of 0s and 1s\nByte: just enough memory to store letter or small number\n\nDivided into eight bits\nBit: electrical component that can hold positive or negative charge, like on/off switch\nThe on/off pattern of bits in a byte represents data stored in the byte"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#storing-numbers",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#storing-numbers",
    "title": "Introduction to Computers and Programming",
    "section": "Storing Numbers",
    "text": "Storing Numbers\n\nBit represents two values, 0 and 1\nComputers use binary numbering system\n\nPosition of digit jis assigned the value 2j-1\nTo determine value of binary number sum position values of the 1s\n\nByte size limits are 0 and 255\n\n0 = all bits off; 255 = all bits on\nTo store larger number, use several bytes"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#storing-characters",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#storing-characters",
    "title": "Introduction to Computers and Programming",
    "section": "Storing Characters",
    "text": "Storing Characters\n\nData stored in computer must be stored as binary number\nCharacters are converted to numeric code, numeric code stored in memory\n\nMost important coding scheme is ASCII\n\nASCII is limited: defines codes for only 128 characters\n\nUnicode coding scheme becoming standard\n\nCompatible with ASCII\nCan represent characters for other languages"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#advanced-number-storage",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#advanced-number-storage",
    "title": "Introduction to Computers and Programming",
    "section": "Advanced Number Storage",
    "text": "Advanced Number Storage\n\nTo store negative numbers and real numbers, computers use binary numbering and encoding schemes\n\nNegative numbers encoded using two’s complement\nReal numbers encoded using floating-point notation"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#other-type-of-data",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#other-type-of-data",
    "title": "Introduction to Computers and Programming",
    "section": "Other Type of Data",
    "text": "Other Type of Data\n\nDigital: describes any device that stores data as binary numbers\nDigital images are composed of pixels\n\nTo store images, each pixel is converted to a binary number representing the pixel’s color\n\nDigital music is composed of sections called samples\n\nTo store music, each sample is converted to a binary number"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#cpu",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#cpu",
    "title": "Introduction to Computers and Programming",
    "section": "CPU",
    "text": "CPU\n\nCPU designed to perform simple operations on pieces of data\n\nExamples: reading data, adding, subtracting, multiplying, and dividing numbers\n\nUnderstands instructions written in machine language and included in its instruction set\n\nEach brand of CPU has its own instruction set\n\nTo carry out meaningful calculation, CPU must perform many operations"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#from-machine-language-to-assembly-language",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#from-machine-language-to-assembly-language",
    "title": "Introduction to Computers and Programming",
    "section": "From Machine Language to Assembly Language",
    "text": "From Machine Language to Assembly Language\n\nAssembler: translates assembly language to machine language for execution by CPU\nAssembly language: uses short words (mnemonics) for instructions instead of binary numbers\n\nImpractical for people to write in machine language\nEasier for programmers to work with"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#high-level-languages",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#high-level-languages",
    "title": "Introduction to Computers and Programming",
    "section": "High-Level Languages",
    "text": "High-Level Languages\n\nLow-level language: close in nature to machine language\n\nExample: assembly language\n\nHigh-Level language: allows simple creation of powerful and complex programs\n\nNo need to know how CPU works or write large number of instructions\nMore intuitive to understand"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#keywords-operators-and-syntax-an-overview",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#keywords-operators-and-syntax-an-overview",
    "title": "Introduction to Computers and Programming",
    "section": "Keywords, Operators, and Syntax: an Overview",
    "text": "Keywords, Operators, and Syntax: an Overview\n\nKeywords: predefined words used to write program in high-level language\nEach keyword has specific meaning\nOperators: perform operations on data\nExample: math operators to perform arithmetic\nSyntax: set of rules to be followed when writing program\nStatement: individual instruction used in high-level language"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#compilers-and-interpreters",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#compilers-and-interpreters",
    "title": "Introduction to Computers and Programming",
    "section": "Compilers and Interpreters",
    "text": "Compilers and Interpreters\nPrograms written in high-level languages must be translated into machine language to be executed\n\nCompiler: translates high-level language program into separate machine language program\n\nMachine language program can be executed at any time"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#interactive-mode",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#interactive-mode",
    "title": "Introduction to Computers and Programming",
    "section": "Interactive Mode",
    "text": "Interactive Mode\n\nWhen you start Python in interactive mode, you will see a prompt\n\nIndicates the interpreter is waiting for a Python statement to be typed\nPrompt reappears after previous statement is executed\nError message displayed If you incorrectly type a statement\n\nGood way to learn new parts of Python"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#writing-python-programs-and-running-them-in-script-mode",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#writing-python-programs-and-running-them-in-script-mode",
    "title": "Introduction to Computers and Programming",
    "section": "Writing Python Programs and Running Them in Script Mode",
    "text": "Writing Python Programs and Running Them in Script Mode\n\nStatements entered in interactive mode are not saved as a program\nTo have a program use script mode\n\nSave a set of Python statements in a file\nThe filename should have the .pyextension\nTo run the file, or script, type python filename at the operating system command line"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#the-idle-programming-environment",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#the-idle-programming-environment",
    "title": "Introduction to Computers and Programming",
    "section": "The IDLE Programming Environment",
    "text": "The IDLE Programming Environment\n\nIDLE (Integrated Development Program): single program that provides tools to write, execute and test a program\nAutomatically installed when Python language is installed\nRuns in interactive mode\nHas built-in text editor with features designed to help write Python programs"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter8/chapter8.html#short-video-introduction",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter8/chapter8.html#short-video-introduction",
    "title": "More About Strings",
    "section": "SHORT VIDEO INTRODUCTION",
    "text": "SHORT VIDEO INTRODUCTION"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter8/chapter8.html#section",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter8/chapter8.html#section",
    "title": "More About Strings",
    "section": "#",
    "text": "#"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter8/chapter8.html#section-1",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter8/chapter8.html#section-1",
    "title": "More About Strings",
    "section": "##",
    "text": "##"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter8/chapter8.html#assignment",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter8/chapter8.html#assignment",
    "title": "More About Strings",
    "section": "Assignment",
    "text": "Assignment\n\n\n\nWhat to do\n\n\n\nRequirement\n\nPDF format\nfile name should be include your student id and name\n\nstuID_name_title.pdf (e.g. 1111111_ChungilChae_SelfIntroduction.pdf)\n\n\nDue date\n\nby Feb23(Sun) 11:59PM\nNO LATE SUBMISSION ALLOWED!!!!"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter6/chapter6.html#short-video-introduction",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter6/chapter6.html#short-video-introduction",
    "title": "Files and Exceptions",
    "section": "SHORT VIDEO INTRODUCTION",
    "text": "SHORT VIDEO INTRODUCTION"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter6/chapter6.html#section",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter6/chapter6.html#section",
    "title": "Files and Exceptions",
    "section": "#",
    "text": "#"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter6/chapter6.html#section-1",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter6/chapter6.html#section-1",
    "title": "Files and Exceptions",
    "section": "##",
    "text": "##"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter6/chapter6.html#assignment",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter6/chapter6.html#assignment",
    "title": "Files and Exceptions",
    "section": "Assignment",
    "text": "Assignment\n\n\n\nWhat to do\n\n\n\nRequirement\n\nPDF format\nfile name should be include your student id and name\n\nstuID_name_title.pdf (e.g. 1111111_ChungilChae_SelfIntroduction.pdf)\n\n\nDue date\n\nby Feb23(Sun) 11:59PM\nNO LATE SUBMISSION ALLOWED!!!!"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#short-video-introduction",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#short-video-introduction",
    "title": "Introduction and House Keeping",
    "section": "SHORT VIDEO INTRODUCTION",
    "text": "SHORT VIDEO INTRODUCTION"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#professor",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#professor",
    "title": "Introduction and House Keeping",
    "section": "Professor",
    "text": "Professor\nChad (Dr. Chungil Chae)\n\n\n\n\n\nChad (Chungil Chae)\nCBPM B223 | cchae@kean.edu\nAssistant Professor at CBPM, WKU since 2020 Fall\nCall ma Chad, but in formal situation and space, Dr.Chae or Prof.Chae\nTeaching business analytics major courses\n\nMGS 3001: Python for Business\nMGS 3101: Foundation of Business Analytics\nMGS 3701: Data Mining\nMGS 4701: Application of Business Analytics"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#teaching-assistant",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#teaching-assistant",
    "title": "Introduction and House Keeping",
    "section": "Teaching Assistant",
    "text": "Teaching Assistant\n\n\n\n\n\n\n**** ()\n(wku.edu.cn?)\nmajor in **"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#chapter-1",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#chapter-1",
    "title": "Introduction and House Keeping",
    "section": "Chapter 1:",
    "text": "Chapter 1:\nThis chapter begins by giving a very concrete and easy-to-understand explanation of how computers work, how data is stored and manipulated, and why we write programs in high-level languages. An introduction to Python, interactive mode, script mode, and the IDLE environment are also given."
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#chapter-2",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#chapter-2",
    "title": "Introduction and House Keeping",
    "section": "Chapter 2:",
    "text": "Chapter 2:\nThis chapter introduces the program development cycle, variables, data types, and simple programs that are written as sequence structures. The student learns to write simple programs that read input from the keyboard, perform mathematical operations, and produce formatted screen output. Pseudocode and flowcharts are also introduced as tools for designing programs. The chapter also includes an optional introduction to the turtle graphics library."
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#chapter-3",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#chapter-3",
    "title": "Introduction and House Keeping",
    "section": "Chapter 3:",
    "text": "Chapter 3:\nIn this chapter, the student learns about relational operators and Boolean expressions and is shown how to control the flow of a program with decision structures. The if, if-else, and if-elif-else statements are covered. Nested decision structures and logical operators are discussed as well. The chapter also includes an optional turtle graphics section, with a discussion of how to use decision structures to test the state of the turtle."
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#chapter-4",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#chapter-4",
    "title": "Introduction and House Keeping",
    "section": "Chapter 4:",
    "text": "Chapter 4:\nThis chapter shows the student how to create repetition structures using the while loop and for loop. Counters, accumulators, running totals, and sentinels are discussed, as well as techniques for writing input validation loops. The chapter also includes an optional section on using loops to draw designs with the turtle graphics library."
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#chapter-5",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#chapter-5",
    "title": "Introduction and House Keeping",
    "section": "Chapter 5:",
    "text": "Chapter 5:\nIn this chapter, the student first learns how to write and call void functions. The chapter shows the benefits of using functions to modularize programs and discusses the top-down design approach. Then, the student learns to pass arguments to functions. Common library functions, such as those for generating random numbers, are discussed. After learning how to call library functions and use their return value, the student learns to define and call his or her own functions. Then the student learns how to use modules to organize functions. An optional section includes a discussion of modularizing turtle graphics code with functions."
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#chapter-6",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#chapter-6",
    "title": "Introduction and House Keeping",
    "section": "Chapter 6:",
    "text": "Chapter 6:\nThis chapter introduces sequential file input and output. The student learns to read and write large sets of data and store data as fields and records. The chapter concludes by discussing exceptions and shows the student how to write exception-handling code."
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#chapter-7",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#chapter-7",
    "title": "Introduction and House Keeping",
    "section": "Chapter 7:",
    "text": "Chapter 7:\nThis chapter introduces the student to the concept of a sequence in Python and explores the use of two common Python sequences: lists and tuples. The student learns to use lists for arraylike operations, such as storing objects in a list, iterating over a list, searching for items in a list, and calculating the sum and average of items in a list. The chapter discusses list comprehension expressions, slicing, and many of the list methods. One- and two-dimensional lists are covered. The chapter also includes a discussion of the matplotlib package, and how to use it to plot charts and graphs from lists."
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#chapter-8",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#chapter-8",
    "title": "Introduction and House Keeping",
    "section": "Chapter 8:",
    "text": "Chapter 8:\nIn this chapter, the student learns to process strings at a detailed level. String slicing and algorithms that step through the individual characters in a string are discussed, and several built-in functions and string methods for character and text processing are introduced. This chapter also includes examples of string tokenizing and parsing CSV files."
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#chapter-9",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#chapter-9",
    "title": "Introduction and House Keeping",
    "section": "Chapter 9:",
    "text": "Chapter 9:\nThis chapter introduces the dictionary and set data structures. The student learns to store data as key-value pairs in dictionaries, search for values, change existing values, add new key-value pairs, delete key-value pairs, and write dictionary comprehensions. The student learns to store values as unique elements in sets and perform common set operations such as union, intersection, difference, and symmetric difference. Set comprehensions are also introduced. The chapter concludes with a discussion of object serialization and introduces the student to the Python pickle module."
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#chapter-10",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#chapter-10",
    "title": "Introduction and House Keeping",
    "section": "Chapter 10:",
    "text": "Chapter 10:\nThis chapter compares procedural and object-oriented programming practices. It covers the fundamental concepts of classes and objects. Attributes, methods, encapsulation and data hiding, init functions (which are similar to constructors), accessors, and mutators are discussed. The student learns how to model classes with UML and how to find the classes in a particular problem."
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#chapter-11",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#chapter-11",
    "title": "Introduction and House Keeping",
    "section": "Chapter 11:",
    "text": "Chapter 11:\nThe study of classes continues in this chapter with the subjects of inheritance and polymorphism. The topics covered include superclasses, subclasses, how __init__functions work in inheritance, method overriding, and polymorphism."
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#chapter-12",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#chapter-12",
    "title": "Introduction and House Keeping",
    "section": "Chapter 12:",
    "text": "Chapter 12:\nThis chapter discusses recursion and its use in problem solving. A visual trace of recursive calls is provided, and recursive applications are discussed. Recursive algorithms for many tasks are presented, such as finding factorials, finding a greatest common denominator (GCD), and summing a range of values in a list, and the classic Towers of Hanoi example are presented."
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#chapter-13",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#chapter-13",
    "title": "Introduction and House Keeping",
    "section": "Chapter 13:",
    "text": "Chapter 13:\nThis chapter discusses the basic aspects of designing a GUI application using the tkinter module in Python. Fundamental widgets, such as labels, buttons, entry fields, radio buttons, check buttons, list boxes, and dialog boxes, are covered. The student also learns how events work in a GUI application and how to write callback functions to handle events. The Chapter includes a discussion of the Canvas widget, and how to use it to draw lines, rectangles, ovals, arcs, polygons, and text."
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#chapter-14",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#chapter-14",
    "title": "Introduction and House Keeping",
    "section": "Chapter 14:",
    "text": "Chapter 14:\nThis chapter introduces the student to database programming. The chapter first introduces the basic concepts of databases, such as tables, rows, and primary keys. Then the student learns to use SQLite to connect to a database in Python. SQL is introduced and the student learns to execute queries and statements that search for rows, add new rows, update existing rows, and delete rows. CRUD applications are demonstrated, and the chapter concludes with a discussion of relational data."
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#class-information",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#class-information",
    "title": "Introduction and House Keeping",
    "section": "Class Information",
    "text": "Class Information\n\nMGS 3001 W01/W02: Python Programming for Business\nW01 Class time: T, TH 5:30 pm - 6:45 pm\nW01 Class room: CBPM C226\nW02 Class time: T, TH 7:00 pm - 8:15 pm\nW02 Class room: CBPM C226"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#textbook-and-resource",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#textbook-and-resource",
    "title": "Introduction and House Keeping",
    "section": "Textbook and Resource",
    "text": "Textbook and Resource\n\nMain textbook(Gaddis, 2021)\n\nGaddis, T. (2021). Introduction to research methods: A hands-on approach (Fifth). Pearson Education, Inc.\n\nSub textbook\n\nAI-Assisted Programming: Better Planning, Coding, Testing, and Deployment (Taulli, 2024)\nPractical Statistics for Data Scientists50+ Essential Concepts Using R and Python (Bruce, Bruce, & Gedeck, 2020)"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#in-class",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#in-class",
    "title": "Introduction and House Keeping",
    "section": "In CLass",
    "text": "In CLass\n\nYou are expected to read chapter and course material before class\nBased on your class participation, you will get extra score\nComputer and other digital device is allowed ONLY students uses it for class related purpose.\nIn case instuctor find unauthorized useage of digital device, you will be asked to leave class."
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#attendence-and-absent",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#attendence-and-absent",
    "title": "Introduction and House Keeping",
    "section": "Attendence and Absent",
    "text": "Attendence and Absent\n\nDON”T SENT ME EMAIL or ANY MESSAGE about YOUR ABSENT in ADVANCE\nMore than three times of absents automatically will be marked as F\nAttendence will be managed in student performance application\nWhen instructor or TA check your attendence and if you are not in class, no matter what reason, your attendence will be marked as absent.\nHowever, if you have proper and official evidence that WKU allow for absent, bring it to your instructor for revise your absent mark to attendece."
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#integration",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#integration",
    "title": "Introduction and House Keeping",
    "section": "Integration",
    "text": "Integration\n\nPlagiarism is not tolerated\n\nRight after find plagiarism, it will be reported to Office of Vice Chancellor for Academic Affairs directly\nStudent will be kicked out from class immediately\nRead Academic Integrity Policy"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#generative-ai-use",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter0/chapter0.html#generative-ai-use",
    "title": "Introduction and House Keeping",
    "section": "Generative AI Use",
    "text": "Generative AI Use\nStudents are permitted to use AI tools, including, but not limited to, ChatGhT, in this course to generate ideas and brainstorm.\n\nThink of generative AI as an always-available brainstorming partner. However, you should note that the material generated by these programs may be inaccurate, incomplete, or otherwise problematic. Beware that use may also stifle your independent thinking and creativity.\nAcademic work involves developing essential skills such as critical thinking, problem-solving, and effective communication, which cannot be fully developed by relying solely on Artificial Intelligence (AI)."
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter10/chapter10.html#short-video-introduction",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter10/chapter10.html#short-video-introduction",
    "title": "Classes and Object-Oriented Programming",
    "section": "SHORT VIDEO INTRODUCTION",
    "text": "SHORT VIDEO INTRODUCTION"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter10/chapter10.html#section",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter10/chapter10.html#section",
    "title": "Classes and Object-Oriented Programming",
    "section": "#",
    "text": "#"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter10/chapter10.html#section-1",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter10/chapter10.html#section-1",
    "title": "Classes and Object-Oriented Programming",
    "section": "##",
    "text": "##"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter10/chapter10.html#assignment",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter10/chapter10.html#assignment",
    "title": "Classes and Object-Oriented Programming",
    "section": "Assignment",
    "text": "Assignment\n\n\n\nWhat to do\n\n\n\nRequirement\n\nPDF format\nfile name should be include your student id and name\n\nstuID_name_title.pdf (e.g. 1111111_ChungilChae_SelfIntroduction.pdf)\n\n\nDue date\n\nby Feb23(Sun) 11:59PM\nNO LATE SUBMISSION ALLOWED!!!!"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter5/chapter5.html#short-video-introduction",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter5/chapter5.html#short-video-introduction",
    "title": "Functions",
    "section": "SHORT VIDEO INTRODUCTION",
    "text": "SHORT VIDEO INTRODUCTION"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter5/chapter5.html#section",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter5/chapter5.html#section",
    "title": "Functions",
    "section": "#",
    "text": "#"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter5/chapter5.html#section-1",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter5/chapter5.html#section-1",
    "title": "Functions",
    "section": "##",
    "text": "##"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter5/chapter5.html#assignment",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter5/chapter5.html#assignment",
    "title": "Functions",
    "section": "Assignment",
    "text": "Assignment\n\n\n\nWhat to do\n\n\n\nRequirement\n\nPDF format\nfile name should be include your student id and name\n\nstuID_name_title.pdf (e.g. 1111111_ChungilChae_SelfIntroduction.pdf)\n\n\nDue date\n\nby Feb23(Sun) 11:59PM\nNO LATE SUBMISSION ALLOWED!!!!"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter3/chapter3.html#short-video-introduction",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter3/chapter3.html#short-video-introduction",
    "title": "Decision Structures and Boolean Logic",
    "section": "SHORT VIDEO INTRODUCTION",
    "text": "SHORT VIDEO INTRODUCTION"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter3/chapter3.html#section",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter3/chapter3.html#section",
    "title": "Decision Structures and Boolean Logic",
    "section": "#",
    "text": "#"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter3/chapter3.html#section-1",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter3/chapter3.html#section-1",
    "title": "Decision Structures and Boolean Logic",
    "section": "##",
    "text": "##"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter3/chapter3.html#assignment",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter3/chapter3.html#assignment",
    "title": "Decision Structures and Boolean Logic",
    "section": "Assignment",
    "text": "Assignment\n\n\n\nWhat to do\n\n\n\nRequirement\n\nPDF format\nfile name should be include your student id and name\n\nstuID_name_title.pdf (e.g. 1111111_ChungilChae_SelfIntroduction.pdf)\n\n\nDue date\n\nby Feb23(Sun) 11:59PM\nNO LATE SUBMISSION ALLOWED!!!!"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter13/chapter13.html#short-video-introduction",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter13/chapter13.html#short-video-introduction",
    "title": "GUI Programming",
    "section": "SHORT VIDEO INTRODUCTION",
    "text": "SHORT VIDEO INTRODUCTION"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter13/chapter13.html#section",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter13/chapter13.html#section",
    "title": "GUI Programming",
    "section": "#",
    "text": "#"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter13/chapter13.html#section-1",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter13/chapter13.html#section-1",
    "title": "GUI Programming",
    "section": "##",
    "text": "##"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter13/chapter13.html#assignment",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter13/chapter13.html#assignment",
    "title": "GUI Programming",
    "section": "Assignment",
    "text": "Assignment\n\n\n\nWhat to do\n\n\n\nRequirement\n\nPDF format\nfile name should be include your student id and name\n\nstuID_name_title.pdf (e.g. 1111111_ChungilChae_SelfIntroduction.pdf)\n\n\nDue date\n\nby Feb23(Sun) 11:59PM\nNO LATE SUBMISSION ALLOWED!!!!"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter12/chapter12.html#short-video-introduction",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter12/chapter12.html#short-video-introduction",
    "title": "Recursion",
    "section": "SHORT VIDEO INTRODUCTION",
    "text": "SHORT VIDEO INTRODUCTION"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter12/chapter12.html#section",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter12/chapter12.html#section",
    "title": "Recursion",
    "section": "#",
    "text": "#"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter12/chapter12.html#section-1",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter12/chapter12.html#section-1",
    "title": "Recursion",
    "section": "##",
    "text": "##"
  },
  {
    "objectID": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter12/chapter12.html#assignment",
    "href": "Course/MGS3001/03_Lecture_Note/LectureNote_Chad/PPT/chapter12/chapter12.html#assignment",
    "title": "Recursion",
    "section": "Assignment",
    "text": "Assignment\n\n\n\nWhat to do\n\n\n\nRequirement\n\nPDF format\nfile name should be include your student id and name\n\nstuID_name_title.pdf (e.g. 1111111_ChungilChae_SelfIntroduction.pdf)\n\n\nDue date\n\nby Feb23(Sun) 11:59PM\nNO LATE SUBMISSION ALLOWED!!!!"
  },
  {
    "objectID": "Course/MGS3001/07_Script/python-analytics/barchart_race/barchart2.html",
    "href": "Course/MGS3001/07_Script/python-analytics/barchart_race/barchart2.html",
    "title": "Chad's Course Portfolio",
    "section": "",
    "text": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as ticker\nimport matplotlib.animation as animation\nfrom IPython.display import HTML\n\n\ndf = pd.read_csv('https://gist.githubusercontent.com/johnburnmurdoch/4199dbe55095c3e13de8d5b2e5e5307a/raw/fa018b25c24b7b5f47fd0568937ff6c04e384786/city_populations',usecols=['name', 'group', 'year', 'value'])\ndf.head(3)\n\n\n\n\n\n\n\n\nname\ngroup\nyear\nvalue\n\n\n\n\n0\nAgra\nIndia\n1575\n200.0\n\n\n1\nAgra\nIndia\n1576\n212.0\n\n\n2\nAgra\nIndia\n1577\n224.0\n\n\n\n\n\n\n\n\ncurrent_year = 2018\ndff = (df[df['year'].eq(current_year)]\n       .sort_values(by='value', ascending=True)\n       .head(10))\ndff\n\n\n\n\n\n\n\n\nname\ngroup\nyear\nvalue\n\n\n\n\n2537\nKarachi\nAsia\n2018\n18185.2\n\n\n4327\nNew York\nNorth America\n2018\n18713.0\n\n\n1336\nDhaka\nAsia\n2018\n19632.6\n\n\n1195\nCairo\nMiddle East\n2018\n19849.6\n\n\n4679\nOsaka\nAsia\n2018\n20409.0\n\n\n3574\nMexico City\nLatin America\n2018\n21520.4\n\n\n5445\nSao Paulo\nLatin America\n2018\n21697.8\n\n\n3748\nMumbai\nIndia\n2018\n22120.0\n\n\n689\nBeijing\nAsia\n2018\n22674.2\n\n\n5547\nShanghai\nAsia\n2018\n25778.6\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(15, 8))\nax.barh(dff['name'], dff['value'])\n\n\n\n\n\n\n\n\n\ncolors = dict(zip(\n    ['India', 'Europe', 'Asia', 'Latin America',\n     'Middle East', 'North America', 'Africa'],\n    ['#adb0ff', '#ffb3ff', '#90d595', '#e48381',\n     '#aafbff', '#f7bb5f', '#eafb50']\n))\ngroup_lk = df.set_index('name')['group'].to_dict()\n\n\nfig, ax = plt.subplots(figsize=(15, 8))\ndff = dff[::-1]   # flip values from top to bottom\n# pass colors values to `color=`\nax.barh(dff['name'], dff['value'], color=[colors[group_lk[x]] for x in dff['name']])\n# iterate over the values to plot labels and values (Tokyo, Asia, 38194.2)\nfor i, (value, name) in enumerate(zip(dff['value'], dff['name'])):\n    ax.text(value, i,     name,            ha='right')  # Tokyo: name\n    ax.text(value, i-.25, group_lk[name],  ha='right')  # Asia: group name\n    ax.text(value, i,     value,           ha='left')   # 38194.2: value\n# Add year right middle portion of canvas\nax.text(1, 0.4, current_year, transform=ax.transAxes, size=46, ha='right')\n\nText(1, 0.4, '2018')\n\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots(figsize=(15, 8))\ndef draw_barchart(year):\n    dff = df[df['year'].eq(year)].sort_values(by='value', ascending=True).tail(10)\n    ax.clear()\n    ax.barh(dff['name'], dff['value'], color=[colors[group_lk[x]] for x in dff['name']])\n    dx = dff['value'].max() / 200\n    for i, (value, name) in enumerate(zip(dff['value'], dff['name'])):\n        ax.text(value-dx, i,     name,           size=14, weight=600, ha='right', va='bottom')\n        ax.text(value-dx, i-.25, group_lk[name], size=10, color='#444444', ha='right', va='baseline')\n        ax.text(value+dx, i,     f'{value:,.0f}',  size=14, ha='left',  va='center')\n    # ... polished styles\n    ax.text(1, 0.4, year, transform=ax.transAxes, color='#777777', size=46, ha='right', weight=800)\n    ax.text(0, 1.06, 'Population (thousands)', transform=ax.transAxes, size=12, color='#777777')\n    ax.xaxis.set_major_formatter(ticker.StrMethodFormatter('{x:,.0f}'))\n    ax.xaxis.set_ticks_position('top')\n    ax.tick_params(axis='x', colors='#777777', labelsize=12)\n    ax.set_yticks([])\n    ax.margins(0, 0.01)\n    ax.grid(which='major', axis='x', linestyle='-')\n    ax.set_axisbelow(True)\n    ax.text(0, 1.12, 'The most populous cities in the world from 1500 to 2018',\n            transform=ax.transAxes, size=24, weight=600, ha='left')\n    ax.text(1, 0, 'by @pratapvardhan; credit @jburnmurdoch', transform=ax.transAxes, ha='right',\n            color='#777777', bbox=dict(facecolor='white', alpha=0.8, edgecolor='white'))\n    plt.box(False)\n    \ndraw_barchart(2018)\n\n/var/folders/2b/_nc5068s71723wyftryb7nk00000gn/T/ipykernel_49355/563786439.py:17: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n  ax.set_yticks([])\n\n\n\n\n\n\n\n\n\n\nimport matplotlib.animation as animation\nfrom IPython.display import HTML\nfig, ax = plt.subplots(figsize=(15, 8))\nanimator = animation.FuncAnimation(fig, draw_barchart, frames=range(1968, 2019))\nHTML(animator.to_jshtml()) \n# or use animator.to_html5_video() or animator.save()\n\n/var/folders/2b/_nc5068s71723wyftryb7nk00000gn/T/ipykernel_49355/563786439.py:17: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n  ax.set_yticks([])\n/var/folders/2b/_nc5068s71723wyftryb7nk00000gn/T/ipykernel_49355/563786439.py:17: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n  ax.set_yticks([])\n/var/folders/2b/_nc5068s71723wyftryb7nk00000gn/T/ipykernel_49355/563786439.py:17: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n  ax.set_yticks([])\n/var/folders/2b/_nc5068s71723wyftryb7nk00000gn/T/ipykernel_49355/563786439.py:17: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n  ax.set_yticks([])\n/var/folders/2b/_nc5068s71723wyftryb7nk00000gn/T/ipykernel_49355/563786439.py:17: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n  ax.set_yticks([])\n/var/folders/2b/_nc5068s71723wyftryb7nk00000gn/T/ipykernel_49355/563786439.py:17: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n  ax.set_yticks([])\n/var/folders/2b/_nc5068s71723wyftryb7nk00000gn/T/ipykernel_49355/563786439.py:17: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n  ax.set_yticks([])\n/var/folders/2b/_nc5068s71723wyftryb7nk00000gn/T/ipykernel_49355/563786439.py:17: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n  ax.set_yticks([])\n/var/folders/2b/_nc5068s71723wyftryb7nk00000gn/T/ipykernel_49355/563786439.py:17: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n  ax.set_yticks([])\n/var/folders/2b/_nc5068s71723wyftryb7nk00000gn/T/ipykernel_49355/563786439.py:17: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n  ax.set_yticks([])\n/var/folders/2b/_nc5068s71723wyftryb7nk00000gn/T/ipykernel_49355/563786439.py:17: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n  ax.set_yticks([])\n/var/folders/2b/_nc5068s71723wyftryb7nk00000gn/T/ipykernel_49355/563786439.py:17: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n  ax.set_yticks([])\n/var/folders/2b/_nc5068s71723wyftryb7nk00000gn/T/ipykernel_49355/563786439.py:17: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n  ax.set_yticks([])\n/var/folders/2b/_nc5068s71723wyftryb7nk00000gn/T/ipykernel_49355/563786439.py:17: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n  ax.set_yticks([])\n/var/folders/2b/_nc5068s71723wyftryb7nk00000gn/T/ipykernel_49355/563786439.py:17: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n  ax.set_yticks([])\n/var/folders/2b/_nc5068s71723wyftryb7nk00000gn/T/ipykernel_49355/563786439.py:17: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n  ax.set_yticks([])\n/var/folders/2b/_nc5068s71723wyftryb7nk00000gn/T/ipykernel_49355/563786439.py:17: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n  ax.set_yticks([])\n/var/folders/2b/_nc5068s71723wyftryb7nk00000gn/T/ipykernel_49355/563786439.py:17: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n  ax.set_yticks([])\n/var/folders/2b/_nc5068s71723wyftryb7nk00000gn/T/ipykernel_49355/563786439.py:17: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n  ax.set_yticks([])\n/var/folders/2b/_nc5068s71723wyftryb7nk00000gn/T/ipykernel_49355/563786439.py:17: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n  ax.set_yticks([])\n/var/folders/2b/_nc5068s71723wyftryb7nk00000gn/T/ipykernel_49355/563786439.py:17: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n  ax.set_yticks([])\n/var/folders/2b/_nc5068s71723wyftryb7nk00000gn/T/ipykernel_49355/563786439.py:17: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n  ax.set_yticks([])\n/var/folders/2b/_nc5068s71723wyftryb7nk00000gn/T/ipykernel_49355/563786439.py:17: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n  ax.set_yticks([])\n/var/folders/2b/_nc5068s71723wyftryb7nk00000gn/T/ipykernel_49355/563786439.py:17: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n  ax.set_yticks([])\n/var/folders/2b/_nc5068s71723wyftryb7nk00000gn/T/ipykernel_49355/563786439.py:17: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n  ax.set_yticks([])\n/var/folders/2b/_nc5068s71723wyftryb7nk00000gn/T/ipykernel_49355/563786439.py:17: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n  ax.set_yticks([])\n/var/folders/2b/_nc5068s71723wyftryb7nk00000gn/T/ipykernel_49355/563786439.py:17: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n  ax.set_yticks([])\n/var/folders/2b/_nc5068s71723wyftryb7nk00000gn/T/ipykernel_49355/563786439.py:17: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n  ax.set_yticks([])\n/var/folders/2b/_nc5068s71723wyftryb7nk00000gn/T/ipykernel_49355/563786439.py:17: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n  ax.set_yticks([])\n/var/folders/2b/_nc5068s71723wyftryb7nk00000gn/T/ipykernel_49355/563786439.py:17: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n  ax.set_yticks([])\n/var/folders/2b/_nc5068s71723wyftryb7nk00000gn/T/ipykernel_49355/563786439.py:17: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n  ax.set_yticks([])\n/var/folders/2b/_nc5068s71723wyftryb7nk00000gn/T/ipykernel_49355/563786439.py:17: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n  ax.set_yticks([])\n/var/folders/2b/_nc5068s71723wyftryb7nk00000gn/T/ipykernel_49355/563786439.py:17: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n  ax.set_yticks([])\n/var/folders/2b/_nc5068s71723wyftryb7nk00000gn/T/ipykernel_49355/563786439.py:17: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n  ax.set_yticks([])\n/var/folders/2b/_nc5068s71723wyftryb7nk00000gn/T/ipykernel_49355/563786439.py:17: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n  ax.set_yticks([])\n/var/folders/2b/_nc5068s71723wyftryb7nk00000gn/T/ipykernel_49355/563786439.py:17: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n  ax.set_yticks([])\n/var/folders/2b/_nc5068s71723wyftryb7nk00000gn/T/ipykernel_49355/563786439.py:17: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n  ax.set_yticks([])\n/var/folders/2b/_nc5068s71723wyftryb7nk00000gn/T/ipykernel_49355/563786439.py:17: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n  ax.set_yticks([])\n/var/folders/2b/_nc5068s71723wyftryb7nk00000gn/T/ipykernel_49355/563786439.py:17: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n  ax.set_yticks([])\n/var/folders/2b/_nc5068s71723wyftryb7nk00000gn/T/ipykernel_49355/563786439.py:17: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n  ax.set_yticks([])\n/var/folders/2b/_nc5068s71723wyftryb7nk00000gn/T/ipykernel_49355/563786439.py:17: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n  ax.set_yticks([])\n/var/folders/2b/_nc5068s71723wyftryb7nk00000gn/T/ipykernel_49355/563786439.py:17: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n  ax.set_yticks([])\n/var/folders/2b/_nc5068s71723wyftryb7nk00000gn/T/ipykernel_49355/563786439.py:17: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n  ax.set_yticks([])\n/var/folders/2b/_nc5068s71723wyftryb7nk00000gn/T/ipykernel_49355/563786439.py:17: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n  ax.set_yticks([])\n/var/folders/2b/_nc5068s71723wyftryb7nk00000gn/T/ipykernel_49355/563786439.py:17: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n  ax.set_yticks([])\n/var/folders/2b/_nc5068s71723wyftryb7nk00000gn/T/ipykernel_49355/563786439.py:17: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n  ax.set_yticks([])\n/var/folders/2b/_nc5068s71723wyftryb7nk00000gn/T/ipykernel_49355/563786439.py:17: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n  ax.set_yticks([])\n/var/folders/2b/_nc5068s71723wyftryb7nk00000gn/T/ipykernel_49355/563786439.py:17: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n  ax.set_yticks([])\n/var/folders/2b/_nc5068s71723wyftryb7nk00000gn/T/ipykernel_49355/563786439.py:17: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n  ax.set_yticks([])\n/var/folders/2b/_nc5068s71723wyftryb7nk00000gn/T/ipykernel_49355/563786439.py:17: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n  ax.set_yticks([])\n/var/folders/2b/_nc5068s71723wyftryb7nk00000gn/T/ipykernel_49355/563786439.py:17: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n  ax.set_yticks([])\n/var/folders/2b/_nc5068s71723wyftryb7nk00000gn/T/ipykernel_49355/563786439.py:17: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n  ax.set_yticks([])\n\n\n\n\n\n\n\n\n\n  \n  \n    \n    \n      \n          \n      \n        \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n      \n          \n    \n    \n      \n      Once\n      \n      Loop\n      \n      Reflect\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwith plt.xkcd():\n    fig, ax = plt.subplots(figsize=(15, 8))\n    draw_barchart(2018)\n\n/var/folders/2b/_nc5068s71723wyftryb7nk00000gn/T/ipykernel_49355/563786439.py:17: MatplotlibDeprecationWarning: Support for passing numbers through unit converters is deprecated since 3.5 and support will be removed two minor releases later; use Axis.convert_units instead.\n  ax.set_yticks([])"
  },
  {
    "objectID": "Course/MGS3001/07_Script/python-analytics/maptest.html",
    "href": "Course/MGS3001/07_Script/python-analytics/maptest.html",
    "title": "Chad's Course Portfolio",
    "section": "",
    "text": "#this example, we're going to consider the whole world, centered on Europe.\n\n# import the library\nimport folium\n\n# Make an empty map\nm = folium.Map(location=[20,0], tiles=\"OpenStreetMap\", zoom_start=2)\n\n# Show the map\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n# Import the pandas library\nimport pandas as pd\n\n# Make a data frame with dots to show on the map\ndata = pd.DataFrame({\n   'lon':[-58, 2, 145, 30.32, -4.03, -73.57, 36.82, -38.5],\n   'lat':[-34, 49, -38, 59.93, 5.33, 45.52, -1.29, -12.97],\n   'name':['Buenos Aires', 'Paris', 'melbourne', 'St Petersbourg', 'Abidjan', 'Montreal', 'Nairobi', 'Salvador'],\n   'value':[10, 12, 40, 70, 23, 43, 100, 43]\n}, dtype=str)\n\ndata\n\n\n\n\n\n\n\n\nlon\nlat\nname\nvalue\n\n\n\n\n0\n-58\n-34\nBuenos Aires\n10\n\n\n1\n2\n49\nParis\n12\n\n\n2\n145\n-38\nmelbourne\n40\n\n\n3\n30.32\n59.93\nSt Petersbourg\n70\n\n\n4\n-4.03\n5.33\nAbidjan\n23\n\n\n5\n-73.57\n45.52\nMontreal\n43\n\n\n6\n36.82\n-1.29\nNairobi\n100\n\n\n7\n-38.5\n-12.97\nSalvador\n43\n\n\n\n\n\n\n\n\n# add marker one by one on the map\nfor i in range(0,len(data)):\n   folium.Circle(\n      location=[data.iloc[i]['lat'], data.iloc[i]['lon']],\n      popup=data.iloc[i]['name'],\n      radius=float(data.iloc[i]['value'])*20000,\n      color='crimson',\n      fill=True,\n      fill_color='crimson'\n   ).add_to(m)\n\n# Show the map again\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook"
  },
  {
    "objectID": "Course/MGS3001/07_Script/python-analytics/code_in_class_Analysis.html",
    "href": "Course/MGS3001/07_Script/python-analytics/code_in_class_Analysis.html",
    "title": "Chad's Course Portfolio",
    "section": "",
    "text": "pip install statsmodels dfply\n\nLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nRequirement already satisfied: statsmodels in /usr/local/lib/python3.7/dist-packages (0.10.2)\nCollecting dfply\n  Downloading dfply-0.3.3-py3-none-any.whl (612 kB)\n     |████████████████████████████████| 612 kB 9.1 MB/s \nRequirement already satisfied: pandas&gt;=0.19 in /usr/local/lib/python3.7/dist-packages (from statsmodels) (1.3.5)\nRequirement already satisfied: patsy&gt;=0.4.0 in /usr/local/lib/python3.7/dist-packages (from statsmodels) (0.5.2)\nRequirement already satisfied: scipy&gt;=0.18 in /usr/local/lib/python3.7/dist-packages (from statsmodels) (1.4.1)\nRequirement already satisfied: numpy&gt;=1.11 in /usr/local/lib/python3.7/dist-packages (from statsmodels) (1.21.6)\nRequirement already satisfied: python-dateutil&gt;=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=0.19-&gt;statsmodels) (2.8.2)\nRequirement already satisfied: pytz&gt;=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas&gt;=0.19-&gt;statsmodels) (2022.1)\nRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from patsy&gt;=0.4.0-&gt;statsmodels) (1.15.0)\nInstalling collected packages: dfply\nSuccessfully installed dfply-0.3.3\n\n\n\nimport pandas as pd\nimport numpy as np\nfrom dfply import *\nimport statsmodels.formula.api as smf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n\n/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n  import pandas.util.testing as tm\n\n\n\n# get data\nurl = \"http://peopleanalytics-regression-book.org/data/ugtests.csv\"\nugtests = pd.read_csv(url)\n\n\nsns.scatterplot(data=ugtests, x=\"Yr1\", y=\"Final\")\n\n\n\n\n\n\n\n\n\nfig = px.scatter_3d(ugtests, x='Yr3', y='Final', z='Yr1')\nfig.show()\n\n\n# define model\nmodel = smf.ols(formula = \"Final ~ Yr3 + Yr2 + Yr1\", data = ugtests)\n\n\n# fit model\nugtests_model = model.fit()\n\n\n# see results summary\nprint(ugtests_model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  Final   R-squared:                       0.530\nModel:                            OLS   Adj. R-squared:                  0.529\nMethod:                 Least Squares   F-statistic:                     365.5\nDate:                Wed, 29 Jun 2022   Prob (F-statistic):          8.22e-159\nTime:                        05:48:54   Log-Likelihood:                -4711.6\nNo. Observations:                 975   AIC:                             9431.\nDf Residuals:                     971   BIC:                             9451.\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     14.1460      5.480      2.581      0.010       3.392      24.900\nYr3            0.8657      0.029     29.710      0.000       0.809       0.923\nYr2            0.4313      0.033     13.267      0.000       0.367       0.495\nYr1            0.0760      0.065      1.163      0.245      -0.052       0.204\n==============================================================================\nOmnibus:                        0.762   Durbin-Watson:                   2.006\nProb(Omnibus):                  0.683   Jarque-Bera (JB):                0.795\nSkew:                           0.067   Prob(JB):                        0.672\nKurtosis:                       2.961   Cond. No.                         858.\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nfrom google.colab import files\nairports = files.upload()\nflight = files.upload()\n\n\n     \n     \n      Upload widget is only available when the cell has been executed in the\n      current browser session. Please rerun this cell to enable.\n      \n       \n\n\n\npd.DataFrame(airports.items())\n\n\nflight.head\n\n\nsummary = flight.describe()\nsummary = summary.transpose()\nsummary.head\n\n\n(flight &gt;&gt;\n select(X.origin, X.dest, X.hour) &gt;&gt;\n head()\n)\n\n\n(flight &gt;&gt;\n drop(X.year, X.month, X.day) &gt;&gt;\n head()\n)\n\n\n(flight &gt;&gt;\n group_by(X.origin) &gt;&gt;\n summarize(mean_distance = X.distance.mean())\n)\n\n\n(flight &gt;&gt;\n  mask(X.hour &gt; 10) &gt;&gt; # step 1\n  mutate(speed = X.distance / (X.air_time * 60)) &gt;&gt; # step 2\n  group_by(X.origin) &gt;&gt; # step 3a\n  summarize(mean_speed = X.speed.mean()) &gt;&gt; # step 3b\n  arrange(X.mean_speed, ascending=False) # step 4\n)\n\n\naverage = df['Price'].mean()\nprint(average)"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-17-ProbSolutions-RF.html",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-17-ProbSolutions-RF.html",
    "title": "Chapter 17: Regression-Based Forecasting",
    "section": "",
    "text": "2019-2020 Galit Shmueli, Peter C. Bruce, Peter Gedeck\n\nData Mining for Business Analytics: Concepts, Techniques, and Applications in Python (First Edition) Galit Shmueli, Peter C. Bruce, Peter Gedeck, and Nitin R. Patel. 2019.\nDate: 2020-03-08\nPython Version: 3.8.2 Jupyter Notebook Version: 5.6.1\nPackages: - dmba: 0.0.12 - matplotlib: 3.2.0 - numpy: 1.18.1 - pandas: 1.0.1 - statsmodels: 0.11.1\nThe assistance from Mr. Kuber Deokar and Ms. Anuja Kulkarni in preparing these solutions is gratefully acknowledged.\n# import required packages for this chapter\nfrom pathlib import Path\nimport warnings\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.formula.api as sm\nfrom statsmodels.tsa import tsatools\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.graphics import tsaplots\n\nfrom dmba import regressionSummary\n\nimport matplotlib.pylab as plt\n\n%matplotlib inline\n# Working directory:\n#\n# We assume that data are kept in the same directory as the notebook. If you keep your \n# data in a different folder, replace the argument of the `Path`\nDATA = Path('.')\n# and then load data using \n#\n# pd.read_csv(DATA / ‘filename.csv’)"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-17-ProbSolutions-RF.html#solution-17.1.a",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-17-ProbSolutions-RF.html#solution-17.1.a",
    "title": "Chapter 17: Regression-Based Forecasting",
    "section": "Solution 17.1.a",
    "text": "Solution 17.1.a\nPlot the pre-event AIR time series. What time series components appear?\n\ndf = pd.read_csv(DATA / 'Sept11Travel.csv')\n\n# convert the date information to a datetime object\ndf['Date'] = pd.to_datetime(df.Month, format='%b-%y')\n\nair_ts = pd.Series(df['Air RPM (000s)'].values, index=df.Date, name='Air')\npre_air_ts = air_ts[:'2001-08-31']\npost_air_ts = air_ts['2001-08-31':]\n\nrail_ts = pd.Series(df['Rail PM'].values, index=df.Date, name='Rail')\npre_rail_ts = rail_ts[:'2001-08-31']\npost_rail_ts = rail_ts['2001-08-31':]\n\ncar_ts = pd.Series(df['VMT (billions)'].values, index=df.Date, name='Car')\npre_car_ts = car_ts[:'2001-08-31']\npost_car_ts = car_ts['2001-08-31':]\n\nax = pre_air_ts.plot()\nax.set_yscale('log')\nplt.show()\n\n\n\n\n\n\n\n\nFrom the time plot we can say that the following components appear in the given time series:\nLevel, trend, seasonality and noise."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-17-ProbSolutions-RF.html#solution-17.1.b",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-17-ProbSolutions-RF.html#solution-17.1.b",
    "title": "Chapter 17: Regression-Based Forecasting",
    "section": "Solution 17.1.b",
    "text": "Solution 17.1.b\nThe Figure in the book shows a time plot of the seasonally adjusted pre-September-11 AIR series. Which of the following methods would be adequate for forecasting the series shown in the figure?\n\nLinear regression model seasonality\nLinear regression model with trend\nLinear regression model with trend and seasonality\nLinear regression model seasonality - NO\nLinear regression model with trend - YES\nLinear regression model with trend and seasonality - NO"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-17-ProbSolutions-RF.html#solution-17.1.c",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-17-ProbSolutions-RF.html#solution-17.1.c",
    "title": "Chapter 17: Regression-Based Forecasting",
    "section": "Solution 17.1.c",
    "text": "Solution 17.1.c\nSpecify a linear regression model for the AIR series that would produce a seasonally adjusted series similar to the Figure shown in the book, with multiplicative seasonality. What is the outcome variable? What are the predictors?\n\nOutcome variable = log Y\nPredictor = 11 dummies, for 11 of the 12 months (one reference category)"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-17-ProbSolutions-RF.html#solution-17.1.d",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-17-ProbSolutions-RF.html#solution-17.1.d",
    "title": "Chapter 17: Regression-Based Forecasting",
    "section": "Solution 17.1.d",
    "text": "Solution 17.1.d\nRun the regression model from (c). Remember to use only pre-event data.\n\npre_air_df = tsatools.add_trend(pre_air_ts, trend='ct')\npre_air_df['Month'] = pre_air_df.index.month\n\nlm_ts = sm.ols(formula='np.log(Air) ~ C(Month)', data=pre_air_df).fit()\nlm_ts.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nnp.log(Air)\nR-squared:\n0.322\n\n\nModel:\nOLS\nAdj. R-squared:\n0.263\n\n\nMethod:\nLeast Squares\nF-statistic:\n5.515\n\n\nDate:\nSun, 08 Mar 2020\nProb (F-statistic):\n3.60e-07\n\n\nTime:\n20:21:12\nLog-Likelihood:\n68.335\n\n\nNo. Observations:\n140\nAIC:\n-112.7\n\n\nDf Residuals:\n128\nBIC:\n-77.37\n\n\nDf Model:\n11\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n17.5598\n0.045\n391.626\n0.000\n17.471\n17.649\n\n\nC(Month)[T.2]\n-0.0616\n0.063\n-0.971\n0.333\n-0.187\n0.064\n\n\nC(Month)[T.3]\n0.1410\n0.063\n2.224\n0.028\n0.016\n0.267\n\n\nC(Month)[T.4]\n0.0955\n0.063\n1.507\n0.134\n-0.030\n0.221\n\n\nC(Month)[T.5]\n0.1250\n0.063\n1.971\n0.051\n-0.001\n0.250\n\n\nC(Month)[T.6]\n0.2064\n0.063\n3.254\n0.001\n0.081\n0.332\n\n\nC(Month)[T.7]\n0.2717\n0.063\n4.285\n0.000\n0.146\n0.397\n\n\nC(Month)[T.8]\n0.2983\n0.063\n4.704\n0.000\n0.173\n0.424\n\n\nC(Month)[T.9]\n0.0803\n0.065\n1.239\n0.218\n-0.048\n0.209\n\n\nC(Month)[T.10]\n0.1137\n0.065\n1.753\n0.082\n-0.015\n0.242\n\n\nC(Month)[T.11]\n0.0280\n0.065\n0.432\n0.666\n-0.100\n0.156\n\n\nC(Month)[T.12]\n0.0699\n0.065\n1.078\n0.283\n-0.058\n0.198\n\n\n\n\n\n\n\n\nOmnibus:\n59.298\nDurbin-Watson:\n0.032\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n8.596\n\n\nSkew:\n0.010\nProb(JB):\n0.0136\n\n\nKurtosis:\n1.786\nCond. No.\n12.8\n\n\n\nWarnings:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\ncheck the Estimate for Season2 (Feb), -0.600 means Feb numbers (on an average) are less than Jan and positive estimates for all other seasons means they are more than Jan.\n\nSolution 17.1.d.i\nWhat can we learn from the statistical insignificance of the coefficients for October and September?\nWe learn that the apparently positive seasonal effect for Sep. (P-value=0.218) is not statistically significant. However, it is statistically significant for October (P-value=0.082).\n\n\nSolution 17.1.d.ii\nThe actual value of AIR (air revenue passenger miles) in January 1990 was 35.153577 billion. What is the residual for this month, using the regression model? Report the residual in terms of air revenue passenger miles.\n\n# Note that the model predicts the logarithm of the outcome, we need to `exp` it\npredicted = np.exp(lm_ts.predict(pre_air_df))[0]\nactual = pre_air_ts[0]\nprint('Predicted for Jan 1990   :', predicted)\nprint('Actual value for Jan 1990:', actual)\nresidual = actual - predicted\nprint('Residual for Jan 1990    :', residual)\n\nPredicted for Jan 1990   : 42278658.58950092\nActual value for Jan 1990: 35153577\nResidual for Jan 1990    : -7125081.589500919"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-17-ProbSolutions-RF.html#solution-17.2.a",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-17-ProbSolutions-RF.html#solution-17.2.a",
    "title": "Chapter 17: Regression-Based Forecasting",
    "section": "Solution 17.2.a",
    "text": "Solution 17.2.a\nWhich of the following regression models would fit the series best? (Choose one.)\n\nLinear trend model\nLinear trend model with seasonality\nQuadratic trend model\nQuadratic trend model with seasonality\n\nOf the four options, the quadratic trend model would fit the series best."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-17-ProbSolutions-RF.html#solution-17.2.b",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-17-ProbSolutions-RF.html#solution-17.2.b",
    "title": "Chapter 17: Regression-Based Forecasting",
    "section": "Solution 17.2.b",
    "text": "Solution 17.2.b\nIf we computed the autocorrelation of this series, would the lag-1 autocorrelation exhibit negative, positive, or no autocorrelation? How can you see this from the plot?\nPositive autocorrelation - each succeeding value in the series stays fairly close to the previous value."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-17-ProbSolutions-RF.html#solution-17.2.c",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-17-ProbSolutions-RF.html#solution-17.2.c",
    "title": "Chapter 17: Regression-Based Forecasting",
    "section": "Solution 17.2.c",
    "text": "Solution 17.2.c\nCompute the autocorrelation of the series and produce an ACF plot. Verify your answer to the previous question.\n\ntsaplots.plot_acf(df_ts)\nplt.show()"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-17-ProbSolutions-RF.html#solution-17.3.a",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-17-ProbSolutions-RF.html#solution-17.3.a",
    "title": "Chapter 17: Regression-Based Forecasting",
    "section": "Solution 17.3.a",
    "text": "Solution 17.3.a\nFit a regression model with a linear trend and additive seasonality. Use the entire series (excluding the last two quarters) as the training set."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-17-ProbSolutions-RF.html#solution-17.3.b",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-17-ProbSolutions-RF.html#solution-17.3.b",
    "title": "Chapter 17: Regression-Based Forecasting",
    "section": "Solution 17.3.b",
    "text": "Solution 17.3.b\nA partial output of the regression model is shown in this Table (where C(Quarter)[T.2] is the Quarter 2 dummy).\n\ndf = tsatools.add_trend(df_ts, trend='ct')\ndf['Quarter'] = df.index.quarter\n\ntrain_df = df[:-2]\nvalid_df = df[-2:]\n\nlm = sm.ols(formula='Revenue ~ trend + C(Quarter)', data=train_df).fit()\nprint(pd.DataFrame({'coef': lm.params, 'std err': lm.bse}))\n\n# warning expected\nprint(lm.summary())\n\n                        coef     std err\nIntercept         906.750000  115.346119\nC(Quarter)[T.2]   -15.107143  119.659603\nC(Quarter)[T.3]    89.166667  128.673983\nC(Quarter)[T.4]  2101.726190  129.165419\ntrend              47.107143   11.256629\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                Revenue   R-squared:                       0.977\nModel:                            OLS   Adj. R-squared:                  0.967\nMethod:                 Least Squares   F-statistic:                     97.18\nDate:                Sun, 08 Mar 2020   Prob (F-statistic):           2.13e-07\nTime:                        20:21:13   Log-Likelihood:                -88.547\nNo. Observations:                  14   AIC:                             187.1\nDf Residuals:                       9   BIC:                             190.3\nDf Model:                           4                                         \nCovariance Type:            nonrobust                                         \n===================================================================================\n                      coef    std err          t      P&gt;|t|      [0.025      0.975]\n-----------------------------------------------------------------------------------\nIntercept         906.7500    115.346      7.861      0.000     645.819    1167.681\nC(Quarter)[T.2]   -15.1071    119.660     -0.126      0.902    -285.796     255.582\nC(Quarter)[T.3]    89.1667    128.674      0.693      0.506    -201.914     380.247\nC(Quarter)[T.4]  2101.7262    129.165     16.272      0.000    1809.534    2393.919\ntrend              47.1071     11.257      4.185      0.002      21.643      72.571\n==============================================================================\nOmnibus:                        5.015   Durbin-Watson:                   2.269\nProb(Omnibus):                  0.081   Jarque-Bera (JB):                2.414\nSkew:                          -0.166   Prob(JB):                        0.299\nKurtosis:                       5.007   Cond. No.                         36.0\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n/Users/gedeck/opt/anaconda3/envs/dmba-notebooks/lib/python3.8/site-packages/scipy/stats/stats.py:1534: UserWarning: kurtosistest only valid for n&gt;=20 ... continuing anyway, n=14\n  warnings.warn(\"kurtosistest only valid for n&gt;=20 ... continuing \"\n\n\n\nregressionSummary(valid_df['Revenue'], lm.predict(valid_df))\n\n\nRegression statistics\n\n                      Mean Error (ME) : 183.1429\n       Root Mean Squared Error (RMSE) : 313.6820\n            Mean Absolute Error (MAE) : 254.6667\n          Mean Percentage Error (MPE) : 3.0194\nMean Absolute Percentage Error (MAPE) : 7.4047\n\n\nUse this output to answer the following questions:\n\nSolution 17.3.b.i\nWhich two statistics (and their values) measure how well this model fits the training data?\nThe multiple R-squared (0.98) indicates what portion of the overall variation is explained by the regression, and the RMS Error (314) gives an idea of the magnitude of the typical error (regardless of direction).\n\n\nSolution 17.3.b.ii\nWhich two statistics (and their values) measure the predictive accuracy of this model?\nThe RMS error (314) gives an idea of the magnitude of the typical error, and the Mean Error (183) an idea of the overall average error (with errors in one direction netting out errors in the other).\n\n\nSolution 17.3.b.iii\nAfter adjusting for trend, what is the average difference between sales in Q3 and sales in Q1?\n\nq3q1_differences = (df_ts[2:].values - df_ts[:-2].values)[[0, 4, 8, 12]]\nprint('Differences: ', q3q1_differences)\nprint('mean difference: ', q3q1_differences.mean())\n\nDifferences:  [156 174 163 169]\nmean difference:  165.5\n\n\nThe actual Q3-Q1 differences are 156+174+163+169, the average is 165.5. \\$94 million of this is actually trend (the coefficient for trend is \\$47 million, so two quarter’s worth is \\$94 million), leaving \\$71 million as the average Q3-Q1 difference after adjusting for trend .\n\n\nSolution 17.3.b.iv\nAfter adjusting for seasonality, which quarter (\\(Q_1, Q_2, Q_3\\), or \\(Q_4\\)) has the highest average sales?\nAfter adjusting for seasonality, you are left with trend, and the trend is for increasing sales. Therefore, Q4, the last quarter, will have the highest average sales."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-17-ProbSolutions-RF.html#solution-17.4.a",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-17-ProbSolutions-RF.html#solution-17.4.a",
    "title": "Chapter 17: Regression-Based Forecasting",
    "section": "Solution 17.4.a",
    "text": "Solution 17.4.a\nFit an AR(1) model to the close price series. Report the coefficient table.\n\n# warning expected\nARIMA(ts, order=(1, 0, 0)).fit(disp=0).summary()\n\n/Users/gedeck/opt/anaconda3/envs/dmba-notebooks/lib/python3.8/site-packages/statsmodels/tsa/base/tsa_model.py:216: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n  warnings.warn('A date index has been provided, but it has no'\n\n\n\nARMA Model Results\n\n\nDep. Variable:\nWalmart\nNo. Observations:\n248\n\n\nModel:\nARMA(1, 0)\nLog Likelihood\n-349.796\n\n\nMethod:\ncss-mle\nS.D. of innovations\n0.987\n\n\nDate:\nSun, 08 Mar 2020\nAIC\n705.593\n\n\nTime:\n20:21:13\nBIC\n716.133\n\n\nSample:\n0\nHQIC\n709.836\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n52.9511\n1.328\n39.863\n0.000\n50.348\n55.555\n\n\nar.L1.Walmart\n0.9558\n0.019\n51.090\n0.000\n0.919\n0.993\n\n\n\n\n\n\nRoots\n\n\n\nReal\nImaginary\nModulus\nFrequency\n\n\nAR.1\n1.0462\n+0.0000j\n1.0462\n0.0000"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-17-ProbSolutions-RF.html#solution-17.4.b",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-17-ProbSolutions-RF.html#solution-17.4.b",
    "title": "Chapter 17: Regression-Based Forecasting",
    "section": "Solution 17.4.b",
    "text": "Solution 17.4.b\nWhich of the following is/are relevant for testing whether this stock is a random walk?\n\nThe autocorrelations of the close prices series\nThe AR(1) slope coefficient\nThe AR(1) constant coefficient\nThe autocorrelations of the close prices series - NO\nThe AR(1) slope coefficient - YES\nThe AR(1) constant coefficient - NO"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-17-ProbSolutions-RF.html#solution-17.4.c",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-17-ProbSolutions-RF.html#solution-17.4.c",
    "title": "Chapter 17: Regression-Based Forecasting",
    "section": "Solution 17.4.c",
    "text": "Solution 17.4.c\nDoes the AR model indicate that this is a random walk? Explain how you reached your conclusion.\n\ntsaplots.plot_acf(ts, lags=10)\ntsaplots.plot_acf(np.diff(ts), lags=10)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst, note that the ACF plot indicates no substantial autocorrelation for the differences. Fitting an AR(1) model to the close price series yields a slope coefficient of 0.956 with standard error 0.019. The slope coefficient is 2.3 standard errors away from the value 1. At a 5% significance level we’d say that this is not a random walk, but at 1% it is. In conclusion, the series appears to be close to a random walk."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-17-ProbSolutions-RF.html#solution-17.4.d",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-17-ProbSolutions-RF.html#solution-17.4.d",
    "title": "Chapter 17: Regression-Based Forecasting",
    "section": "Solution 17.4.d",
    "text": "Solution 17.4.d\nWhat are the implications of finding that a time-series is a random walk? Choose the correct statement(s) below.\n\nIt is impossible to obtain forecasts that are more accurate than naive forecasts for the series\nThe series is random\nThe changes in the series from one period to the next are random\nIt is impossible to obtain forecasts that are more accurate than naive forecasts for the series - YES (except for the naive prediction that yesterday’s price is a good predictor of today’s)\nThe series is random - NO (there is an underlying value around which there is noise (errors))\nThe changes in the series from one period to the next are random - YES"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-17-ProbSolutions-RF.html#solution-17.5.a",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-17-ProbSolutions-RF.html#solution-17.5.a",
    "title": "Chapter 17: Regression-Based Forecasting",
    "section": "Solution 17.5.a",
    "text": "Solution 17.5.a\nThe forecaster decided that there is an exponential trend in the series. In order to fit a regression-based model that accounts for this trend, which of the following operations must be performed?\n\nTake log of quarter index\nTake log of sales\nTake an exponent of sales\nTake an exponent of quarter index\nTake log of quarter index - NO\nTake log of sales - YES\nTake an exponent of sales - NO\nTake an exponent of quarter index - NO"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-17-ProbSolutions-RF.html#solution-17.5.b",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-17-ProbSolutions-RF.html#solution-17.5.b",
    "title": "Chapter 17: Regression-Based Forecasting",
    "section": "Solution 17.5.b",
    "text": "Solution 17.5.b\nFit a regression model with an exponential trend and seasonality, using the first 20 quarters as the training data (remember to first partition the series into training and validation series).\n\ndf = tsatools.add_trend(df_ts, trend='ct')\ndf['Quarter'] = [f'Q{(idx-1) % 4 + 1}' for idx in df.index]\n\ntrain_df = df[:20]\nvalid_df = df[20:]\n\nlm_expo = sm.ols(formula='np.log(Sales) ~ trend + Quarter', data=df[:20]).fit()"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-17-ProbSolutions-RF.html#solution-17.5.c",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-17-ProbSolutions-RF.html#solution-17.5.c",
    "title": "Chapter 17: Regression-Based Forecasting",
    "section": "Solution 17.5.c",
    "text": "Solution 17.5.c\nA partial output is shown in the Table. From the output, after adjusting for trend, are Q2 average sales higher, lower, or approximately equal to the average Q1 sales?\n\nprint(lm_expo.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:          np.log(Sales)   R-squared:                       0.979\nModel:                            OLS   Adj. R-squared:                  0.974\nMethod:                 Least Squares   F-statistic:                     175.9\nDate:                Sun, 08 Mar 2020   Prob (F-statistic):           2.08e-12\nTime:                        20:21:14   Log-Likelihood:                 42.865\nNo. Observations:                  20   AIC:                            -75.73\nDf Residuals:                      15   BIC:                            -70.75\nDf Model:                           4                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          t      P&gt;|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nIntercept        10.7489      0.019    574.057      0.000      10.709      10.789\nQuarter[T.Q2]     0.0250      0.021      1.202      0.248      -0.019       0.069\nQuarter[T.Q3]     0.1653      0.021      7.917      0.000       0.121       0.210\nQuarter[T.Q4]     0.4337      0.021     20.572      0.000       0.389       0.479\ntrend             0.0111      0.001      8.561      0.000       0.008       0.014\n==============================================================================\nOmnibus:                        0.809   Durbin-Watson:                   0.728\nProb(Omnibus):                  0.667   Jarque-Bera (JB):                0.424\nSkew:                           0.351   Prob(JB):                        0.809\nKurtosis:                       2.872   Cond. No.                         52.6\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nCoefficient of the Q_2 is 0.024, which is very low close to 0%. This implies that an average sale of Q2 is approximately equal to the average Q1 sales. Approximately equal (p-value is insignificant)"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-17-ProbSolutions-RF.html#solution-17.5.d",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-17-ProbSolutions-RF.html#solution-17.5.d",
    "title": "Chapter 17: Regression-Based Forecasting",
    "section": "Solution 17.5.d",
    "text": "Solution 17.5.d\nUse this model to forecast sales in quarters 21 and 22.\n\npredict_lm_expo = np.exp(lm_expo.predict(train_df))\nax = train_df.plot(y='Sales')\npredict_lm_expo.plot(ax=ax)\nvalid_df.plot(ax=ax, y='Sales', color='C0', linestyle='dashed')\nnp.exp(lm_expo.predict(valid_df)).plot(ax=ax, y='Sales', color='C1', linestyle='dashed')\nplt.show()\n\n\n\n\n\n\n\n\nPredictions for quarters 21 and 22\n\nnp.exp(lm_expo.predict(valid_df))[:2]\n\nQuarter\n21    58793.709570\n22    60951.509541\ndtype: float64\n\n\n\nregressionSummary(valid_df['Sales'][:2], np.exp(lm_expo.predict(valid_df))[:2])\n\n\nRegression statistics\n\n                      Mean Error (ME) : 2977.3904\n       Root Mean Squared Error (RMSE) : 3131.7550\n            Mean Absolute Error (MAE) : 2977.3904\n          Mean Percentage Error (MPE) : 4.6919\nMean Absolute Percentage Error (MAPE) : 4.6919"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-17-ProbSolutions-RF.html#solution-17.5.e",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-17-ProbSolutions-RF.html#solution-17.5.e",
    "title": "Chapter 17: Regression-Based Forecasting",
    "section": "Solution 17.5.e",
    "text": "Solution 17.5.e\nThe plots in the Figure describe the fit (top) and forecast errors (bottom) from this regression model.\n\nSolution 17.5.e.i\nRecreate these plots.\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    \n    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(8,6))\n    np.exp(lm_expo.predict(df[:20])).plot(ax=axes[0], color='C1', style='.-')\n    resid = df[:20].Sales - np.exp(lm_expo.predict(df[:20]))\n    resid.plot(ax=axes[1], color='C1', style='.-')\n    df[:20].Sales.plot(ax=axes[0], color='C0', style='.-')\n\n    axes[0].set_ylabel('Sales')\n    axes[1].set_ylabel('Residuals')\n\n    plt.show()\n\n\n\n\n\n\n\n\n\n\nSolution 17.5.e.ii\nBased on these plots, what can you say about your forecasts for quarters 21 and 22? Are they likely to over-forecast, under-forecast, or be reasonably close to the real sales values?\nBased on the U-shape of the residual plot, at Q20, you would expect Q21 and Q22 estimates to be too low (actual - forecast &gt; 0). If we look at the actual data (below), it turns out that the predictions were indeed too low, indicating that the fitted exponential trend is inadequate. From the above estimates for Q21 and Q22 we can see that the predicted forecast values for quarter 21 and 22 are below the actual values.\n\nvalid_df['Sales'][:2]\n\nQuarter\n21    60800\n22    64900\nName: Sales, dtype: int64"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-17-ProbSolutions-RF.html#solution-17.5.f",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-17-ProbSolutions-RF.html#solution-17.5.f",
    "title": "Chapter 17: Regression-Based Forecasting",
    "section": "Solution 17.5.f",
    "text": "Solution 17.5.f\nFrom the forecast errors plot, which of the following statements appear true?\n\nSeasonality is not captured well\nThe regression model fits the data well\nThe trend in the data is not captured well by the model\nSeasonality is not captured well - NO\nThe regression model fits the data well - NO\nThe trend in the data is not captured well by the model - YES"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-17-ProbSolutions-RF.html#solution-17.5.g",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-17-ProbSolutions-RF.html#solution-17.5.g",
    "title": "Chapter 17: Regression-Based Forecasting",
    "section": "Solution 17.5.g",
    "text": "Solution 17.5.g\nWhich of the following solutions is adequate and a parsimonious solution for improving model fit?\n\nFit a quadratic trend model to the residuals (with Quarter and Quarter\\(^2\\))\nFit an AR model to the residuals\nFit a quadratic trend model to Sales (with Quarter and Quarter\\(^2\\))\nFit a quadratic trend model to the residuals (with Quarter and Quarter\\(^2\\)) - NO\nFit an AR model to the residuals - NO\nFit a quadratic trend model to Sales (with Quarter and Quarter\\(^2\\)) - YES\n\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    \n    lm_quadratic = sm.ols(formula='np.log(Sales) ~ trend + np.square(trend) + Quarter', data=df[:20]).fit()\n    predict_lm_quadratic = np.exp(lm_quadratic.predict(df))\n    # ax = df_ts.plot()\n    # predict_lm_quadratic.plot(ax=ax)\n\n    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(10,9))\n    df[:20].Sales.plot(ax=axes[0], color='C0')\n    np.exp(lm_quadratic.predict(df[:20])).plot(ax=axes[0], color='C1')\n    resid = df[:20].Sales - np.exp(lm_quadratic.predict(df[:20]))\n    resid.plot(ax=axes[1], color='C1')\n\n    df[20:].Sales.plot(ax=axes[0], color='C0')\n    np.exp(lm_quadratic.predict(df[20:])).plot(ax=axes[0], color='C1')\n    resid = df[20:].Sales - np.exp(lm_quadratic.predict(df[20:]))\n    resid.plot(ax=axes[1], color='C1')\n\n    plt.show()"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-17-ProbSolutions-RF.html#solution-17.6.a",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-17-ProbSolutions-RF.html#solution-17.6.a",
    "title": "Chapter 17: Regression-Based Forecasting",
    "section": "Solution 17.6.a",
    "text": "Solution 17.6.a\nBased on the two time plots, which predictors should be included in the regression model? What is the total number of predictors in the model?\n12 predictors - the monthly index variable (trend), plus 11 seasonal variables (one for each month minus 1 - including all 12 would lead to multicollinearity errors in the regression)."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-17-ProbSolutions-RF.html#solution-17.6.b",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-17-ProbSolutions-RF.html#solution-17.6.b",
    "title": "Chapter 17: Regression-Based Forecasting",
    "section": "Solution 17.6.b",
    "text": "Solution 17.6.b\nRun a regression model with Sales (in Australian dollars) as the outcome variable, and with a linear trend and monthly seasonality. Remember to fit only the training data. Call this model A.\n\ndf = tsatools.add_trend(df_ts, trend='ct')\ndf['month'] = df.index.month\n\ntrain_df = df[:'2000-12-31']\nvalid_df = df['2001-01-01':]\n\nmodelA = sm.ols(formula='Sales ~ trend + C(month)', data=train_df).fit()\n\n\nSolution 17.6.b.i\nExamine the estimated coefficients: which month tends to have the highest average sales during the year? Why is this reasonable?\n\nprint(modelA.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  Sales   R-squared:                       0.790\nModel:                            OLS   Adj. R-squared:                  0.748\nMethod:                 Least Squares   F-statistic:                     18.53\nDate:                Sun, 08 Mar 2020   Prob (F-statistic):           9.44e-16\nTime:                        20:21:15   Log-Likelihood:                -720.48\nNo. Observations:                  72   AIC:                             1467.\nDf Residuals:                      59   BIC:                             1497.\nDf Model:                          12                                         \nCovariance Type:            nonrobust                                         \n==================================================================================\n                     coef    std err          t      P&gt;|t|      [0.025      0.975]\n----------------------------------------------------------------------------------\nIntercept      -3065.5544   2640.262     -1.161      0.250   -8348.706    2217.597\nC(month)[T.2]   1119.3842   3422.055      0.327      0.745   -5728.132    7966.901\nC(month)[T.3]   4408.8450   3422.564      1.288      0.203   -2439.690    1.13e+04\nC(month)[T.4]   1462.5675   3423.413      0.427      0.671   -5387.665    8312.800\nC(month)[T.5]   1446.1950   3424.600      0.422      0.674   -5406.414    8298.804\nC(month)[T.6]   1867.9775   3426.126      0.545      0.588   -4987.685    8723.640\nC(month)[T.7]   2988.5633   3427.990      0.872      0.387   -3870.830    9847.956\nC(month)[T.8]   3227.5808   3430.192      0.941      0.351   -3636.218    1.01e+04\nC(month)[T.9]   3955.5600   3432.731      1.152      0.254   -2913.320    1.08e+04\nC(month)[T.10]  4821.6574   3435.606      1.403      0.166   -2052.975    1.17e+04\nC(month)[T.11]  1.152e+04   3438.817      3.351      0.001    4643.581    1.84e+04\nC(month)[T.12]  3.247e+04   3442.362      9.432      0.000    2.56e+04    3.94e+04\ntrend            245.3642     34.083      7.199      0.000     177.165     313.564\n==============================================================================\nOmnibus:                       82.427   Durbin-Watson:                   0.634\nProb(Omnibus):                  0.000   Jarque-Bera (JB):             1266.965\nSkew:                           3.203   Prob(JB):                    7.62e-276\nKurtosis:                      22.526   Cond. No.                         524.\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nDecember - this is the holiday season (and the height of the summer in Australia).\n\n\nSolution 17.6.b.ii\nThe estimated trend coefficient in model A is 245.36. What does this mean?\nSeasonally adjusted sales increase by 240.1 Australian dollars per month\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10,4))\nfor sub_df in (train_df, valid_df):\n    sub_df.Sales.plot(ax=axes[0], color='C0')\n    modelA.predict(sub_df).plot(ax=axes[0], color='C1')\n    resid = sub_df.Sales - modelA.predict(sub_df)\n    resid.plot(ax=axes[1], color='C1')\n    \nplt.show()"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-17-ProbSolutions-RF.html#solution-17.6.c",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-17-ProbSolutions-RF.html#solution-17.6.c",
    "title": "Chapter 17: Regression-Based Forecasting",
    "section": "Solution 17.6.c",
    "text": "Solution 17.6.c\nRun a regression model with an exponential trend and multiplicative seasonality. Remember to fit only the training data. Call this model B.\n\nmodelB = sm.ols(formula='np.log(Sales) ~ trend + C(month)', data=train_df).fit()\nprint(modelB.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:          np.log(Sales)   R-squared:                       0.942\nModel:                            OLS   Adj. R-squared:                  0.931\nMethod:                 Least Squares   F-statistic:                     80.40\nDate:                Sun, 08 Mar 2020   Prob (F-statistic):           6.21e-32\nTime:                        20:21:16   Log-Likelihood:                 25.021\nNo. Observations:                  72   AIC:                            -24.04\nDf Residuals:                      59   BIC:                             5.554\nDf Model:                          12                                         \nCovariance Type:            nonrobust                                         \n==================================================================================\n                     coef    std err          t      P&gt;|t|      [0.025      0.975]\n----------------------------------------------------------------------------------\nIntercept          7.6464      0.084     90.898      0.000       7.478       7.815\nC(month)[T.2]      0.2820      0.109      2.587      0.012       0.064       0.500\nC(month)[T.3]      0.6950      0.109      6.374      0.000       0.477       0.913\nC(month)[T.4]      0.3739      0.109      3.428      0.001       0.156       0.592\nC(month)[T.5]      0.4217      0.109      3.865      0.000       0.203       0.640\nC(month)[T.6]      0.4470      0.109      4.095      0.000       0.229       0.665\nC(month)[T.7]      0.5834      0.109      5.341      0.000       0.365       0.802\nC(month)[T.8]      0.5469      0.109      5.004      0.000       0.328       0.766\nC(month)[T.9]      0.6356      0.109      5.811      0.000       0.417       0.854\nC(month)[T.10]     0.7295      0.109      6.664      0.000       0.510       0.949\nC(month)[T.11]     1.2010      0.110     10.961      0.000       0.982       1.420\nC(month)[T.12]     1.9522      0.110     17.800      0.000       1.733       2.172\ntrend              0.0211      0.001     19.449      0.000       0.019       0.023\n==============================================================================\nOmnibus:                        0.280   Durbin-Watson:                   1.036\nProb(Omnibus):                  0.869   Jarque-Bera (JB):                0.465\nSkew:                          -0.029   Prob(JB):                        0.793\nKurtosis:                       2.611   Cond. No.                         524.\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nSolution 17.6.c.i\nFitting a model to log(Sales) with a linear trend is equivalent to fitting a model to Sales (in dollars) with what type of trend?\nExponential trend\n\n\nSolution 17.6.c.ii\nThe estimated trend coefficient in model B is 0.02. What does this mean?\nSeasonally-adjusted sales increase by 2% per month on average.\n\n\nSolution 17.6.c.iii\nUse this model to forecast the sales in February 2002.\nIn order to predict February 2002, we need to rebuild a model with the full dataset\n\nmodelBfull = sm.ols(formula='np.log(Sales) ~ trend + C(month)', data=df).fit()\n\nfuture_df = pd.DataFrame({\n    'const': [1.0, 1.0],\n    'trend': [85.0, 86.0],\n    'month': [1, 2],\n}, index=['2002-01-01', '2002-02-01'])\nnp.exp(modelBfull.predict(future_df))\n\n2002-01-01    13484.062109\n2002-02-01    17724.450248\ndtype: float64"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-17-ProbSolutions-RF.html#solution-17.6.d",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-17-ProbSolutions-RF.html#solution-17.6.d",
    "title": "Chapter 17: Regression-Based Forecasting",
    "section": "Solution 17.6.d",
    "text": "Solution 17.6.d\nCompare the two regression models (A and B) in terms of forecast performance. Which model is preferable for forecasting? Mention at least two reasons based on the information in the outputs.\n\nregressionSummary(valid_df['Sales'], modelA.predict(valid_df))\nregressionSummary(valid_df['Sales'], np.exp(modelB.predict(valid_df)))\n\n\nRegression statistics\n\n                      Mean Error (ME) : 8251.5127\n       Root Mean Squared Error (RMSE) : 17451.5469\n            Mean Absolute Error (MAE) : 10055.2761\n          Mean Percentage Error (MPE) : 10.5340\nMean Absolute Percentage Error (MAPE) : 26.6657\n\nRegression statistics\n\n                      Mean Error (ME) : 4824.4941\n       Root Mean Squared Error (RMSE) : 7101.4442\n            Mean Absolute Error (MAE) : 5191.6695\n          Mean Percentage Error (MPE) : 12.3594\nMean Absolute Percentage Error (MAPE) : 15.5191\n\n\nFitting the natural log in model B damps the variability in the seasonal component so it is additive. It can now be modeled by dummy variables. Also the log makes the trend linear and suitable for linear regression. In model A we fit a linear trend to data that had a nonlinear trend."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-17-ProbSolutions-RF.html#solution-17.6.e",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-17-ProbSolutions-RF.html#solution-17.6.e",
    "title": "Chapter 17: Regression-Based Forecasting",
    "section": "Solution 17.6.e",
    "text": "Solution 17.6.e\nContinuing with model B, create an ACF plot until lag 15 for the forecast errors. Now fit an AR model with lag 2 [ARIMA(2,0,0)] to the forecast errors.\n\nax = tsaplots.plot_acf(modelB.resid, lags=15)\nplt.show()\n\n\n\n\n\n\n\n\n\narimaB = ARIMA(modelBfull.resid, order=(2, 0, 0), freq='MS').fit(disp=0)\narimaB.summary()\n\n\nARMA Model Results\n\n\nDep. Variable:\ny\nNo. Observations:\n84\n\n\nModel:\nARMA(2, 0)\nLog Likelihood\n46.278\n\n\nMethod:\ncss-mle\nS.D. of innovations\n0.139\n\n\nDate:\nSun, 08 Mar 2020\nAIC\n-84.556\n\n\nTime:\n20:21:16\nBIC\n-74.833\n\n\nSample:\n01-01-1995\nHQIC\n-80.647\n\n\n\n- 12-01-2001\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n-0.0026\n0.044\n-0.058\n0.954\n-0.089\n0.084\n\n\nar.L1.y\n0.3488\n0.103\n3.392\n0.001\n0.147\n0.550\n\n\nar.L2.y\n0.3182\n0.103\n3.089\n0.002\n0.116\n0.520\n\n\n\n\n\n\nRoots\n\n\n\nReal\nImaginary\nModulus\nFrequency\n\n\nAR.1\n1.3074\n+0.0000j\n1.3074\n0.0000\n\n\nAR.2\n-2.4034\n+0.0000j\n2.4034\n0.5000\n\n\n\n\n\n\nSolution 17.6.e.i\nExamining the ACF plot and the estimated coefficients of the AR(2) model (and their statistical significance), what can we learn about the forecasts that result from model B?\nThey should be adjusted to account for the autocorrelation at lags 1 and 2.\n\n\nSolution 17.6.e.i\nUse the autocorrelation information to compute an improved forecast for January 2002, using model B and the AR(2) model above.\n\nforecast, _, conf_int = arimaB.forecast(1)\narimaCorrection = forecast[0]\narimaCorrection\n\n0.06501310628340459\n\n\nThe untransformed prediction from modelBfull is\n\nmodelPrediction = modelBfull.predict(future_df)[0]\nmodelPrediction\n\n9.50926368247093\n\n\nWe can combine the prediction and the correction\n\nprint('corrected prediction: ', modelPrediction + arimaCorrection)\nprint('predicted sales: ', np.exp(modelPrediction + arimaCorrection))\n\ncorrected prediction:  9.574276788754334\npredicted sales:  14389.827160919325"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-17-ProbSolutions-RF.html#solution-17.6.f",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-17-ProbSolutions-RF.html#solution-17.6.f",
    "title": "Chapter 17: Regression-Based Forecasting",
    "section": "Solution 17.6.f",
    "text": "Solution 17.6.f\nHow would you model these data differently if the goal was to understand the different components of sales in the souvenir shop between 1995–2001? Mention two differences.\nNo data partitioning. Evaluate model on the basis of data fit, not forecast error. Since Model B is the more accurate model we can go ahead and fit the model to all of the data, then generate the residuals for all the data, then fit an ARIMA model to the residuals which will then be used to adjust the forecast. We do not have a trend showing in the residuals since it has been taken out by the regression model. We have significant autocorrelation at lags 1 and 2, so we fit an ARIMA (2,0,0) to the residuals."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-17-ProbSolutions-RF.html#solution-17.8.a",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-17-ProbSolutions-RF.html#solution-17.8.a",
    "title": "Chapter 17: Regression-Based Forecasting",
    "section": "Solution 17.8.a",
    "text": "Solution 17.8.a\nWhich forecasting method would you choose if you had to choose the same method for all series? Why?\nModel/Method: Holt-Winter’s exponential smoothing Reasons: 1. Ability of the model to capture both seasonality and trend that are present in the various data series. 2. Adaptability of the model - will adapt to the various patterns of different data series 3. Holt-Winter’s model is a reasonable model for all series - it will be able to capture and adjust to changing levels of trends and seasonality swings. It is parsimonious and should produce good fit as well as good forecasts"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-17-ProbSolutions-RF.html#solution-17.8.b",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-17-ProbSolutions-RF.html#solution-17.8.b",
    "title": "Chapter 17: Regression-Based Forecasting",
    "section": "Solution 17.8.b",
    "text": "Solution 17.8.b\nFortified wine has the largest market share of the above six types of wine. You are asked to focus on fortified wine sales alone, and produce as accurate as possible forecasts for the next 2 months.\n\nStart by partitioning the data: use the period until December 1993 as the training set.\nFit a regression model to sales with a linear trend and additive seasonality.\n\n\ndf = pd.read_csv(DATA / 'AustralianWines.csv', na_values=['*'])\ndf['Date'] = pd.to_datetime(df.Month, format='%b-%y')\n\ndf = tsatools.add_trend(pd.Series(df['Fortified'].values, index=df.Date, name='fortified'), trend='ct')\ndf['month'] = df.index.month\n\ntrain_df = df[:'1993-12-31']\nvalid_df = df['1994-01-01':]\nax = train_df.plot(y=['fortified'])\nvalid_df.plot(y=['fortified'], ax=ax)\nplt.show()\n# df.head()\n\n\n\n\n\n\n\n\n\nmodel = sm.ols(formula='fortified ~ trend + C(month)', data=train_df).fit()\nprint(model.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:              fortified   R-squared:                       0.891\nModel:                            OLS   Adj. R-squared:                  0.883\nMethod:                 Least Squares   F-statistic:                     105.6\nDate:                Sun, 08 Mar 2020   Prob (F-statistic):           4.09e-68\nTime:                        20:21:18   Log-Likelihood:                -1188.7\nNo. Observations:                 168   AIC:                             2403.\nDf Residuals:                     155   BIC:                             2444.\nDf Model:                          12                                         \nCovariance Type:            nonrobust                                         \n==================================================================================\n                     coef    std err          t      P&gt;|t|      [0.025      0.975]\n----------------------------------------------------------------------------------\nIntercept       2679.6545     88.043     30.436      0.000    2505.735    2853.574\nC(month)[T.2]    361.1331    112.625      3.207      0.002     138.656     583.611\nC(month)[T.3]    791.9804    112.628      7.032      0.000     569.497    1014.464\nC(month)[T.4]   1048.3992    112.633      9.308      0.000     825.906    1270.893\nC(month)[T.5]   1619.3894    112.640     14.377      0.000    1396.882    1841.897\nC(month)[T.6]   1691.2367    112.649     15.013      0.000    1468.712    1913.762\nC(month)[T.7]   2410.0126    112.660     21.392      0.000    2187.466    2632.559\nC(month)[T.8]   2116.2171    112.673     18.782      0.000    1893.645    2338.790\nC(month)[T.9]   1115.7073    112.688      9.901      0.000     893.105    1338.310\nC(month)[T.10]   922.1975    112.705      8.182      0.000     699.562    1144.833\nC(month)[T.11]  1373.6163    112.724     12.186      0.000    1150.943    1596.290\nC(month)[T.12]  1583.1779    112.745     14.042      0.000    1360.463    1805.893\ntrend            -10.4188      0.475    -21.923      0.000     -11.358      -9.480\n==============================================================================\nOmnibus:                        9.128   Durbin-Watson:                   1.677\nProb(Omnibus):                  0.010   Jarque-Bera (JB):                9.178\nSkew:                           0.494   Prob(JB):                       0.0102\nKurtosis:                       3.578   Cond. No.                     1.21e+03\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 1.21e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\n\nSolution 17.8.b.i\nCreate the ‘actual vs. forecast’ plot. What can you say about the model fit?\n\n# train_df = df[:'1993-12-31']\n# valid_df = df['1994-01-01':]\nax = train_df.plot(y=['fortified'], color='C0')\nvalid_df.plot(y=['fortified'], ax=ax, color='C0', linestyle='dashed')\nmodel.predict(train_df).plot(y=['fortified'], ax=ax, color='C1')\nmodel.predict(valid_df).plot(y=['fortified'], ax=ax, color='C1', linestyle='dashed')\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nSolution 17.8.b.ii\nUse the regression model to forecast sales in January and February 1994.\n\nmodel.predict(valid_df)[:2]\n\nDate\n1994-01-01     918.881868\n1994-02-01    1269.596154\ndtype: float64"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-17-ProbSolutions-RF.html#solution-17.8.c",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-17-ProbSolutions-RF.html#solution-17.8.c",
    "title": "Chapter 17: Regression-Based Forecasting",
    "section": "Solution 17.8.c",
    "text": "Solution 17.8.c\nCreate an ACF plot for the residuals from the above model until lag 12.\n\ntsaplots.plot_acf(model.resid, lags=12)\nplt.show()\n\n\n\n\n\n\n\n\nExamining this plot (only), which of the following statements are reasonable conclusions?\n\nDecembers (month 12) are not captured well by the model.\nThere is a strong correlation between sales on the same calendar month.\nThe model does not capture the seasonality well.\nWe should try to fit an autoregressive model with lag 12 to the residuals.\nDecembers (month 12) are not captured well by the model - NO (don’t forget, the “12” in the ACF plot refers to lag of 12 months, not to December!)\nThere is a strong correlation between sales on the same calendar month - YES (this is what the high coefficient for lag 12 means\nThe model does not capture the seasonality well - YES (this follows from the above)\nWe should try to fit an autoregressive model with lag 12 to the residuals - YES"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-12-ProbSolutions-DA.html",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-12-ProbSolutions-DA.html",
    "title": "Chapter 12: Discriminant Analysis",
    "section": "",
    "text": "2019-2020 Galit Shmueli, Peter C. Bruce, Peter Gedeck\n\nData Mining for Business Analytics: Concepts, Techniques, and Applications in Python (First Edition) Galit Shmueli, Peter C. Bruce, Peter Gedeck, and Nitin R. Patel. 2019.\nDate: 2020-03-08\nPython Version: 3.8.2 Jupyter Notebook Version: 5.6.1\nPackages: - dmba: 0.0.12 - matplotlib: 3.2.0 - numpy: 1.18.1 - pandas: 1.0.1 - scikit-learn: 0.22.2\nThe assistance from Mr. Kuber Deokar and Ms. Anuja Kulkarni in preparing these solutions is gratefully acknowledged.\n# Import required packages for this chapter\nfrom pathlib import Path\n\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.model_selection import train_test_split\n\nimport matplotlib.pylab as plt\n\nfrom dmba import classificationSummary\nfrom dmba import gainsChart, liftChart\n\n%matplotlib inline\n# Working directory:\n#\n# We assume that data are kept in the same directory as the notebook. If you keep your \n# data in a different folder, replace the argument of the `Path`\nDATA = Path('.')\n# and then load data using \n#\n# pd.read_csv(DATA / ‘filename.csv’)"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-12-ProbSolutions-DA.html#problem-12.1.a",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-12-ProbSolutions-DA.html#problem-12.1.a",
    "title": "Chapter 12: Discriminant Analysis",
    "section": "Problem 12.1.a",
    "text": "Problem 12.1.a\nCompute summary statistics for the predictors separately for loan acceptors and nonacceptors. For continuous predictors, compute the mean and standard deviation. For categorical predictors, compute the percentages. Are there predictors where the two classes differ substantially?\n\nbank_df[y==1].describe().loc[['mean', 'std'],:]\n\n\n\n\n\n\n\n\nAge\nExperience\nIncome\nFamily\nCCAvg\nEducation\nMortgage\nPersonal Loan\nSecurities Account\nCD Account\nOnline\nCreditCard\n\n\n\n\nmean\n45.066667\n19.843750\n144.745833\n2.612500\n3.905354\n2.233333\n100.845833\n1.0\n0.125000\n0.291667\n0.60625\n0.297917\n\n\nstd\n11.590964\n11.582443\n31.584429\n1.115393\n2.097681\n0.753373\n160.847862\n0.0\n0.331064\n0.455004\n0.48909\n0.457820\n\n\n\n\n\n\n\n\nbank_df[y==0].describe().loc[['mean', 'std'],:]\n\n\n\n\n\n\n\n\nAge\nExperience\nIncome\nFamily\nCCAvg\nEducation\nMortgage\nPersonal Loan\nSecurities Account\nCD Account\nOnline\nCreditCard\n\n\n\n\nmean\n45.367257\n20.132301\n66.237389\n2.373451\n1.729009\n1.843584\n51.789381\n0.0\n0.102212\n0.035841\n0.595796\n0.293584\n\n\nstd\n11.450427\n11.456672\n40.578534\n1.148771\n1.567647\n0.839975\n92.038931\n0.0\n0.302961\n0.185913\n0.490792\n0.455454\n\n\n\n\n\n\n\nFor the predictor “income” the mean is much higher for acceptors than for nonacceptors. Similarly for “CCAverage” and “Mortgage,” the mean is higher for acceptors than for nonacceptors.\nCategorical predictors: “acceptors” are more likely to have higher level degrees (Education levels 2 and 3, graduate and professional), and more likely to have CD accounts."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-12-ProbSolutions-DA.html#problem-12.1.b",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-12-ProbSolutions-DA.html#problem-12.1.b",
    "title": "Chapter 12: Discriminant Analysis",
    "section": "Problem 12.1.b",
    "text": "Problem 12.1.b\nExamine the model performance on the validation set. ## Problem 12.1.b.i What is the accuracy rate?\n\nclassificationSummary(valid_y, da_class.predict(valid_X))\n\nConfusion Matrix (Accuracy 0.9480)\n\n       Prediction\nActual    0    1\n     0 1777   30\n     1   74  119\n\n\nThe accuracy of the discriminant analysis model is 94.8%."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-12-ProbSolutions-DA.html#problem-12.1.b.ii",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-12-ProbSolutions-DA.html#problem-12.1.b.ii",
    "title": "Chapter 12: Discriminant Analysis",
    "section": "Problem 12.1.b.ii",
    "text": "Problem 12.1.b.ii\nIs one type of misclassification more likely than the other?\nYes - the number of cases of actual acceptors classified as nonacceptors is much higher than the number of cases of nonacceptors classified as acceptors."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-12-ProbSolutions-DA.html#problem-12.1.b.iii",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-12-ProbSolutions-DA.html#problem-12.1.b.iii",
    "title": "Chapter 12: Discriminant Analysis",
    "section": "Problem 12.1.b.iii",
    "text": "Problem 12.1.b.iii\nSelect three customers who were misclassified as acceptors and three who were misclassified as nonacceptors. The goal is to determine why they are misclassified. First, examine their probability of being classified as acceptors: is it close to the threshold of 0.5? If not, compare their predictor values to the summary statistics of the two classes to determine why they were misclassified.\n\n# combine valid_X with actual value (valid_y) and predicted probability\nsuccess_probability = da_class.predict_proba(valid_X)[:,1]\nmergedData = pd.concat(\n    [\n        valid_X, \n        pd.DataFrame({'actual': valid_y}, index=valid_X.index),\n        pd.DataFrame({'prob': success_probability}, index=valid_X.index),\n    ], axis=1, ignore_index=False, sort=False)  \nmergedData = mergedData.sort_values(by='prob')\n\n# loan acceptors misclassified as non-acceptors\nmis_acceptors = mergedData[mergedData.actual == 1].head(3)\nmis_acceptors\n\n\n\n\n\n\n\n\nAge\nExperience\nIncome\nFamily\nCCAvg\nMortgage\nSecurities Account\nCD Account\nOnline\nCreditCard\nEducation_2\nEducation_3\nactual\nprob\n\n\n\n\n1577\n34\n8\n65\n1\n3.0\n227\n0\n0\n1\n0\n0\n0\n1\n0.000263\n\n\n349\n26\n2\n60\n2\n3.0\n132\n0\n0\n0\n0\n0\n0\n1\n0.000602\n\n\n2158\n50\n25\n83\n4\n3.1\n0\n0\n0\n0\n1\n0\n0\n1\n0.003128\n\n\n\n\n\n\n\nThe probabilities are less than 0.5, many not even close to that threshold. Looking at customer ID 1577, we can see that their profile is similar to that of a nonacceptor. They have few existing lines of business with the bank (no securities account, no CD account, no credit card.). This is a case of the customer not behaving in the way that similar customers behave.\n\n# loan nonacceptors misclassified as acceptors\nmis_nonacceptors = mergedData[mergedData.actual == 0].tail(3)\nmis_nonacceptors\n\n\n\n\n\n\n\n\nAge\nExperience\nIncome\nFamily\nCCAvg\nMortgage\nSecurities Account\nCD Account\nOnline\nCreditCard\nEducation_2\nEducation_3\nactual\nprob\n\n\n\n\n4937\n33\n8\n162\n1\n8.6\n0\n0\n1\n1\n1\n0\n0\n0\n0.980754\n\n\n785\n46\n22\n164\n2\n7.6\n0\n0\n1\n1\n1\n0\n0\n0\n0.990423\n\n\n2305\n32\n7\n185\n2\n6.7\n0\n0\n1\n1\n1\n0\n0\n0\n0.995511\n\n\n\n\n\n\n\nProbabilities are greater than 0.5, and not close to the threshold. Looking at the customer for ID 2305, we see that he has the demographic profile of an acceptor for most relevant variables (income, ccaverage). He is missing some of the “doing business with the bank already” attributes (Securities Acccount), but, overall, the customer profile is that of an acceptor, so it is a case of the customer not behaving in the way that similar customers behave."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-12-ProbSolutions-DA.html#problem-12.1.c",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-12-ProbSolutions-DA.html#problem-12.1.c",
    "title": "Chapter 12: Discriminant Analysis",
    "section": "Problem 12.1.c",
    "text": "Problem 12.1.c\nAs in many marketing campaigns, it is more important to identify customers who will accept the offer rather than customers who will not accept it. Therefore, a good model should be especially accurate at detecting acceptors. Examine the lift chart and decile-wise lift chart for the validation set and interpret them in light of this ranking goal.\n\nproba = da_class.predict_proba(valid_X)\nresult = pd.DataFrame({'actual': valid_y, \n                       'p(0)': [p[0] for p in proba],\n                       'p(1)': [p[1] for p in proba],\n                       'predicted': da_class.predict(valid_X) })\nresult = result.sort_values(by=['p(1)'], ascending=False)\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8, 4))\nax = gainsChart(result.actual, ax=axes[0])\nax.set_ylabel('Cumulative Gains')\nax.set_xlabel('#cases')\nax.set_title('Cumulative Gains Chart')\n\nax = liftChart(result.actual, ax=axes[1], labelBars=True)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nUsing the lift chart we can descend through the customers in order of probability of being a loan acceptor, knowing at each point how effective we have been in “skimming the cream” (acceptors). The decile chart is similar, differing only in that it proceeds in chunks of 10% of the data. From the decile chart we can see that taking the 10% of the records that are ranked by the model as “most probable acceptors” yields 7.5 times as many “acceptors” as would simply selecting 10% of the records at random."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-12-ProbSolutions-DA.html#problem-12.1.d",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-12-ProbSolutions-DA.html#problem-12.1.d",
    "title": "Chapter 12: Discriminant Analysis",
    "section": "Problem 12.1.d",
    "text": "Problem 12.1.d\nCompare the results from the discriminant analysis with those from a logistic regression (both with cutoff 0.5 and the same predictors). Examine the confusion matrices, the lift charts, and the decile charts. Which method performs better on your validation set in detecting the acceptors?\nWe can take the confusion matrix from the logistic regression model in chapter 10.\nConfusion Matrix (Accuracy 0.9595)\n\n       Prediction\nActual    0    1\n     0 1791   16\n     1   65  128\nDiscriminant analysis correctly classifies 115 of the acceptors, logistic regression 128. Therefore the logistic regression’s error rate in detecting acceptors is lower than that of discriminant analysis.\nDecile chart of logistic regression: taking 10% of the records that are ranked by the model as “most probable acceptors” yields 7.8 times as many “acceptors” as would simply selecting 10% of the records at random. Decile chart of discriminant analysis: taking 10% of the records that are ranked by the model as “most probable acceptors” yields 7.5 times as many “acceptors” as would simply selecting 10% of the records at random. Therefore logistic regression performs slightly better than discriminant analysis in detecting acceptors for validation data."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-12-ProbSolutions-DA.html#problem-12.1.e",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-12-ProbSolutions-DA.html#problem-12.1.e",
    "title": "Chapter 12: Discriminant Analysis",
    "section": "Problem 12.1.e",
    "text": "Problem 12.1.e\nThe bank is planning to continue its campaign by sending its offer to 1000 additional customers. Suppose that the cost of sending the offer is \\$1 and the profit from an accepted offer is \\$50. What is the expected profitability of this campaign?\n\nprint(train_y.value_counts())\n\nprint('probability of accepting the loan', 287 / (287 + 2713))\nprint('expected profitability ', 96 * 50 - 1000)\n\n0    2713\n1     287\nName: Personal Loan, dtype: int64\nprobability of accepting the loan 0.09566666666666666\nexpected profitability  3800\n\n\nThe probability of acceptance of a loan = 0.0957 (prior class probability). Out of 1000 customers 9.57% (that is 96) can be expected to accept the loan offer.\nProfit from each accepted offer = $50.\nTotal expected profitability is:\nprofit from these 101 customers - the cost of sending an offer.\nTherefore total expected profitability = 96*50 - 1000 = 3800\nExpected profitability from this campaign is \\$3850."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-12-ProbSolutions-DA.html#problem-12.1.f",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-12-ProbSolutions-DA.html#problem-12.1.f",
    "title": "Chapter 12: Discriminant Analysis",
    "section": "Problem 12.1.f",
    "text": "Problem 12.1.f\nThe cost of misclassifying a loan acceptor customer as a nonacceptor is much higher than the opposite misclassification cost. To minimize the expected cost of misclassification, should the cutoff value for classification (which is currently at 0.5) be increased or decreased?\nIf the cutoff were lowered, then we would classify more “non acceptors” cases as “acceptors” cases (more zeros misclassified as 1).\nThis means that the numerator in the classification error rate for truly completed will be less than or equal to b. Therefore, lowering the cutoff will lead to a reduced (or equal) classification error rate for truly acceptor cases.\nTherefore, if the cutoff is lowered, then\n\nIt will classify more nonacceptors as acceptors.\nIt will minimize the expected cost of misclassification."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-12-ProbSolutions-DA.html#problem-12.2.a",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-12-ProbSolutions-DA.html#problem-12.2.a",
    "title": "Chapter 12: Discriminant Analysis",
    "section": "Problem 12.2.a",
    "text": "Problem 12.2.a\nCreate a scatter plot of Experience vs. Training using color or symbol to differentiate administrators who completed the tasks from those who did not complete them. See if you can identify a line that separates the two classes with minimum misclassification.\n\nax = data[data['Completed task'] == 'Yes'].plot.scatter(x='Training', y='Experience', c='none',\n                                                        facecolors='none', edgecolors='C0', s=10)\ndata[data['Completed task'] == 'No'].plot.scatter(x='Training', y='Experience', c='none',\n                                                  facecolors='none', edgecolors='C1', s=10, ax=ax)\nplt.show()\n\n\n\n\n\n\n\n\nFrom the scatterplot we can observe that programmers who completed the task tend to have more experience. Training, however, does not play much a role in task completion. Therefore, the predictor Experience appears more useful for classifying task completion."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-12-ProbSolutions-DA.html#problem-12.2.b",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-12-ProbSolutions-DA.html#problem-12.2.b",
    "title": "Chapter 12: Discriminant Analysis",
    "section": "Problem 12.2.b",
    "text": "Problem 12.2.b\nRun a discriminant analysis with both predictors using the entire dataset as training data. Among those who completed the tasks, what is the percentage of administrators who are classified incorrectly as failing to complete the tasks?\n\npredictors = ['Training', 'Experience']\noutcome = 'Completed task'\n\nX = data[predictors]\ny = data[outcome]\n\nda = LinearDiscriminantAnalysis()\nda.fit(X, y)\n\nclassificationSummary(y, da.predict(X))\n\nConfusion Matrix (Accuracy 0.9067)\n\n       Prediction\nActual  0  1\n     0 58  2\n     1  5 10\n\n\nAmong 15 administrators who completed the task, 5 are misclassified as failing to complete the task.\nTherefore, the percentage of administrators who completed the task but were misclassified as failing to complete the task is 33.33%."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-12-ProbSolutions-DA.html#problem-12.2.c",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-12-ProbSolutions-DA.html#problem-12.2.c",
    "title": "Chapter 12: Discriminant Analysis",
    "section": "Problem 12.2.c",
    "text": "Problem 12.2.c\nCompute the two classification scores for an administrator with 4 months of experience and six credits of training. Based on these, how would you classify this administrator?\n\nadmin = pd.DataFrame([{'Training': 6.0, 'Experience': 4}])\nprint(da.predict(admin[predictors]))\nda.predict_proba(admin[predictors])\n\n['No']\n\n\narray([[9.99052428e-01, 9.47571894e-04]])\n\n\nThe classification score for class “No” is more than that for class “Yes”. Therefore, we classify the administrator as “fail to complete the task.”"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-12-ProbSolutions-DA.html#problem-12.2.d",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-12-ProbSolutions-DA.html#problem-12.2.d",
    "title": "Chapter 12: Discriminant Analysis",
    "section": "Problem 12.2.d",
    "text": "Problem 12.2.d\nHow much experience must be accumulated by an administrator with four training credits before his or her estimated probability of completing the tasks exceeds 0.5?\n\nfor experience in range(3, 14):\n    admin = pd.DataFrame([{'Training': 4, 'Experience': experience}])\n    print(f'Experience {experience}: {da.predict_proba(admin[predictors])[0][1]:.3f}')\n\nExperience 3: 0.000\nExperience 4: 0.001\nExperience 5: 0.003\nExperience 6: 0.013\nExperience 7: 0.051\nExperience 8: 0.187\nExperience 9: 0.494\nExperience 10: 0.805\nExperience 11: 0.946\nExperience 12: 0.987\nExperience 13: 0.997\n\n\nhe administrator must accumulate more than 9 years experience with 4 training credits so that his/her estimated probability of completing the task exceeds 0.5."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-12-ProbSolutions-DA.html#problem-12.2.e",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-12-ProbSolutions-DA.html#problem-12.2.e",
    "title": "Chapter 12: Discriminant Analysis",
    "section": "Problem 12.2.e",
    "text": "Problem 12.2.e\nCompare the classification accuracy of this model to that resulting from a logistic regression with cutoff 0.5.\n\nlogit_reg = LogisticRegression(penalty=\"l2\", C=1e42, solver='liblinear')\nlogit_reg.fit(X, y)\nclassificationSummary(y, logit_reg.predict(X))\n\nConfusion Matrix (Accuracy 0.9067)\n\n       Prediction\nActual  0  1\n     0 58  2\n     1  5 10\n\n\nAmong 15 administrators who completed task, 5 are misclassified as failing to complete the task. Therefore percentage of administrators who completed the task but were misclassified as failing to complete the task is 33.33%.\nTherefore, the results from both discriminant analysis and logistic regression are the same."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-12-ProbSolutions-DA.html#problem-12.3.a",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-12-ProbSolutions-DA.html#problem-12.3.a",
    "title": "Chapter 12: Discriminant Analysis",
    "section": "Problem 12.3.a",
    "text": "Problem 12.3.a\nTo reduce the number of predictors to a manageable size, examine how each predictor differs between the spam and nonspam e-mails by comparing the spam-class average and nonspam-class average. Which are the 11 predictors that appear to vary the most between spam and nonspam e-mails? From these 11, which words or signs occur more often in spam?\n\ndata = pd.read_csv(DATA / 'Spambase.csv')\ndata.head()\ndata.columns\n\nIndex(['make', 'address', 'all', 'W_3d', 'our', 'over', 'remove', 'internet',\n       'order', 'mail', 'receive', 'will ', 'people', 'report', 'addresses',\n       'free', 'business', 'email', 'you ', 'credit', 'your', 'font', 'W_000',\n       'money', 'hp', 'hpl', 'george', 'W_650', 'lab', 'labs', 'telnet',\n       'W_857', 'data', 'W_415', 'W_85', 'technology', 'W_1999', 'parts', 'pm',\n       'direct', 'cs', 'meeting', 'original', 'project ', 're:', 'edu',\n       'table ', 'conference', 'C;', 'C(', 'C[', 'C!', 'C$', 'C#', 'CAP_avg',\n       'CAP_long', 'CAP_tot', 'Spam'],\n      dtype='object')\n\n\n\nspamData = data[data.Spam == 1]\nnonspamData = data[data.Spam == 0]\n\n# determine absolute difference between spam-class average and nonspam-class average for each of the variables\ndifference = spamData.mean() - nonspamData.mean()\nprint(difference.sort_values(ascending=False).head(9))\nabsDifference = abs(difference)\n\n# Get the 10 values with the largest absolute difference in mean\nabsDifference.sort_values(ascending=False).head(12)\n\nCAP_tot     309.148468\nCAP_long     86.178780\nCAP_avg       7.141864\nSpam          1.000000\nyou           0.994199\nyour          0.941668\nfree          0.444775\nC!            0.403729\nour           0.332915\ndtype: float64\n\n\nCAP_tot     309.148468\nCAP_long     86.178780\nCAP_avg       7.141864\ngeorge        1.263716\nSpam          1.000000\nyou           0.994199\nyour          0.941668\nhp            0.877994\nfree          0.444775\nhpl           0.422822\nC!            0.403729\nour           0.332915\ndtype: float64\n\n\nThe following 11 predictors appear to vary most between spam and non-spam. &gt; our, C!, hpl, free, hp, your, you, george, CAP_avg, CAP_long, CAP_tot\nWhich of these occur more often in spam? &gt; our, C!, free, your, you, CAP_avg, CAP_long, CAP_tot"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-12-ProbSolutions-DA.html#problem-12.3.b",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-12-ProbSolutions-DA.html#problem-12.3.b",
    "title": "Chapter 12: Discriminant Analysis",
    "section": "Problem 12.3.b",
    "text": "Problem 12.3.b\nPartition the data into training and validation sets, then perform a discriminant analysis on the training data using only the 11 predictors.\n\npredictors = ['our', 'C!', 'hpl', 'free', 'hp', 'your', 'you ', 'george', 'CAP_avg', 'CAP_long', 'CAP_tot']\noutcome = 'Spam'\n\n# create training and validation sets\nX = data[predictors]\ny = data[outcome]\n\ntrain_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size=0.4, random_state=1)\nprint('Training set:', train_X.shape, 'Validation set:', valid_X.shape)\n\nda_class = LinearDiscriminantAnalysis()\nda_class.fit(train_X, train_y)\n\nTraining set: (2760, 11) Validation set: (1841, 11)\n\n\nLinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n                           solver='svd', store_covariance=False, tol=0.0001)"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-12-ProbSolutions-DA.html#problem-12.3.c",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-12-ProbSolutions-DA.html#problem-12.3.c",
    "title": "Chapter 12: Discriminant Analysis",
    "section": "Problem 12.3.c",
    "text": "Problem 12.3.c\nIf we are interested mainly in detecting spam messages, is this model useful? Use the confusion matrix, lift chart, and decile chart for the validation set for the evaluation.\n\nclassificationSummary(valid_y, da_class.predict(valid_X))\nprint(f'{239 + 504} spam message, error rate {239 / (504 + 239) * 100:.2f}')\n\nConfusion Matrix (Accuracy 0.8332)\n\n       Prediction\nActual    0    1\n     0 1030   68\n     1  239  504\n743 spam message, error rate 32.17\n\n\nAmong the 743 spam messages, 239 are misclassified as non-spam. Therefore the error rate is 32.17%, which is moderately high.\n\nproba = da_class.predict_proba(valid_X)\nresult = pd.DataFrame({'actual': valid_y, \n                       'p(0)': [p[0] for p in proba],\n                       'p(1)': [p[1] for p in proba],\n                       'predicted': da_class.predict(valid_X) })\nresult = result.sort_values(by=['p(1)'], ascending=False)\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8, 4))\nax = gainsChart(result.actual, ax=axes[0])\nax.set_ylabel('Cumulative Gains')\nax.set_xlabel('#cases')\nax.set_title('Cumulative Gains Chart')\n\nax = liftChart(result.actual, ax=axes[1], labelBars=True)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFrom the decile chart: if we take 10% of the records that are ranked by the model as “most probable spam”, it captures 2.3 times more “spam” messages than simply selecting 10% of the records at random."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-12-ProbSolutions-DA.html#problem-12.3.d",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-12-ProbSolutions-DA.html#problem-12.3.d",
    "title": "Chapter 12: Discriminant Analysis",
    "section": "Problem 12.3.d",
    "text": "Problem 12.3.d\nIn the sample, almost 40% of the e-mail messages were tagged as spam. However, suppose that the actual proportion of spam messages in these e-mail accounts is 10%. Compute the constants of the classification functions to account for this information.\nThe constant in the classification function of da_class is given as:\n\nprint(f'The constant of the classification function is: {da_class.intercept_}')\nprint(f'The priors of the classification function are: {da_class.priors_}')\n\nThe constant of the classification function is: [-2.07053468]\nThe priors of the classification function are: [0.61231884 0.38768116]\n\n\nIf we want to calculate the constant for a different prior, we need to determine first the log-differences in the priors and then adjust the constant.\n\nlog_priors = np.log(da_class.priors_)\nlog_priors_10 = np.log([0.9, 0.1])\ndiff_log_priors = log_priors[1] - log_priors[0]\ndiff_log_priors_10 = log_priors_10[1] - log_priors_10[0]\nprint(f'Difference of log-prior of classification function: {diff_log_priors}')\nprint(f'Difference of log-prior of classification function for new priors: {diff_log_priors_10}')\n\nadjusted_intercept = da_class.intercept_ - diff_log_priors + diff_log_priors_10\nprint(f'Adjusted intercept for new priors: {adjusted_intercept}')\n\nDifference of log-prior of classification function: -0.45706988046116737\nDifference of log-prior of classification function for new priors: -2.197224577336219\nAdjusted intercept for new priors: [-3.81068937]\n\n\nThe LinearDiscriminantAnalysis implementation in scikit-learn allows specifying the priors directly. We can confirm the correctness of our calculation above using this.\n\nda_class_10 = LinearDiscriminantAnalysis(priors = [0.9, 0.1])\nda_class_10.fit(train_X, train_y)\nclassificationSummary(valid_y, da_class_10.predict(valid_X))\n\nprint(da_class_10.intercept_)\n\nConfusion Matrix (Accuracy 0.6594)\n\n       Prediction\nActual    0    1\n     0 1086   12\n     1  615  128\n[-3.81068937]"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-12-ProbSolutions-DA.html#problem-12.3.e",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-12-ProbSolutions-DA.html#problem-12.3.e",
    "title": "Chapter 12: Discriminant Analysis",
    "section": "Problem 12.3.e",
    "text": "Problem 12.3.e\nA spam filter that is based on your model is used, so that only messages that are classified as nonspam are delivered, while messages that are classified as spam are quarantined. In this case, misclassifying a nonspam e-mail (as spam) has much heftier results. Suppose that the cost of quarantining a nonspam e-mail is 20 times that of not detecting a spam message. Compute the constants of the classification functions to account for these costs (assume that the proportion of spam is reflected correctly by the sample proportion).\n\nlog_priors = np.log(da_class.priors_)\ndiff_log_priors = log_priors[1] - log_priors[0]\nprint(f'Difference of log-prior of classification function: {diff_log_priors}')\n\n# Adjust intercept using the log of the cost-ratio\nadjusted_intercept = da_class.intercept_ - np.log(20)\nprint(f'Intercept before adjustment {da_class.intercept_}')\nprint(f'Adjusted intercept including cost: {adjusted_intercept}')\n\nDifference of log-prior of classification function: -0.45706988046116737\nIntercept before adjustment [-2.07053468]\nAdjusted intercept including cost: [-5.06626695]\n\n\nFor the LinearDiscriminantAnalysis implementation in scikit-learn it is better to adjust the priors to take the misclassification costs into account.\n\nK = 20\nprint(f'Ratio of costs (q1/q2): {K}')\n\npriorNonspam = da_class.priors_[0]\npriorSpam = da_class.priors_[1]\nadjPriorNonspam = priorNonspam * K / (priorNonspam * K + priorSpam)\nadjPriorSpam = priorSpam / (priorNonspam * K + priorSpam)\n\nprint(f'Adjusted prior of nonspam: {adjPriorNonspam}')\nprint(f'Adjusted prior of spam: {adjPriorSpam}')\n\nRatio of costs (q1/q2): 20\nAdjusted prior of nonspam: 0.9693145970748495\nAdjusted prior of spam: 0.03068540292515056\n\n\n\nda_class_spam = LinearDiscriminantAnalysis(priors = [adjPriorNonspam, adjPriorSpam])\nda_class_spam.fit(train_X, train_y)\nclassificationSummary(valid_y, da_class_spam.predict(valid_X))\nda_class_spam.intercept_\n\nConfusion Matrix (Accuracy 0.6122)\n\n       Prediction\nActual    0    1\n     0 1092    6\n     1  708   35\n\n\narray([-5.06626695])\n\n\nWe can see that adjusting the priors leads to a model which predicts fewer nonspam messages as spam."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-18-ProbSolutions-SM.html",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-18-ProbSolutions-SM.html",
    "title": "Chapter 18: Smoothing Methods",
    "section": "",
    "text": "2019-2020 Galit Shmueli, Peter C. Bruce, Peter Gedeck\n\nData Mining for Business Analytics: Concepts, Techniques, and Applications in Python (First Edition) Galit Shmueli, Peter C. Bruce, Peter Gedeck, and Nitin R. Patel. 2019.\nDate: 2020-03-08\nPython Version: 3.8.2 Jupyter Notebook Version: 5.6.1\nPackages: - dmba: 0.0.12 - matplotlib: 3.2.0 - numpy: 1.18.1 - pandas: 1.0.1 - statsmodels: 0.11.1\nThe assistance from Mr. Kuber Deokar and Ms. Anuja Kulkarni in preparing these solutions is gratefully acknowledged.\n# import required packages for this chapter\nfrom pathlib import Path\nimport warnings\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.formula.api as sm\nfrom statsmodels.tsa import tsatools\nfrom statsmodels.graphics import tsaplots\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\n\nimport matplotlib.pylab as plt\n\nfrom dmba import regressionSummary\n\n%matplotlib inline\n# Working directory:\n#\n# We assume that data are kept in the same directory as the notebook. If you keep your \n# data in a different folder, replace the argument of the `Path`\nDATA = Path('.')\n# and then load data using \n#\n# pd.read_csv(DATA / ‘filename.csv’)"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-18-ProbSolutions-SM.html#solution-18.1.a",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-18-ProbSolutions-SM.html#solution-18.1.a",
    "title": "Chapter 18: Smoothing Methods",
    "section": "Solution 18.1.a",
    "text": "Solution 18.1.a\nCreate a time plot for the pre-event AIR time series. What time series components appear from the plot?\n\ndf = pd.read_csv(DATA / 'Sept11Travel.csv')\ndf['Date'] = pd.to_datetime(df.Month, format='%b-%y')\n\nair_ts = pd.Series(df['Air RPM (000s)'].values, index=df.Date, name='Air')\nrail_ts = pd.Series(df['Rail PM'].values, index=df.Date, name='Rail')\ncar_ts = pd.Series(df['VMT (billions)'].values, index=df.Date, name='Car')\n\npre_air_ts = air_ts[:'2001-01-01']\npre_air_ts.plot()\nplt.show()"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-18-ProbSolutions-SM.html#solution-18.1.b",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-18-ProbSolutions-SM.html#solution-18.1.b",
    "title": "Chapter 18: Smoothing Methods",
    "section": "Solution 18.1.b",
    "text": "Solution 18.1.b\nThe Figure in the book shows a time plot of the seasonally adjusted pre-September-11 AIR series. Which of the following smoothing methods would be adequate for forecasting this series?\n\nMoving average (with what window width?)\nSimple exponential smoothing\nHolt exponential smoothing\nHolt–Winter’s exponential smoothing\nMoving average (with what window width?) - NO\nSimple exponential smoothing - NO\nHolt exponential smoothing - YES (also called double exponential smoothing - used when a trend is present)\nHolt-Winter’s exponential smoothing - NO (Holt-Winters is used when trend and seasonality are present, but the seasonality has already been removed from this series)"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-18-ProbSolutions-SM.html#solution-18.3.a",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-18-ProbSolutions-SM.html#solution-18.3.a",
    "title": "Chapter 18: Smoothing Methods",
    "section": "Solution 18.3.a",
    "text": "Solution 18.3.a\nCompute the sales forecast for January 1999 based on a moving average with \\(w = 4\\)."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-18-ProbSolutions-SM.html#solution-18.3.b",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-18-ProbSolutions-SM.html#solution-18.3.b",
    "title": "Chapter 18: Smoothing Methods",
    "section": "Solution 18.3.b",
    "text": "Solution 18.3.b\nCompute the forecast error for the above forecast.\n\nSales = [27, 31, 58, 63, 59]\nforecast = sum(Sales[0:4]) / 4\nprint('18.3a) Forecast ', forecast)\nprint('18.3b) Forecast error ', Sales[4] - forecast)\n\n18.3a) Forecast  44.75\n18.3b) Forecast error  14.25\n\n\n\nsales = pd.Series(Sales)\nmovingAverage = sales.rolling(4).mean()\n\nforecast = movingAverage[3]\nprint('18.3a) Forecast ', forecast)\nprint('18.3b) Forecast error ', sales[4] - forecast)\n\n18.3a) Forecast  44.75\n18.3b) Forecast error  14.25"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-18-ProbSolutions-SM.html#solution-18.4.a",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-18-ProbSolutions-SM.html#solution-18.4.a",
    "title": "Chapter 18: Smoothing Methods",
    "section": "Solution 18.4.a",
    "text": "Solution 18.4.a\nThe value of zero that is obtained for the trend smoothing constant means that (choose one of the following):\n\nThere is no trend.\nThe trend is estimated only from the first two periods.\nThe trend is updated throughout the data.\nThe trend is statistically insignificant.\nThere is no trend. - NO\nThe trend is estimated only from the first two periods. - YES\nThe trend is updated throughout the data. - NO\nThe trend is statistically insignificant. - YES"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-18-ProbSolutions-SM.html#solution-18.4.b",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-18-ProbSolutions-SM.html#solution-18.4.b",
    "title": "Chapter 18: Smoothing Methods",
    "section": "Solution 18.4.b",
    "text": "Solution 18.4.b\nWhat is the danger of using the optimal smoothing constant values?\nOverfitting"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-18-ProbSolutions-SM.html#solution-18.5.a",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-18-ProbSolutions-SM.html#solution-18.5.a",
    "title": "Chapter 18: Smoothing Methods",
    "section": "Solution 18.5.a",
    "text": "Solution 18.5.a\nWhich of the following methods would not be suitable for forecasting this series?\n\nMoving average of raw series - Would not be suitable\nMoving average of deseasonalized series - Would not be suitable\nSimple exponential smoothing of the raw series - Would not be suitable\nDouble exponential smoothing of the raw series - Would not be suitable\nHolt–Winter’s exponential smoothing of the raw series\nRegression model fit to the raw series\nRandom walk model fit to the raw series - Would not be suitable"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-18-ProbSolutions-SM.html#solution-18.5.b",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-18-ProbSolutions-SM.html#solution-18.5.b",
    "title": "Chapter 18: Smoothing Methods",
    "section": "Solution 18.5.b",
    "text": "Solution 18.5.b\nThe forecaster was tasked to generate forecasts for 4 quarters ahead. He therefore partitioned the data such that the last 4 quarters were designated as the validation period. The forecaster approached the forecasting task by using multiplicative Holt-Winter’s exponential smoothing. The smoothing parameters used were \\(\\alpha=0.2, \\beta=0.15, \\gamma=0.05\\)."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-18-ProbSolutions-SM.html#solution-18.5.b.i",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-18-ProbSolutions-SM.html#solution-18.5.b.i",
    "title": "Chapter 18: Smoothing Methods",
    "section": "Solution 18.5.b.i",
    "text": "Solution 18.5.b.i\nRun this method on the data.\n\n# run exponential smoothing with additive trend and multiplicative seasonal\nexpSmooth = ExponentialSmoothing(df_ts[:20], trend='additive', seasonal='multiplicative', seasonal_periods=4)\nexpSmoothFit = expSmooth.fit(smoothing_level=0.2, smoothing_slope=0.15, smoothing_seasonal=0.05)\n\nax = df_ts[:20].plot(color='black', linewidth=0.5)\ndf_ts[20:].plot(ax=ax, color='black', linewidth=0.25)\nexpSmoothFit.fittedvalues.plot(ax=ax)\n\nforecast = pd.DataFrame({\n    'Quarter': df_ts[20:].index.values, \n    'Actual': df_ts[20:],\n    'Forecast': expSmoothFit.forecast(len(df_ts[20:])).values\n})\nforecast['Error'] = forecast['Actual'] - forecast['Forecast']\nforecast['mape'] = np.abs(forecast['Error']) / np.abs(forecast['Actual'])\nforecast.plot(y='Forecast', ax=ax, style='--', linewidth=2, color='C0')\nforecast\n\n# warning expected\n\n/Users/gedeck/opt/anaconda3/envs/dmba-notebooks/lib/python3.8/site-packages/statsmodels/tsa/base/tsa_model.py:213: ValueWarning: An unsupported index was provided and will be ignored when e.g. forecasting.\n  warnings.warn('An unsupported index was provided and will be'\n/Users/gedeck/opt/anaconda3/envs/dmba-notebooks/lib/python3.8/site-packages/statsmodels/tsa/base/tsa_model.py:580: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.\n  warnings.warn('No supported index is available.'\n\n\n\n\n\n\n\n\n\nQuarter\nActual\nForecast\nError\nmape\n\n\nQuarter\n\n\n\n\n\n\n\n\n\n21\n21\n60800\n58940.902683\n1859.097317\n0.030577\n\n\n22\n22\n64900\n61416.610944\n3483.389056\n0.053673\n\n\n23\n23\n76997\n72077.383125\n4919.616875\n0.063894\n\n\n24\n24\n103337\n95804.447363\n7532.552637\n0.072893"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-18-ProbSolutions-SM.html#solution-18.5.b.ii",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-18-ProbSolutions-SM.html#solution-18.5.b.ii",
    "title": "Chapter 18: Smoothing Methods",
    "section": "Solution 18.5.b.ii",
    "text": "Solution 18.5.b.ii\nThe forecasts for the validation set are given above. Compute the MAPE values for the forecasts of quarters 21 and 22.\n\nregressionSummary(df_ts[20:22].values, forecast['Forecast'].values[:2])\n\n\nRegression statistics\n\n                      Mean Error (ME) : 2671.2432\n       Root Mean Squared Error (RMSE) : 2791.9744\n            Mean Absolute Error (MAE) : 2671.2432\n          Mean Percentage Error (MPE) : 4.2125\nMean Absolute Percentage Error (MAPE) : 4.2125"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-18-ProbSolutions-SM.html#solution-18.5.c",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-18-ProbSolutions-SM.html#solution-18.5.c",
    "title": "Chapter 18: Smoothing Methods",
    "section": "Solution 18.5.c",
    "text": "Solution 18.5.c\nThe fit and residuals from the exponential smoothing are shown in the next Figure. Using all the information thus far, is this model suitable for forecasting quarters 21 and 22?\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    \n    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(8,6))\n\n    ax = df_ts[:20].plot(ax=axes[0], color='C0', style='.-')\n    expSmoothFit.fittedvalues.plot(ax=ax, color='C1', style='.-')\n    ax.set_ylabel('Sales ($)')\n    ax.set_title('Exp. Smoothing: Actual Vs. Forecast (Training Data)')\n    ax.xaxis.set_ticks(np.arange(1, 21, 2))\n\n    residuals = df_ts[:20] - expSmoothFit.fittedvalues\n    axes[1].axhline(y=0, color='black', linewidth=0.5)\n    ax = residuals.plot(ax=axes[1], color='C1', style='.-')\n    ax.set_ylabel('Forecast Error')\n    ax.set_title('Exp. Smoothing Forecast Errors (Training Data)')\n    ax.xaxis.set_ticks(np.arange(1, 21, 2))\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFrom the plots we can see that the actual and forecasted values in periods 21-22 match well in Holt Winter’s exponential smoothing; better than in the regression model. The MAPE for Holt Winter’s exponential smoothing (4.21%) is also lower than the MAPE for the regression model (4.69%, see Problem 17.5). This implies that the more suitable model for forecasting quarters 21 and 22 is Holt Winter’s exponential smoothing. However, there could be other practical reasons that would make the regression model preferable."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-18-ProbSolutions-SM.html#solution-18.6.a",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-18-ProbSolutions-SM.html#solution-18.6.a",
    "title": "Chapter 18: Smoothing Methods",
    "section": "Solution 18.6.a",
    "text": "Solution 18.6.a\nWhich of the following methods would be suitable for forecasting this series if applied to the raw data?\n\nMoving average - NO\nSimple exponential smoothing - NO\nDouble exponential smoothing - NO\nHolt–Winter’s exponential smoothing -YES"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-18-ProbSolutions-SM.html#solution-18.6.b",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-18-ProbSolutions-SM.html#solution-18.6.b",
    "title": "Chapter 18: Smoothing Methods",
    "section": "Solution 18.6.b",
    "text": "Solution 18.6.b\nApply a moving average with window span \\(w=4\\) to the data. Use all but the last year as the training set. Create a time plot of the moving average series.\n\nax = df_ts.plot(x='Quarter', y='Shipments')\n\nma_trailing = df_ts[:-4].rolling(4).mean()\nma_trailing = pd.Series(ma_trailing[:-1].values, index=ma_trailing.index[1:])\n\nma_trailing.plot(ax=ax)\nplt.show()\ndf_ts[-4:]\n\n\n\n\n\n\n\n\nQuarter\n1989Q1    4245\n1989Q2    4900\n1989Q3    4585\n1989Q4    4533\nFreq: Q-DEC, Name: shipments, dtype: int64\n\n\n\nSolution 18.6.b.i\nWhat does the MA(4) chart reveal?\nThere is an increasing trend until 1987, then a slight decreasing trend, with a final stabilization.\n\n\nSolution 18.6.b.ii\nUse the MA(4) model to forecast appliance sales in Q1-1990.\n\n\nSolution 18.6.b.iii\nUse the MA(4) model to forecast appliance sales in Q1-1991.\n\nma_trailing_full = df_ts.rolling(4).mean()\nprint(f'forecast Q1-1990: {ma_trailing_full[-1]}')\nprint(f'forecast Q1-1991: same as stable trend {ma_trailing_full[-1]}')\n\nforecast Q1-1990: 4565.75\nforecast Q1-1991: same as stable trend 4565.75\n\n\n\n\nSolution 18.6.b.iv\nIs the forecast for Q1-1990 most likely to under-estimate, over-estimate or accurately estimate the actual sales on Q1-1990? Explain.\nIt should be fairly accurate - the recent trend is stable, and Q1 values have been close to trend in the past.\n\n\nSolution 18.6.b.v\nManagement feels most comfortable with moving averages. The analyst therefore plans to use this method for forecasting future quarters. What else should be considered before using the MA(4) to forecast future quarterly shipments of household appliances?\nSeasonal adjustment"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-18-ProbSolutions-SM.html#solution-18.6.c",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-18-ProbSolutions-SM.html#solution-18.6.c",
    "title": "Chapter 18: Smoothing Methods",
    "section": "Solution 18.6.c",
    "text": "Solution 18.6.c\nWe now focus on forecasting beyond 1989. In the following, continue to use all but the last year as the training set, and the last four quarters as the validation set. First, fit a regression model to sales with a linear trend and quarterly seasonality to the training data. Next, apply Holt–Winter’s exponential smoothing with smoothing parameters \\(\\alpha=0.2, \\beta=0.15, \\gamma=0.05\\) to the training data. Choose an adequate ‘season length.’\nRegression model\n\ndf_lm = tsatools.add_trend(pd.Series(df.Shipments.values, index=df.Quarter, name='Sales'), trend='ct')\ndf_lm['Qtr'] = df_lm.index.quarter\nmodelA = sm.ols(formula='Sales ~ trend + C(Qtr)', data=df_lm[:-4]).fit()\n# warning expected\nprint(modelA.summary())\nax = df_ts[:-4].plot(color='black', linewidth=0.5)\ndf_ts[-4:].plot(ax=ax, color='black', linewidth=0.25)\nmodelA.fittedvalues.plot(ax=ax)\nmodelA.predict(df_lm[-4:]).plot(ax=ax, style='--', linewidth=2, color='C0')\nplt.show()\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  Sales   R-squared:                       0.749\nModel:                            OLS   Adj. R-squared:                  0.658\nMethod:                 Least Squares   F-statistic:                     8.208\nDate:                Sun, 08 Mar 2020   Prob (F-statistic):            0.00255\nTime:                        20:21:23   Log-Likelihood:                -100.84\nNo. Observations:                  16   AIC:                             211.7\nDf Residuals:                      11   BIC:                             215.5\nDf Model:                           4                                         \nCovariance Type:            nonrobust                                         \n===============================================================================\n                  coef    std err          t      P&gt;|t|      [0.025      0.975]\n-------------------------------------------------------------------------------\nIntercept    4054.9250    101.132     40.095      0.000    3832.336    4277.514\nC(Qtr)[T.2]   271.2750    112.981      2.401      0.035      22.605     519.945\nC(Qtr)[T.3]    85.8000    114.029      0.752      0.468    -165.176     336.776\nC(Qtr)[T.4]  -232.9250    115.754     -2.012      0.069    -487.698      21.848\ntrend          35.7250      8.904      4.012      0.002      16.127      55.323\n==============================================================================\nOmnibus:                        0.690   Durbin-Watson:                   1.504\nProb(Omnibus):                  0.708   Jarque-Bera (JB):                0.345\nSkew:                           0.345   Prob(JB):                        0.841\nKurtosis:                       2.799   Cond. No.                         42.5\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n/Users/gedeck/opt/anaconda3/envs/dmba-notebooks/lib/python3.8/site-packages/scipy/stats/stats.py:1534: UserWarning: kurtosistest only valid for n&gt;=20 ... continuing anyway, n=16\n  warnings.warn(\"kurtosistest only valid for n&gt;=20 ... continuing \"\n\n\n\n\n\n\n\n\n\n\nregressionSummary(df_lm[:-4].Sales, modelA.fittedvalues)\n\n\nRegression statistics\n\n                      Mean Error (ME) : -0.0000\n       Root Mean Squared Error (RMSE) : 132.0699\n            Mean Absolute Error (MAE) : 104.0688\n          Mean Percentage Error (MPE) : -0.0906\nMean Absolute Percentage Error (MAPE) : 2.3544\n\n\n\nSolution 18.6.c.i\nCompute the MAPE for the validation data using the regression model.\n\nregressionSummary(df_lm[-4:].Sales, modelA.predict(df_lm[-4:]))\n\n\nRegression statistics\n\n                      Mean Error (ME) : -181.1250\n       Root Mean Squared Error (RMSE) : 241.8138\n            Mean Absolute Error (MAE) : 181.1250\n          Mean Percentage Error (MPE) : -4.1085\nMean Absolute Percentage Error (MAPE) : 4.1085\n\n\n\n\nSolution 18.6.c.ii\nCompute the MAPE for the validation data using Holt-Winter’s exponential smoothing.\n\nexpSmooth = ExponentialSmoothing(df_ts[:-4], trend='additive', seasonal='additive', seasonal_periods=4, freq='Q')\nexpSmoothFit = expSmooth.fit()\n\nax = df_ts[:-4].plot(color='black', linewidth=0.5)\ndf_ts[-4:].plot(ax=ax, color='black', linewidth=0.25)\nexpSmoothFit.fittedvalues.plot(ax=ax)\nexpSmoothFit.forecast(4).plot(ax=ax, style='--', linewidth=2, color='C0')\nplt.show()\n\n\n\n\n\n\n\n\n\nregressionSummary(df_lm[-4:].Sales, expSmoothFit.forecast(4))\n\n\nRegression statistics\n\n                      Mean Error (ME) : -71.9849\n       Root Mean Squared Error (RMSE) : 175.6162\n            Mean Absolute Error (MAE) : 144.7682\n          Mean Percentage Error (MPE) : -1.7119\nMean Absolute Percentage Error (MAPE) : 3.2845\n\n\n\n\nSolution 18.6.c.iii\nWhich model would you prefer to use for forecasting Q1-1990? Give three reasons.\nRegression: simplicity, low MAPE on the validation data, small residuals.\nHolt-Winter’s exponential smoothing shows only a very small improvment over regression #also does not show large improvement over Regression, is not as simple.\n\n\nSolution 18.6.c.iv\nIf we optimize the smoothing parameters in the Holt-Winter’s method, is it likely to get values that are close to zero? Why or why not?\nYes. It appears that both the regression and HW models yield very similar results, and therefore there is probably a global trend/seasonality going on. The Holt-Winter’s model would be able to capture this at the start and then would not need to update anymore."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-18-ProbSolutions-SM.html#solution-18.8.a",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-18-ProbSolutions-SM.html#solution-18.8.a",
    "title": "Chapter 18: Smoothing Methods",
    "section": "Solution 18.8.a",
    "text": "Solution 18.8.a\nReproduce the time plot with the overlaying MA(4) line.\n\ndf = pd.read_csv(DATA / 'NaturalGasSales.csv')\nax = df.plot(x='Quarter', y='Gas Sales', style='.-')\nma_trailing = df['Gas Sales'].rolling(4).mean()\nma_trailing.plot(ax=ax)\nax.legend([\"Gas Sales\", \"4 per. Mov. Avg. (Gas Sales)\"]);\nax.set_xlabel('Season')\nax.set_ylabel('Billion BTU')\nax.set_xticklabels(['', 'Summer-2001', '', 'Summer-2002', '', 'Summer-2003', '', 'Summer-2004'])\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-18-ProbSolutions-SM.html#solution-18.8.b",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-18-ProbSolutions-SM.html#solution-18.8.b",
    "title": "Chapter 18: Smoothing Methods",
    "section": "Solution 18.8.b",
    "text": "Solution 18.8.b\nWhat can we learn about the series from the MA line?\nSales are on a declining trend."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-18-ProbSolutions-SM.html#solution-18.8.c",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-18-ProbSolutions-SM.html#solution-18.8.c",
    "title": "Chapter 18: Smoothing Methods",
    "section": "Solution 18.8.c",
    "text": "Solution 18.8.c\nRun a moving average forecaster with adequate season length. Are forecasts generated by this method expected to over-forecast, under-forecast, or accurately forecast actual sales? Why?\n\nprint('Forecast sales: ', ma_trailing.iloc[-1])\n\nForecast sales:  143.25\n\n\nBecause of the declining trend, a simple moving average will tend to overestimate future sales."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-18-ProbSolutions-SM.html#solution-18.9.a",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-18-ProbSolutions-SM.html#solution-18.9.a",
    "title": "Chapter 18: Smoothing Methods",
    "section": "Solution 18.9.a",
    "text": "Solution 18.9.a\nWhich forecasting method would you choose if you had to choose the same method for all series? Why?\nModel/Method: Holt-Winter’s exponential smoothing Reasons: 1. Ability of the model to capture both seasonality and trend that are present in the various data series. 2. Adaptability of the model - will adapt to the various patterns of different data series. 3. Holt-Winter’s model is a reasonable model for all series - it will be able to capture and adjust to changing levels of trends and seasonality swings. It is parsimonious and should produce good fit as well as good forecasts."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-18-ProbSolutions-SM.html#solution-18.9.b",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-18-ProbSolutions-SM.html#solution-18.9.b",
    "title": "Chapter 18: Smoothing Methods",
    "section": "Solution 18.9.b",
    "text": "Solution 18.9.b\nFortified wine has the largest market share of the above six types of wine. You are asked to focus on fortified wine sales alone, and produce as accurate as possible forecasts for the next 2 months.\n\nStart by partitioning the data using the period until December 1993 as the training set.\nApply Holt-Winter’s exponential smoothing to sales with an appropriate season length (use smoothing parameters \\(\\alpha=0.2, \\beta=0.15, \\gamma=0.05\\)).\n\n\ndf = pd.read_csv(DATA / 'AustralianWines.csv', na_values=['*'])\ndf['Date'] = pd.to_datetime(df.Month, format='%b-%y')\ndf.index = df.Date\n\ntrain_df = df[:'1993-12-31'].Fortified\nvalid_df = df['1994-01-01':].Fortified\nax = train_df.plot()\nvalid_df.plot(ax=ax)\nplt.show()\n\n\n\n\n\n\n\n\n\nexpSmooth = ExponentialSmoothing(train_df, trend='additive', seasonal='additive', seasonal_periods=12, freq='MS')\nexpSmoothFit = expSmooth.fit(smoothing_level=0.2, smoothing_slope=0.15, smoothing_seasonal=0.05)\n\nax = train_df.plot(color='black', linewidth=0.5)\ntrain_df.plot(ax=ax, color='black', linewidth=0.25)\nexpSmoothFit.fittedvalues.plot(ax=ax)\nexpSmoothFit.forecast(len(valid_df)).plot(ax=ax, style='--', linewidth=2, color='C0')\nplt.show()"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-18-ProbSolutions-SM.html#solution-18.9.c",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-18-ProbSolutions-SM.html#solution-18.9.c",
    "title": "Chapter 18: Smoothing Methods",
    "section": "Solution 18.9.c",
    "text": "Solution 18.9.c\nCreate an ACF plot for the residuals from the Holt-Winter’s exponential smoothing until lag-12.\n\ntsaplots.plot_acf(expSmoothFit.resid)\nplt.xlim(0, 13)\nplt.show()\n\n\n\n\n\n\n\n\n\nSolution 18.9.c.i\nExamining this plot, which of the following statements are reasonable conclusions?\n\nDecembers (month 12) are not captured well by the model.\nThere is a strong correlation between sales on the same calendar month.\nThe model does not capture the seasonality well.\nWe should try to fit an autoregressive model with lag-12 to the residuals.\nWe should first deseasonalize the data and then apply Holt-Winter’s exponential smoothing.\n\nAnswers: - Decembers (month 12) are not captured well by the model. - NO - There is a strong correlation between sales on the same calendar month. - YES - The model does not capture the seasonality well. - YES - We should try to fit an autoregressive model with lag-12 to the residuals. - YES - We should first deseasonalize the data and then apply Holt-Winter’s exponential smoothing. - NO\n\n\nSolution 18.9.c.i\nHow can you handle the above effect without adding another layer to your model?\nIncrease gamma in Holt-Winters model.\n\nexpSmooth = ExponentialSmoothing(train_df, trend='additive', seasonal='additive', seasonal_periods=12, freq='MS')\nexpSmoothFit = expSmooth.fit(smoothing_level=0.2, smoothing_slope=0.15, smoothing_seasonal=0.4)\n\ntsaplots.plot_acf(expSmoothFit.resid)\nplt.xlim(0, 13)\nplt.show()"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-05-ProbSolutions-Assessment.html",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-05-ProbSolutions-Assessment.html",
    "title": "Chapter 5: Evaluating Predictive Performance",
    "section": "",
    "text": "2019-2020 Galit Shmueli, Peter C. Bruce, Peter Gedeck\n\nData Mining for Business Analytics: Concepts, Techniques, and Applications in Python (First Edition) Galit Shmueli, Peter C. Bruce, Peter Gedeck, and Nitin R. Patel. 2019.\nDate: 2020-03-08\nPython Version: 3.8.2 Jupyter Notebook Version: 5.6.1\nPackages: - dmba: 0.0.12 - matplotlib: 3.2.0 - pandas: 1.0.1 - scikit-learn: 0.22.2\nThe assistance from Mr. Kuber Deokar and Ms. Anuja Kulkarni in preparing these solutions is gratefully acknowledged.\n\n\n# import required packages for this chapter\n\nfrom pathlib import Path\n\nimport math\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import accuracy_score, roc_curve, auc\nimport matplotlib.pylab as plt \n\nfrom dmba import regressionSummary, classificationSummary\nfrom dmba import liftChart, gainsChart\n\nno display found. Using non-interactive Agg backend\n\n\n\n# Working directory:\n#\n# We assume that data are kept in the same directory as the notebook. If you keep your \n# data in a different folder, replace the argument of the `Path`\n# and then load data using \n#\n# pd.read_csv(‘filename.csv’)\n\n\n5.1\nA data mining routine has been applied to a transaction dataset and has classified 88 records as fraudulent (30 correctly so) and 952 as non-fraudulent (920 correctly so). Construct the confusion matrix and calculate the overall error rate.\nAnswer:\nclassification confusion matrix\n|----------------------------------------------------------------------------|\n|             |                     Predicted Class                          |\n|----------------------------------------------------------------------------|\n| Actual Class|             C0               |             C1                |\n|----------------------------------------------------------------------------|\n|      C0     | n0,0 = number of correctly   | n0,1 = number of C0 cases     | \n|             |  classified C0 cases         |  incorrectly classified as C1 | \n|----------------------------------------------------------------------------|\n|      C1     | n1,0 = number of C1 cases in-| n1,1 = number of correctly    |\n|             |  coreectly classified as C0  |  classified C1 cases          |\n|----------------------------------------------------------------------------|\n\nTherefore in our problem the confusion matrix is\n\nclassification confusion matrix\n|----------------------------------------------------------------------|\n|                    |                 Predicted Class                 |\n|----------------------------------------------------------------------|\n| Actual Class       |         Fraudulant(1)    |   Non-fraudulant (0) |\n|----------------------------------------------------------------------|\n| Fraudulant (1)     |             30           |        32            | \n|----------------------------------------------------------------------|\n| Non-fraudulant (0) |             58           |       920            |\n|----------------------------------------------------------------------|\n\n\nformula for overall error rate\noverall Error Rate = (n0,1 + n1,0) / n, where n is the total number of records.\n\nerror_rate = (32 + 58) / 1040\nerror_rate\n\n0.08653846153846154\n\n\nSo the overall error rate is 8.65%.\n\n\n5.2\nSuppose that this routine has an adjustable cutoff (threshold) mechanism by which you can alter the proportion of records classified as fraudulent. Describe how moving the cutoff up or down would affect\n5.2.a. the classification error rate for records that are truly fraudulent\n5.2.b. the classification error rate for records that are truly nonfraudulent\nAnswer:\nclassification confusion matrix\n|--------------------------------------------------------|\n|                   |          Predicted Class           |\n|--------------------------------------------------------|\n| Actual Class      | Fraudulant (1) | Non-fraudulant (0)|\n|--------------------------------------------------------|\n| Fraudulant (1)    |       a        |         b         |\n|--------------------------------------------------------|\n| Non-fraudulant (0)|       c        |         d         |\n|--------------------------------------------------------|\n\nThe classification error rate for truly fraudulent records (with this 0.5 cutoff) is b/(a+b)\nThe classification error rate for truly non-fraudulent records (with this 0.5 cutoff) is c/(c+d)\nLowering the cutoff (here, below 0.5) leads to classifying more records, both fraudulent and non-fraudulent, as fraudulent: a and c both increase, b and d decline.\na. With respect to the classification error rate for truly fraudulent records, the error rate, b/(a+b), decreases as b goes up. As you lower the standard for calling a record fraudulent, you catch more and more of the real frauds.\nb. With respect to the classification error rate for truly non-fraudulent records, the error rate, c/(c+d), increases as c goes up. As you lower the standard for calling a record fraudulent, you mistakenly identify more and more non-frauds as frauds.\nIncreasing the cutoff (here, above 0.5) leads to classifying more records, both fraudulent and non-fraudulent, as non-fraudulent: b and d both increase, a and c decline.\na. With respect to the classification error rate for truly fraudulent records, the error rate, b/(a+b), increases as b goes up. As you raise the standard for calling a record fraudulent, you miss more and more of the real frauds.\nb. With respect to the classification error rate for truly non-fraudulent records, the error rate, c/(c+d), decreases as d goes up. As you raise the standard for calling a record fraudulent, fewer non-frauds get mis-labeled as frauds.\n\n\n5.3\nFiscalNote is a startup founded by a Washington, DC entrepreneur and funded by a Singapore sovereign wealth fund, the Winklevoss twins of Facebook fame, and others. It uses machine learning and data mining techniques to predict for its clients whether legislation in the US Congress and in US state legislatures will pass or not. The company reports 94% accuracy. (Washington Post, November 21, 2014, “Capital Business”)\nConsidering just bills introduced in the US Congress, do a bit of internet research to learn about numbers of bills introduced and passage rates. Identify the possible types of misclassifications, and comment on the use of overall accuracy as a metric. Include a discussion of other possible metrics and the potential role of propensities.\nAnswer:\nWeb research on govtrack.us shows that, in the 113th Congress (which covered 2013 and 2014), over 10,000 pieces of legislation were introduced but only 3% passed as enacted laws. “Enacted laws” does not include the 6% that were passed as (usually meaningless) resolutions.\nIf we focus just on classifying each bill, and use overall accuracy as a metric, we could achieve 97% accuracy just by predicting that nothing will pass (as a law).\nA data mining model might make two types of classification errors - saying that a bill will pass when it won’t, and saying a bill won’t pass when it will. The second type of error is probably more costly than the first - identifying the small number of bills that will pass is probably of more interest than overall accuracy. Therefore, a useful metric would be sensitivity to “will pass” - the proportion of “will pass” bills that were correctly predicted (alongside with specificity, which is the proportion of “will not pass” that are correctly ruled out).\nHowever, rather than just assigning a 0/1 class to each bill (classification), we will probably be more interested in ranking the bills and estimating a propensity (probability) for passage for each bill. We would then focus on the high probability bills, and not be so concerned with the low probability bills.\nWith ranking as our goal, we could use lift as a metric for how well a model separates out the “will pass” bills. As part of the calculation for lift, the bills would be ranked by their propensity to pass. Lift gives a picture of how much better the model does than not using a model.\n\n\n5.4\nConsider Figure 5.12, the decile lift chart for the transaction data model, applied to new data.\n5.4.a. Interpret the meaning of the first and second bars from the left.\nAnswer:\nLeft-most bar: If we take the 10% “most probable 1’s (frauds)” (as ranked by the model), it will catch 6.5 times as many 1’s (frauds), as would a random selection of 10% of the records.\n#2nd bar from left: If we take the second highest decile (10%) of records that are ranked by the model as “the most probable 1’s (frauds)” it will catch 2.7 times as many 1’s (frauds), as would a random selection of 10% of the records.\n5.4.b. Explain how you might use this information in practice.\nAnswer:\nConsider a tax authority that wants to allocate their resources for investigating firms that are most likely to submit fraudulent tax returns. Suppose that there are resources for auditing only 10% of firms. Rather than taking a random sample, they can select the top 10% of firms that are predicted to be most likely to report fraudulently (according to the decile chart). Or, to preserve the principle that anyone might be audited, they can establish differential probabilities for being sampled – those in the top deciles being much more likely to be audited.\n5.4.c. Another analyst comments that you could improve the accuracy of the model by classifying everything as nonfraudulent. If you do that, what is the error rate?\nAnswer:\nWe have the following confusion matrix from Problem 5.1.\nclassification confusion matrix\n|----------------------------------------------------------------------|\n|                    |                 Predicted Class                 |\n|----------------------------------------------------------------------|\n| Actual Class       |         Fraudulant(1)    |   Non-fraudulant (0) |\n|----------------------------------------------------------------------|\n| Fraudulant (1)     |             30           |        32            | \n|----------------------------------------------------------------------|\n| Non-fraudulant (0) |             58           |       920            |\n|----------------------------------------------------------------------|\n\n\nAccording to the new analyst our classification confusion matrix becomes-\nclassification confusion matrix\n|----------------------------------------------------------------------|\n|                    |                 Predicted Class                 |\n|----------------------------------------------------------------------|\n| Actual Class       |         Fraudulant(1)    |   Non-fraudulant (0) |\n|----------------------------------------------------------------------|\n| Fraudulant (1)     |              0           |        88            | \n|----------------------------------------------------------------------|\n| Non-fraudulant (0) |              0           |       952            |\n|----------------------------------------------------------------------|\n\n\n\n# Overall misclassification rate\n\nerror_rate = 88/1040\nerror_rate\n\n0.08461538461538462\n\n\nWe see that the misclassification error rate is lower (8.46%) with the “everything non-fraudulent” proposal (although only slightly).\n5.4.d. Comment on the usefulness, in this situation, of these two metrics of model performance (error rate and lift).\nAnswer:\nThe likely purpose of this analysis is to identify fraudulent records. The overall “error rate” is not likely to help much in evaluating competing methods for doing so. The key factor here is the ability to identify records that have a high probability of being fraudulent, and this is what lift measures. Using lift, you can “descend” through the records in order of probability of being fraudulent, knowing at each point how much more likely you are to be getting a fraudulent record than naively selecting at random. The “error rate” measure, by contrast, reveals nothing about the efficiency of identifying fraudulent records.\nThe vast majority of records are non-fraudulent, and correctly classifying nonfraudulent records drives the overall error rate. One can achieve a very respectably low error rate just by classifying everything as non-fraudulent, which is not practically useful.\n5.5.\nA large number of insurance records are to be examined to develop a model for predicting fraudulent claims. Of the claims in the historical database, 1% were judged to be fraudulent. A sample is taken to develop a model, and oversampling is used to provide a balanced sample in light of the very low response rate. When applied to this sample (n = 800), the model ends up correctly classifying 310 frauds, and 270 nonfrauds. It missed 90 frauds, and classified 130 records incorrectly as frauds when they were not.\n5.5.a. Produce the confusion matrix for the sample as it stands.\nAnswer:\nclassification confusion matrix\n|--------------------------------------------------------|\n|                   |          Predicted Class           |\n|--------------------------------------------------------|\n| Actual Class      |        1       |         0         |\n|--------------------------------------------------------|\n|          1        |      310       |        90         |\n|--------------------------------------------------------|\n|          0        |      130       |       270         |\n|--------------------------------------------------------|\n\n\n# Misclassification rate\n\nmiscl_rate1 = (90 + 130) / 800\nmiscl_rate1\n\n0.275\n\n\nSo the overall misclassification rate is 27.5%.\nThe model ends up classifying (310 + 130) / 800 = 0.55 = 55% of the records as fraudulent.\n5.5.b. Find the adjusted misclassification rate (adjusting for the oversampling).\nAnswer:\nNow we need to add enough zeros so that the 1’s only constitute 1% of the total and the 0’s constitute 99% of the total (where is X is the total).\n                                                                                                           \n400 + 0.99*x = x\nTherefore x = 40, 000\nNumber of zeros = 0.99 * 40, 000 = 39600\n\n#classification confusion matrix\n|-------------------------------------------------------------|\n|                   |          Predicted Class       |        |\n|-------------------------------------------------------------|\n|    Actual Class   |        1       |         0     | Total  |\n|-------------------------------------------------------------|\n|          1        |      310       |        90     | 400    |\n|----------------------------------------------------|--------|\n|          0        |    12870       |     26730     | 39600  |\n|----------------------------------------------------|--------|\n|       Total       |    13180       |     26820     | 40000  |\n|-------------------------------------------------------------|\n\n\n# overall misclassification rate\n\nmiscl_rate2 = (90 + 12870) / 40000\nmiscl_rate2\n\n0.324\n\n\nThe model ends up classifying (310 + 12870) / 40000 = 0.3295 = 32.95% of the records as fraudulent.\n5.5.c. What percentage of new records would you expect to be classified as fraudulent?\nFrom the above calculations, we expect 32.95% of the records to be classified as frauds.\n\n\n5.6\nA firm that sells software services has been piloting a new product and has records of 500 customers who have either bought the services or decided not to. The target value is the estimated profit from each sale (excluding sales costs). The global mean is about 2128 dollars. However, the cost of the sales effort is not cheap— the company figures it comes to 2500 dollars for each of the 500 customers (whether they buy or not). The firm developed a predictive model in hopes of being able to identify the top spenders in the future. The cumulative gains and decile lift charts for the validation set are shown in Figure 5.13.\n5.6.a If the company begins working with a new set of 1000 leads to sell the same services, similar to the 500 in the pilot study, without any use of predictive modeling to target sales efforts, what is the estimated profit?\nAnswer:\nIf the 1000 new leads are like those in the pilot, then the company can expect the same mean profit per sale of 2500 dollars, or 2,500,000 dollars for the 1000 leads. This does not include the cost of the sales effort, which would cost an estimated 2.5 million dollars. In other words, it would not be a profitable move.\n5.6.b. If the firm wants the average profit on each sale to roughly double the sales effort cost, and applies an appropriate cutoff with this predictive model to a new set of 1000 leads, how far down the new list of 1000 should it proceed (how many deciles)?\nAnswer:\nIf the profit must double the sales effort cost, that would be 5000 dollars. This is twice the average profit across all customers. The company could achieve this by attempting sales to the first (top) decile among the customers, where the lift is about 2.1. However, it should not go beyond this point.\n5.6.c Still considering the new list of 1000 leads, if the company applies this predictive model with a lower cutoff of 2500, how far should it proceed down the ranked leads, in terms of deciles?\nAnswer:\nIf the cutoff is lowered to 2500 dollars, a lift as low as 2500/2500 or 1.0 could be tolerated. This would mean going all the way through the 5th decile, or half the customers. It would also mean that the product is breakeven at the margin (i.e. the last group of leads produces no net profit)\n5.6.d. Why use this two-stage process for predicting sales—why not simply develop a model for predicting profit for the 1000 new leads?\nAnswer:\nA model to predict overall profit for all 1000 leads would not be useful. The profit would be zero. Individual profit predictions for each of the 1000 leads would be useful, and might be sufficient, but generating staged solutions (corresponding to different cutoffs and differing lift) helps translate the optimization problem into a business problem, and frame a limited number of decision options for managers. Note: different interpretations of the question are possible with respect to whether profit for all 1000 leads all together is intended, or profit for each of the 1000 leads individually, so some leeway is accorded in marking.\n\n\n5.7\nTable 5.7 shows a small set of predictive model validation results for a classification model, with both actual values and propensities.\n5.7.a. Calculate error rates, sensitivity, and specificity using cutoffs of 0.25, 0.5, and 0.75.\nAnswer:\n\n# create a data frame of data\n\ndata = {'Propensity': [0.03, 0.52, 0.38, 0.82, 0.33, 0.42, 0.55, 0.59, 0.09, 0.21, 0.43, 0.04, 0.08, 0.13, 0.01, 0.79, 0.42, \n                       0.29, 0.08, 0.02],\n        'Actual': [0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]}\n\n# convert to data frame\ndf = pd.DataFrame(data)\n\n\n# cutoff = 0.25\n\nPredicted = [1 if p &gt; 0.25 else 0 for p in df.Propensity]\nclassificationSummary(df.Actual, Predicted, class_names=['0', '1'])\n\nConfusion Matrix (Accuracy 0.6000)\n\n       Prediction\nActual 0 1\n     0 9 8\n     1 0 3\n\n\n\n# overall error rate\nerror_rate = (8) / 20\nprint('\\nError Rate = ', error_rate)\n# sensitivity\nsensitivity = (3) / (3+0)\nprint('\\nSensitivity=',sensitivity)\n# specificity\nspecificity = (9) / (8+9)\nprint('\\nSpecificity = ', specificity)\n\n\nError Rate =  0.4\n\nSensitivity= 1.0\n\nSpecificity =  0.5294117647058824\n\n\n\n# cutoff = 0.5\n\nPredicted = [1 if p &gt; 0.5 else 0 for p in df.Propensity]\nclassificationSummary(df.Actual, Predicted, class_names=['0', '1'])\n\nConfusion Matrix (Accuracy 0.9000)\n\n       Prediction\nActual  0  1\n     0 15  2\n     1  0  3\n\n\n\n# overall error rate\nerror_rate = (2) / 20\nprint('\\nError Rate = ', error_rate)\n# sensitivity\nsensitivity = (3) / (3+0)\nprint('\\nSensitivity=',sensitivity)\n# specificity\nspecificity = (15) / (2+15)\nprint('\\nSpecificity = ', specificity)\n\n\nError Rate =  0.1\n\nSensitivity= 1.0\n\nSpecificity =  0.8823529411764706\n\n\n\n# cutoff = 0.75\n\nPredicted = [1 if p &gt; 0.75 else 0 for p in df.Propensity]\nclassificationSummary(df.Actual, Predicted, class_names=['0', '1'])\n\nConfusion Matrix (Accuracy 0.9500)\n\n       Prediction\nActual  0  1\n     0 17  0\n     1  1  2\n\n\n\n# overall error rate\nerror_rate = (1) / 20\nprint('\\nError Rate = ', error_rate)\n# sensitivity\nsensitivity = (2) / (2+1)\nprint('\\nSensitivity=',sensitivity)\n# specificity\nspecificity = (17) / (0+17)\nprint('\\nSpecificity = ', specificity)\n\n\nError Rate =  0.05\n\nSensitivity= 0.6666666666666666\n\nSpecificity =  1.0\n\n\n5.7.b. Create a decile lift chart.\nAnswer:\n\n# decile lift chart\n# sort data by propensities\ndf = df.sort_values(by=['Propensity'], ascending=False)\n#create decile lift chart\nliftChart(df.Actual)\n\n&lt;AxesSubplot:title={'center':'Decile Lift Chart'}, xlabel='Percentile', ylabel='Lift'&gt;"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.5 Taxi Cancellations.html",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.5 Taxi Cancellations.html",
    "title": "Case 21.5 Taxi Cancellations",
    "section": "",
    "text": "2019 Galit Shmueli, Peter C. Bruce, Peter Gedeck\n\nCase study included in\nData Mining for Business Analytics: Concepts, Techniques, and Applications in Python (First Edition) Galit Shmueli, Peter C. Bruce, Peter Gedeck, and Nitin R. Patel. 2019.\n%matplotlib inline\n\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pylab as plt\n\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n\nfrom dmba import classificationSummary, gainsChart\n\nDATA = Path('.').resolve().parent / 'data'"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.5 Taxi Cancellations.html#step-4.1-logistic-regression-model",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.5 Taxi Cancellations.html#step-4.1-logistic-regression-model",
    "title": "Case 21.5 Taxi Cancellations",
    "section": "Step 4.1: Logistic regression model",
    "text": "Step 4.1: Logistic regression model\n\nlogit_reg = LogisticRegressionCV(penalty=\"l2\", solver='saga', cv=5, max_iter=5000)\nlogit_reg.fit(train_X, train_y)\nlogit_reg_confusion = confusionMatrices(logit_reg, 'Logistic regression')\n\nLogistic regression - training results\nConfusion Matrix (Accuracy 0.9253)\n\n       Prediction\nActual    0    1\n     0 5552    0\n     1  448    0\nLogistic regression - validation results\nConfusion Matrix (Accuracy 0.9263)\n\n       Prediction\nActual    0    1\n     0 3705    0\n     1  295    0"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.5 Taxi Cancellations.html#step-4.2-decision-tree-classifier",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.5 Taxi Cancellations.html#step-4.2-decision-tree-classifier",
    "title": "Case 21.5 Taxi Cancellations",
    "section": "Step 4.2: Decision tree classifier",
    "text": "Step 4.2: Decision tree classifier\n\ndtree = DecisionTreeClassifier()\ndtree.fit(train_X, train_y)\ndtree_confusion = confusionMatrices(dtree, 'Decision tree')\n\nDecision tree - training results\nConfusion Matrix (Accuracy 0.9968)\n\n       Prediction\nActual    0    1\n     0 5551    1\n     1   18  430\nDecision tree - validation results\nConfusion Matrix (Accuracy 0.8745)\n\n       Prediction\nActual    0    1\n     0 3435  270\n     1  232   63\n\n\n\nrf = RandomForestClassifier(n_estimators=100)\nrf.fit(train_X, train_y)\nrf_confusion = confusionMatrices(rf, 'Random forest')\n\nRandom forest - training results\nConfusion Matrix (Accuracy 0.9968)\n\n       Prediction\nActual    0    1\n     0 5547    5\n     1   14  434\nRandom forest - validation results\nConfusion Matrix (Accuracy 0.9197)\n\n       Prediction\nActual    0    1\n     0 3640   65\n     1  256   39\n\n\n\nparam_grid = {\n    'hidden_layer_sizes': list(range(2, 10)),\n}\nneuralNet = MLPClassifier(activation='logistic', solver='lbfgs', random_state=1, max_iter=5000)\n\ngridSearch = GridSearchCV(neuralNet, param_grid, cv=5, n_jobs=-1)\ngridSearch.fit(train_X, train_y)\nprint('Improved score: ', gridSearch.best_score_)\nprint('Improved parameters: ', gridSearch.best_params_)\n\nneuralNet = gridSearch.best_estimator_\nneuralNet_confusion = confusionMatrices(neuralNet, 'Neural network')\n\nImproved score:  0.9253333333333332\nImproved parameters:  {'hidden_layer_sizes': 2}\nNeural network - training results\nConfusion Matrix (Accuracy 0.9257)\n\n       Prediction\nActual    0    1\n     0 5549    3\n     1  443    5\nNeural network - validation results\nConfusion Matrix (Accuracy 0.9257)\n\n       Prediction\nActual    0    1\n     0 3702    3\n     1  294    1"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.2 German Credit.html",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.2 German Credit.html",
    "title": "Case 21.2 German Credit",
    "section": "",
    "text": "2019 Galit Shmueli, Peter C. Bruce, Peter Gedeck\n\nCase study included in\nData Mining for Business Analytics: Concepts, Techniques, and Applications in Python (First Edition) Galit Shmueli, Peter C. Bruce, Peter Gedeck, and Nitin R. Patel. 2019.\n%matplotlib inline\nimport warnings\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pylab as plt\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import confusion_matrix\n\nfrom dmba import classificationSummary, gainsChart, liftChart\n\n\nDATA = Path('.').resolve().parent / 'data'"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.2 German Credit.html#step-1.-analyze-your-data",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.2 German Credit.html#step-1.-analyze-your-data",
    "title": "Case 21.2 German Credit",
    "section": "Step 1. Analyze your data",
    "text": "Step 1. Analyze your data\nReview the predictor variables and guess what their role in a credit decision might be. Are there any surprises in the data?\n\n# Group categorical columns based on information\ncategorical_columns = [column for column in data.columns if len(data[column].unique()) &lt;= 5]\n# print(categorical_columns)\n\nfinancial_history = ['CHK_ACCT', 'HISTORY', 'SAV_ACCT', 'OTHER_INSTALL', 'NUM_CREDITS']\ncredit_information = ['INSTALL_RATE', ]\ncustomer_information = ['EMPLOYMENT', 'MALE_DIV', 'MALE_SINGLE', 'MALE_MAR_or_WID', 'CO-APPLICANT', 'GUARANTOR', \n                      'PRESENT_RESIDENT', 'REAL_ESTATE', 'PROP_UNKN_NONE', 'RENT', 'OWN_RES', 'JOB',\n                      'NUM_DEPENDENTS', 'TELEPHONE', 'FOREIGN']\npurpose = ['NEW_CAR', 'USED_CAR', 'FURNITURE', 'RADIO/TV', 'EDUCATION', 'RETRAINING']\n\n\nFinancial history\nWe can see that information about the financial situation has a strong impact on wether the bank assesses a customer as having good or bad credit.\n\ndef stacked_barcharts(columns, ncols=5):\n    nrows = 1 + (len(columns) - 1) // ncols\n    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(14, nrows * 2.5))\n    for i, column in enumerate(columns):\n        # calculate pivot table\n        pivot = pd.crosstab(data['RESPONSE'], data[column])\n        # divide by column sums to get frequency per column\n        freq = pivot.div(pivot.sum())\n        # display as stacked bar chart with 100%\n        ax = axes[i // 5, i % 5] if nrows &gt; 1 else axes[i]\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            freq.transpose().plot(kind='bar', ax=ax, stacked=True, legend=False)\n    for i in range(len(columns), nrows * 5):\n        ax = axes[i // 5, i % 5] if nrows &gt; 1 else axes[i]\n        fig.delaxes(ax)\n    plt.tight_layout()\n\n# good credit is shown in orange (700 cases), bad credit in blue (300 cases)\nstacked_barcharts(financial_history)\n\n\n\n\n\n\n\n\n\n\nCredit information\n\nIncreasing INSTALL_RATE (higher percentage of disposable income) leads to a slight reduction\nCredits with longer duration have a higher ratio of ‘bad’ customers\nCredits with a larger amount tend to have a higher ratio of ‘bad’ customers\n\n\nstacked_barcharts(credit_information)\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8,4))\nfor i, column in enumerate(['DURATION', 'AMOUNT']):\n    for response, group in data[['RESPONSE', column]].groupby('RESPONSE'):\n        group[column].plot.density(ax=axes[i])\n    axes[i].set_title(column)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCustomer information\nSome of the customer related columns show an effect on the credit assessment. In particular, home ownership shows an effect. Older customers are more likely to be good customers.\n\nstacked_barcharts(customer_information)\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(4,4))\nfor response, group in data[['RESPONSE', column]].groupby('RESPONSE'):\n    group[column].plot.density(ax=ax)\nax.set_title(column)\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstacked_barcharts(purpose, ncols=6)"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.2 German Credit.html#step-2-classification-models",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.2 German Credit.html#step-2-classification-models",
    "title": "Case 21.2 German Credit",
    "section": "Step 2 Classification models",
    "text": "Step 2 Classification models\nDivide the data into training and validation partitions, and develop classification models using the following data mining techniques:\n\nlogistic regression\nclassification trees, and\nneural networks\n\n\nX = data.drop(columns=['RESPONSE'])\ny = data['RESPONSE']\ntrain_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size=0.4, random_state=1)#### Define helper functions\n\n\nDefine helper functions\n\ndef confusionMatrices(model, title):\n    print(title + ' - training results')\n    classificationSummary(train_y, model.predict(train_X))\n    print(title + ' - validation results')\n    valid_pred = model.predict(valid_X)\n    classificationSummary(valid_y, valid_pred)\n    return confusion_matrix(valid_y, valid_pred)\n\n\n\nStep 2.1 Logistic regression model\n\nlogit_reg = LogisticRegression(penalty=\"l2\", C=1e42, solver='liblinear')\nlogit_reg.fit(train_X, train_y)\nlogit_reg_confusion = confusionMatrices(logit_reg, 'Logistic regression')\n\nLogistic regression - training results\nConfusion Matrix (Accuracy 0.7967)\n\n       Prediction\nActual   0   1\n     0 106  79\n     1  43 372\nLogistic regression - validation results\nConfusion Matrix (Accuracy 0.7550)\n\n       Prediction\nActual   0   1\n     0  53  62\n     1  36 249\n\n\n\n\nStep 2.2 Decision tree classifier\n\nclassTree = DecisionTreeClassifier()\nclassTree.fit(train_X, train_y)\n\n# Start with an initial guess for parameters\nparam_grid = {\n    'max_depth': [5, 10, 20, 30, 40], \n    'min_samples_split': [20, 40, 60, 80, 90, 100, 110], \n    'min_impurity_decrease': [0, 0.0005, 0.001, 0.005, 0.01], \n}\ngridSearch = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5, n_jobs=-1)\ngridSearch.fit(train_X, train_y)\nprint('Initial score: ', gridSearch.best_score_)\nprint('Initial parameters: ', gridSearch.best_params_)\n\n# Adapt grid based on result from initial grid search\nparam_grid = {\n    'max_depth': list(range(2, 10)), \n    'min_samples_split': list(range(75, 85)), \n    'min_impurity_decrease': [0, 0.0005, 0.001, 0.005, 0.01], \n}\ngridSearch = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5, n_jobs=-1)\ngridSearch.fit(train_X, train_y)\nprint('Improved score: ', gridSearch.best_score_)\nprint('Improved parameters: ', gridSearch.best_params_)\n\nclassTree = gridSearch.best_estimator_\nclassTree_confusion = confusionMatrices(classTree, 'Decision tree')\n\nInitial score:  0.7233333333333334\nInitial parameters:  {'max_depth': 5, 'min_impurity_decrease': 0.005, 'min_samples_split': 80}\nImproved score:  0.725\nImproved parameters:  {'max_depth': 5, 'min_impurity_decrease': 0.005, 'min_samples_split': 75}\nDecision tree - training results\nConfusion Matrix (Accuracy 0.7733)\n\n       Prediction\nActual   0   1\n     0  93  92\n     1  44 371\nDecision tree - validation results\nConfusion Matrix (Accuracy 0.7275)\n\n       Prediction\nActual   0   1\n     0  43  72\n     1  37 248\n\n\n\n\nStep 2.3 Neural network\n\nparam_grid = {\n    'hidden_layer_sizes': list(range(2, 30)),\n}\nneuralNet = MLPClassifier(activation='logistic', solver='lbfgs', random_state=1, max_iter=5000)\n\ngridSearch = GridSearchCV(neuralNet, param_grid, cv=5, n_jobs=-1)\ngridSearch.fit(train_X, train_y)\nprint('Improved score: ', gridSearch.best_score_)\nprint('Improved parameters: ', gridSearch.best_params_)\n\nneuralNet = gridSearch.best_estimator_\nneuralNet_confusion = confusionMatrices(neuralNet, 'Neural network')\n\nImproved score:  0.7516666666666666\nImproved parameters:  {'hidden_layer_sizes': 17}\nNeural network - training results\nConfusion Matrix (Accuracy 0.7617)\n\n       Prediction\nActual   0   1\n     0 102  83\n     1  60 355\nNeural network - validation results\nConfusion Matrix (Accuracy 0.7375)\n\n       Prediction\nActual   0   1\n     0  56  59\n     1  46 239"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.2 German Credit.html#step-3.-analyze-the-models",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.2 German Credit.html#step-3.-analyze-the-models",
    "title": "Case 21.2 German Credit",
    "section": "Step 3. Analyze the models",
    "text": "Step 3. Analyze the models\nChoose one model from each technique and report the confusion matrix and the cost/gain matrix for the validation data. Which technique has the highest net profit?\nMultiply the confusion matrix with the cost and gain matrices element wise and sum the products.\n\n# Logistic regression\nprint('cost: ', (logit_reg_confusion * cost).to_numpy().sum())\nprint('gain: ', (logit_reg_confusion * gain).to_numpy().sum())\n\ncost:  24200\ngain:  -12700\n\n\n\n# Decision tree classifier\nprint('cost: ', (classTree_confusion * cost).to_numpy().sum())\nprint('gain: ', (classTree_confusion * gain).to_numpy().sum())\n\ncost:  25700\ngain:  -14200\n\n\n\n# Neural network classifier\nprint('cost: ', (neuralNet_confusion * cost).to_numpy().sum())\nprint('gain: ', (neuralNet_confusion * gain).to_numpy().sum())\n\ncost:  28900\ngain:  -17400"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.2 German Credit.html#step-4.-improve-the-model",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.2 German Credit.html#step-4.-improve-the-model",
    "title": "Case 21.2 German Credit",
    "section": "Step 4. Improve the model",
    "text": "Step 4. Improve the model\nLet us try and improve our performance. Rather than accept the default classification of all applicants’ credit status, use the estimated probabilities (propensities) from the logistic regression (where success means 1) as a basis for selecting the best credit risks first, followed by poorer-risk applicants. Create a vector containing the net profit for each record in the validation set. Use this vector to create a decile-wise lift chart for the validation set that incorporates the net profit.\n\nHow far into the validation data should you go to get maximum net profit? (Often, this is specified as a percentile or rounded to deciles.)\nIf this logistic regression model is used to score to future applicants, what “probability of success” cutoff should be used in extending credit?\n\n\nlogit_reg_proba = logit_reg.predict_proba(valid_X)\nlogit_result = pd.DataFrame({'actual': valid_y, \n                             'p(0)': [p[0] for p in logit_reg_proba],\n                             'p(1)': [p[1] for p in logit_reg_proba],\n                             'profit': [100 if v == 1 else -500 for v in valid_y]\n                            })\n\n\ndf = logit_result.sort_values(by=['p(1)'], ascending=False)\ngainsChart(df.profit)\nplt.show()\n\n\n\n\n\n\n\n\na.) The cumulative gain grows until record 150. This corresponds to 150 / 400 = 37.5% of the validation set.\nb.) If we plot the ordered probability p(1) we can determine the cutoff value visually.\n\ndf['p(1)'].reset_index(drop=True).plot()\nplt.show()\n\n\n\n\n\n\n\n\nAlternatively, we can look at the 150-th row of the sorted data frame df.\n\ndf.iloc[150:151, :]\n\n\n\n\n\n\n\n\nactual\np(0)\np(1)\nprofit\n\n\nOBS#\n\n\n\n\n\n\n\n\n633\n1\n0.132674\n0.867326\n100\n\n\n\n\n\n\n\nBased on this analysis, we would choose a cut-off of 0.87."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.7 Direct-Mail Fundraising.html",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.7 Direct-Mail Fundraising.html",
    "title": "Case 21.7 Direct-Mail Fundraising",
    "section": "",
    "text": "2019 Galit Shmueli, Peter C. Bruce, Peter Gedeck\n\nCase study included in\nData Mining for Business Analytics: Concepts, Techniques, and Applications in Python (First Edition) Galit Shmueli, Peter C. Bruce, Peter Gedeck, and Nitin R. Patel. 2019.\n%matplotlib inline\nfrom pathlib import Path\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.neural_network import MLPClassifier\nimport matplotlib.pylab as plt\n\nfrom dmba import classificationSummary, gainsChart\n\nDATA = Path('.').resolve().parent / 'data'"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.7 Direct-Mail Fundraising.html#step-2.1-select-classification-tool-and-parameters",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.7 Direct-Mail Fundraising.html#step-2.1-select-classification-tool-and-parameters",
    "title": "Case 21.7 Direct-Mail Fundraising",
    "section": "Step 2.1: Select classification tool and parameters",
    "text": "Step 2.1: Select classification tool and parameters\nRun at least two classification models of your choosing. Be sure NOT to use TARGET_D in your analysis. Describe the two models that you chose, with sufficient detail (method, parameters, variables, etc.) so that it can be replicated.\n\nLogistic regression\n\nlogit_reg = LogisticRegression(penalty=\"l2\", C=1e42, solver='liblinear')\nlogit_reg.fit(train_X, train_y)\nconfusionMatrices(logit_reg, 'Logistic regression')\n\nLogistic regression - training results\nConfusion Matrix (Accuracy 0.5673)\n\n       Prediction\nActual   0   1\n     0 476 446\n     1 364 586\nLogistic regression - validation results\nConfusion Matrix (Accuracy 0.5737)\n\n       Prediction\nActual   0   1\n     0 349 289\n     1 243 367\n\n\n\n\nClassification tree\n\nclassTree = DecisionTreeClassifier()\nclassTree.fit(train_X, train_y)\n\n# Start with an initial guess for parameters\nparam_grid = {\n    'max_depth': [10, 20, 30, 40], \n    'min_samples_split': [20, 40, 60, 80, 100], \n    'min_impurity_decrease': [0, 0.0005, 0.001, 0.005, 0.01], \n}\ngridSearch = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5, n_jobs=-1)\ngridSearch.fit(train_X, train_y)\nprint('Initial score: ', gridSearch.best_score_)\nprint('Initial parameters: ', gridSearch.best_params_)\n\n# Adapt grid based on result from initial grid search\nparam_grid = {\n    'max_depth': list(range(2, 16)), \n    'min_samples_split': [96, 97, 98, 99, 100, 101, 102, 103], \n    'min_impurity_decrease': [0, 0.0005, 0.001, 0.005, 0.01], \n}\ngridSearch = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5, n_jobs=-1)\ngridSearch.fit(train_X, train_y)\nprint('Improved score: ', gridSearch.best_score_)\nprint('Improved parameters: ', gridSearch.best_params_)\n\nclassTree = gridSearch.best_estimator_\nconfusionMatrices(classTree, 'Decision tree')\n\nInitial score:  0.5747664884135473\nInitial parameters:  {'max_depth': 10, 'min_impurity_decrease': 0.005, 'min_samples_split': 20}\nImproved score:  0.5747664884135473\nImproved parameters:  {'max_depth': 2, 'min_impurity_decrease': 0.005, 'min_samples_split': 96}\nDecision tree - training results\nConfusion Matrix (Accuracy 0.5855)\n\n       Prediction\nActual   0   1\n     0 647 275\n     1 501 449\nDecision tree - validation results\nConfusion Matrix (Accuracy 0.5489)\n\n       Prediction\nActual   0   1\n     0 432 206\n     1 357 253\n\n\n\n\nBagging\n\n# we use the classification tree as the base estimator\nbagging = BaggingClassifier(classTree, max_samples=0.5, max_features=0.5)\nbagging.fit(train_X, train_y)\nconfusionMatrices(bagging, 'Bagged Decision tree')\n\nBagged Decision tree - training results\nConfusion Matrix (Accuracy 0.5844)\n\n       Prediction\nActual   0   1\n     0 644 278\n     1 500 450\nBagged Decision tree - validation results\nConfusion Matrix (Accuracy 0.5553)\n\n       Prediction\nActual   0   1\n     0 440 198\n     1 357 253\n\n\n\n\nAdaboost\n\nadaboost = AdaBoostClassifier(n_estimators=100, base_estimator=classTree)\nadaboost.fit(train_X, train_y)\nconfusionMatrices(adaboost, 'Boosted decision tree')\n\nBoosted decision tree - training results\nConfusion Matrix (Accuracy 0.5855)\n\n       Prediction\nActual   0   1\n     0 647 275\n     1 501 449\nBoosted decision tree - validation results\nConfusion Matrix (Accuracy 0.5489)\n\n       Prediction\nActual   0   1\n     0 432 206\n     1 357 253\n\n\n\n\nRandom forest\n\nrfModel = RandomForestClassifier(max_features=3, min_samples_split=300, \n                                 random_state=0, n_estimators=100, criterion='entropy')\nrfModel.fit(train_X, train_y)\nconfusionMatrices(rfModel, 'Random forest')\n\nRandom forest - training results\nConfusion Matrix (Accuracy 0.6202)\n\n       Prediction\nActual   0   1\n     0 577 345\n     1 366 584\nRandom forest - validation results\nConfusion Matrix (Accuracy 0.5417)\n\n       Prediction\nActual   0   1\n     0 350 288\n     1 284 326\n\n\n\n\nLinear discriminant analysis\n\nldaModel = LinearDiscriminantAnalysis()\nldaModel.fit(train_X, train_y)\nconfusionMatrices(ldaModel, 'Linear discriminant analysis')\n\nLinear discriminant analysis - training results\nConfusion Matrix (Accuracy 0.5652)\n\n       Prediction\nActual   0   1\n     0 477 445\n     1 369 581\nLinear discriminant analysis - validation results\nConfusion Matrix (Accuracy 0.5753)\n\n       Prediction\nActual   0   1\n     0 350 288\n     1 242 368\n\n\n\n\nNeural network\n\nscaleInput = MinMaxScaler()\nscaleInput.fit(train_X * 1.0)\n\nneuralNet = MLPClassifier(hidden_layer_sizes=(10), activation='logistic', solver='lbfgs', max_iter=3000, \n                          random_state=1)\nneuralNet.fit(scaleInput.transform(train_X), train_y)\n\nprint('Neural Network - training results')\nclassificationSummary(train_y, neuralNet.predict(scaleInput.transform(train_X)))\nprint('Neural Network - validation results')\nclassificationSummary(valid_y, neuralNet.predict(scaleInput.transform(valid_X)))\n\nNeural Network - training results\nConfusion Matrix (Accuracy 0.7222)\n\n       Prediction\nActual   0   1\n     0 630 292\n     1 228 722\nNeural Network - validation results\nConfusion Matrix (Accuracy 0.5337)\n\n       Prediction\nActual   0   1\n     0 318 320\n     1 262 348"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.7 Direct-Mail Fundraising.html#step-2.2-classification-under-asymmetric-response-and-cost",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.7 Direct-Mail Fundraising.html#step-2.2-classification-under-asymmetric-response-and-cost",
    "title": "Case 21.7 Direct-Mail Fundraising",
    "section": "Step 2.2 Classification under asymmetric response and cost",
    "text": "Step 2.2 Classification under asymmetric response and cost\nWhat is the reasoning behind using weighted sampling to produce a training set with equal numbers of donors and non-donors? Why not use a simple random sample from the original dataset?\nIn this data set, the cases of interest (the “1’s”) are fairly rare. Since sample sizes for training and validating models are limited by the algorithms, it is best to have fairly equal numbers of 0’s and 1’s to give the algorithm a good shot at finding out what distinguishes the two classes. Think of it this way - if you have 100 1’s and 100 0’s, adding another 800 0’s is not going to help nearly as much as adding 400 of each."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.7 Direct-Mail Fundraising.html#step-2.3-calculate-net-profit",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.7 Direct-Mail Fundraising.html#step-2.3-calculate-net-profit",
    "title": "Case 21.7 Direct-Mail Fundraising",
    "section": "Step 2.3 Calculate net profit",
    "text": "Step 2.3 Calculate net profit\nFor each method, calculate the cumulative gains of net profit for both the training and validation sets based on the actual response rate (5.1%.) Again, the expected donation, given that they are donors, is \\$13.00, and the total cost of each mailing is \\$0.68.\n(Hint: To calculate estimated net profit, we will need to undo the effects of the weighted sampling and calculate the net profit that would reflect the actual response distribution of 5.1% donors and 94.9% non-donors. To do this, divide each row’s net profit by the oversampling weights applicable to the actual status of that row. The oversampling weight for actual donors is 50%/5.1% = 9.8. The oversampling weight for actual non-donors is 50%/94.9% = 0.53.)\nThe cost for each mailing is \\$0.68, donors give on average \\$13. This leads to a net profit of \\$13.00-\\$0.68 for donors and a net loss of \\$0.68 for non-donors.\nTo take the oversampling into account, we need to adjust the net profit as follows:\nDonors: \\[ ($13.00-$0.68) \\frac{5.1\\%}{50\\%} \\approx $1.25664 \\] Non-donors: \\[ -$0.68 \\frac{94.9\\%}{50\\%} \\approx -$1.29064 \\]\nTo calculate the lift of the net profit we can define the following function"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.7 Direct-Mail Fundraising.html#step-2.4-draw-cumulative-gains-curves",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.7 Direct-Mail Fundraising.html#step-2.4-draw-cumulative-gains-curves",
    "title": "Case 21.7 Direct-Mail Fundraising",
    "section": "Step 2.4 Draw cumulative gains curves",
    "text": "Step 2.4 Draw cumulative gains curves\nDraw the different models’ %each model’s net profit cumulative gains curves for the validation set in a single plot (net profit on the \\(y\\)-axis, proportion of list or number mailed on the \\(x\\)-axis). Is there a model that dominates?\nTo calculate the gain of the net profit we can define the following function\n\nnet_profit = [(13 - 0.68)*5.1/50 if respond == 1 else -0.68*94.9/50 for respond in valid_y]\nnet_profit = [(13 - 0.68)/9.8 if respond == 1 else -0.68/0.53 for respond in valid_y]\n\ndef addLiftChart(predict_proba, label, ax=None):\n    df = pd.DataFrame(data={'prob': [p[1] for p in predict_proba], 'actual': net_profit})\n    df = df.sort_values(by=['prob'], ascending=False)\n    ax = gainsChart(df.actual, ax=ax, label=label)\n    return ax\n\nax = addLiftChart(logit_reg.predict_proba(valid_X), 'LogisticRegression')\naddLiftChart(classTree.predict_proba(valid_X), 'ClassificationTree', ax=ax)\naddLiftChart(bagging.predict_proba(valid_X), 'Bagging', ax=ax)\naddLiftChart(adaboost.predict_proba(valid_X), 'AdaBoost', ax=ax)\naddLiftChart(ldaModel.predict_proba(valid_X), 'Linear Discriminant Analysis', ax=ax)\naddLiftChart(rfModel.predict_proba(valid_X), 'Random Forest', ax=ax)\naddLiftChart(neuralNet.predict_proba(scaleInput.transform(valid_X)), 'Neural network', ax=ax)\n\nax.vlines(x=[len(valid_y) * 0.3, len(valid_y) * 0.7], ymin=-40, ymax=80, linestyles='dotted')\nplt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\nplt.show()\n\n\n\n\n\n\n\n\nBased on the lift charts, we would discard the AdaBoost, the Decision Tree classifier, Bagging and the neural network classifier. Logistic regression, linear discriminant analysis, and random forest perform well overall. These three classifiers perform initially comparable up to about 30% of the dataset (first vertical dashed line). The lift of the random forest classifier then drops until at about 80% (second vertical dashed line) the gains curve is joining the other two classifiers.."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.7 Direct-Mail Fundraising.html#step-2.5-select-best-model",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.7 Direct-Mail Fundraising.html#step-2.5-select-best-model",
    "title": "Case 21.7 Direct-Mail Fundraising",
    "section": "Step 2.5: Select best model",
    "text": "Step 2.5: Select best model\nFrom your answer in (2.4), what do you think is the “best” model?\nConsidering these results, we select either the logistic regression or the linear discriminant analysis model.\nYour result will depend considerably on which of the classifier you chose and how you split your dataset into training and validation. Explore what happens when you vary the random seed in the dataset split."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.7 Direct-Mail Fundraising.html#step-3.6",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.7 Direct-Mail Fundraising.html#step-3.6",
    "title": "Case 21.7 Direct-Mail Fundraising",
    "section": "Step 3.6:",
    "text": "Step 3.6:\nUsing your “best” model from Step 2.5, which of these candidates do you predict as donors and non-donors? List them in descending order of the probability of being a donor. Starting at the top of this sorted list, roughly how far down would you go in a mailing campaign?\nWe apply the logisitic regression model to the FutureFundraising.csv dataset.\n\nmailingCandidates = pd.DataFrame({\n    'Row Id': futureFundraising_df['Row Id'],\n    'prediction': logit_reg.predict_proba(futureFundraising_df.drop(columns=['Row Id']))[:,1]\n})\nmailingCandidates = mailingCandidates.sort_values(by=['prediction'], ascending=False).reset_index(drop=True)\nmailingCandidates.head(10)\n\n\n\n\n\n\n\n\nRow Id\nprediction\n\n\n\n\n0\n1014\n0.776190\n\n\n1\n722\n0.775001\n\n\n2\n120\n0.764403\n\n\n3\n56\n0.760704\n\n\n4\n26\n0.756007\n\n\n5\n778\n0.754514\n\n\n6\n215\n0.752654\n\n\n7\n165\n0.748111\n\n\n8\n1263\n0.742441\n\n\n9\n792\n0.741868\n\n\n\n\n\n\n\nBased on these results, you would want to mail at least 50% of the list - we are pretty confident of increasing profit to that point. Mailing to more than 50% of the list seems counterproductive. Business considerations (e.g. resources available, aggressiveness of profit targets, etc.) could govern a choice around 30% range.\nConsidering that we see variation of the gains charts with different splits into training and validation sets, also indictes that we should explore the analysis more to come up with a reasonable cutoff for mailing."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.9 Time series data - Forecasting public transportation demands.html",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.9 Time series data - Forecasting public transportation demands.html",
    "title": "Case 21.9 Time series data - Forecasting public transportation demands",
    "section": "",
    "text": "2019 Galit Shmueli, Peter C. Bruce, Peter Gedeck\n\nCase study included in\nData Mining for Business Analytics: Concepts, Techniques, and Applications in Python (First Edition) Galit Shmueli, Peter C. Bruce, Peter Gedeck, and Nitin R. Patel. 2019.\n%matplotlib inline\n\nfrom pathlib import Path\nimport pandas as pd\nimport matplotlib.pylab as plt\nimport statsmodels.formula.api as sm\nfrom statsmodels.tsa import tsatools, stattools\nfrom dmba import regressionSummary\n\n\nDATA = Path('.').resolve().parent / 'data'"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.9 Time series data - Forecasting public transportation demands.html#step-1-exploratory-analysis",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.9 Time series data - Forecasting public transportation demands.html#step-1-exploratory-analysis",
    "title": "Case 21.9 Time series data - Forecasting public transportation demands",
    "section": "Step 1: Exploratory analysis",
    "text": "Step 1: Exploratory analysis\n\nUse exploratory analysis to identify the components of this time series. Is there a trend? Is there seasonality? If so, how many “seasons” are there? Are there any other visible patterns? Are the patterns global (the same throughout the series) or local?\nConsider the frequency of the data from a practical and technical point of view. What are some options?\nCompare the weekdays and weekends. How do they differ? Consider how these differences can be captured by different methods.\nExamine the series for missing values or unusual values. Think of solutions.\nBased on the patterns that you found in the data, which models or methods should be considered?\n\n\ntimeSteps = pd.to_datetime(data.DATE + ' ' +data.TIME, infer_datetime_format=True)\nts = pd.Series(data.DEMAND.values, index=timeSteps, name='Demand')\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(14,6))\nts.plot(ax=axes[0])\nts['2005-03-02'].plot(ax=axes[1])\nts['2005-03-03'].plot(ax=axes[1])\nts['2005-03-05'].plot(ax=axes[2])\nts['2005-03-06'].plot(ax=axes[2])\naxes[0].set_title('Full time series')\naxes[1].set_title('Two days (2005-03-02 and 2005-03-03)')\naxes[2].set_title('Weekend (2005-03-05 and 2005-03-06)')\nfor ax in axes:\n    ax.set_ylabel('Demand')\nplt.show()\n\n\n\n\n\n\n\n\nThe graph shows no trend. We see a weakly pattern with high demand from Monday to Friday, lower demand on Saturday, and even less demand on Sunday. It seems that the demand is slightly decreasing from Monday to Friday.\nThe workdays have a bimodal pattern with increasing demand until 14:00, a drop in demand until 16:00, and increasing again with the daily peak from 18:00 to 20:00. Saturdays show only a single peak and Sundays have no discernible pattern due to low demand.\nThe dataset has two seasonalities that we need to consider in our forecasting, the daily and the weekly seasonality.\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(14,6))\nts['2005-03-17'].plot(ax=axes[0], style='.-')\nts['2005-03-21'].plot(ax=axes[1], style='.-')\naxes[0].set_title('2005-03-17')\naxes[1].set_title('2005-03-21')\nfor ax in axes:\n    ax.set_ylabel('Demand')\nplt.show()\n\n\n\n\n\n\n\n\nThere are two days with an unusual daily pattern. On 2005-03-17 we see zero demand at 15:30 and on 2005-03-21 three data points with zero demand from 17:45 to 18:15 (the demand of 3 at 17:30 is also highly unusual). It is likely that these are errors in the dataset. There are various options to deal with these data points:\n\nuse as it: the influence of these 5 data points on the model should be minimal due to the large size of the dataset\nremove for the model building\nimpute data points: we could for example replace the data points with the average of the two neighboring data points; in the case of 2005-03-21, this would require a linear interpolation for the four unusual data points. It is possible that the next data points are influenced by the cause of the error as well (the low value of 3 at the 17:30 time point is an indication of this), so we may need a more elaborate way of getting reliable estimates.\nmodel based imputation of data points: first build a model using either approach (1) or (2) and replace the values using the predictions for these data points\n\nA thorough analysis of the way the data were gathered is required to make the best choice in this case. In the following we use method (1). However, feel free to explore the other three approaches in your solution."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.6 Segmenting Consumers of Bath Soap.html",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.6 Segmenting Consumers of Bath Soap.html",
    "title": "Case 21.6 Segmenting Consumers of Bath Soap",
    "section": "",
    "text": "2019 Galit Shmueli, Peter C. Bruce, Peter Gedeck\n\nCase study included in\nData Mining for Business Analytics: Concepts, Techniques, and Applications in Python (First Edition) Galit Shmueli, Peter C. Bruce, Peter Gedeck, and Nitin R. Patel. 2019.\n%matplotlib inline\n\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import BaggingClassifier\n\n\nimport matplotlib.pylab as plt\n\nfrom dmba import classificationSummary, gainsChart\n\nfrom IPython.display import display_html\n\n\nDATA = Path('.').resolve().parent / 'data'"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.6 Segmenting Consumers of Bath Soap.html#load-the-data",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.6 Segmenting Consumers of Bath Soap.html#load-the-data",
    "title": "Case 21.6 Segmenting Consumers of Bath Soap",
    "section": "Load the data",
    "text": "Load the data\n\nbathSoap_df = pd.read_csv(DATA / 'BathSoapHousehold.csv')\nprint(bathSoap_df.shape)\nbathSoap_df.head()\n\n(600, 46)\n\n\n\n\n\n\n\n\n\nMember id\nSEC\nFEH\nMT\nSEX\nAGE\nEDU\nHS\nCHILD\nCS\n...\nPropCat 6\nPropCat 7\nPropCat 8\nPropCat 9\nPropCat 10\nPropCat 11\nPropCat 12\nPropCat 13\nPropCat 14\nPropCat 15\n\n\n\n\n0\n1010010\n4\n3\n10\n1\n4\n4\n2\n4\n1\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n0.000000\n0.028037\n0.0\n0.130841\n0.339564\n\n\n1\n1010020\n3\n2\n10\n2\n2\n4\n4\n2\n1\n...\n0.347048\n0.026834\n0.016100\n0.014311\n0.0\n0.059034\n0.000000\n0.0\n0.080501\n0.000000\n\n\n2\n1014020\n2\n3\n10\n2\n4\n5\n6\n4\n1\n...\n0.121212\n0.033550\n0.010823\n0.008658\n0.0\n0.000000\n0.016234\n0.0\n0.561688\n0.003247\n\n\n3\n1014030\n4\n0\n0\n0\n4\n0\n0\n5\n0\n...\n0.000000\n0.000000\n0.000000\n0.000000\n0.0\n0.000000\n0.000000\n0.0\n0.600000\n0.000000\n\n\n4\n1014190\n4\n1\n10\n2\n3\n4\n4\n3\n1\n...\n0.000000\n0.000000\n0.048193\n0.000000\n0.0\n0.000000\n0.000000\n0.0\n0.144578\n0.000000\n\n\n\n\n5 rows × 46 columns"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.6 Segmenting Consumers of Bath Soap.html#clusters-based-on-purchase-behavior",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.6 Segmenting Consumers of Bath Soap.html#clusters-based-on-purchase-behavior",
    "title": "Case 21.6 Segmenting Consumers of Bath Soap",
    "section": "Clusters based on “purchase behavior”",
    "text": "Clusters based on “purchase behavior”\nNote: Some thought is needed about brand loyalty. For brand loyalty indicators, we have data on\n\npercent of purchases devoted to major brands (i.e. is a customer a total devotee of brand A?), brandIndicator\na catch-all variable for percent of purchases devoted to other smaller brands (to reduce complexity of analysis), and otherBrandIndicator\na derived variable that indicates the maximum share devoted to any one brand.\n\nSince CRISA is compiling this data for general marketing use, and not on behalf of one particular brand, we can say a customer who is fully devoted to brand A is similar to a customer fully devoted to brand B - both are fully loyal customers in their behavior. But if we include all the brand shares in the clustering, the analysis will treat those two customers as very different.\n\nNumber of different brands: No. of Brands\nSwitching between brands: Brand Runs\nProportion of purchases that go to different brands: We use the information in the brandIndicator to determine the maximum proportion a customer spends on one brand (new variable maxBrandIndicator)\n\nWe derive the value of maxBrandIndicator by taking the maximum of all specific brand indicators.\n\nbathSoap_df['maxBrandIndicator'] = bathSoap_df[brandIndicator].max(axis=1)\n\nFor this analysis, we use all purchaseIndicator, maxBrandIndicator and otherBrandIndicator as a description of the customers purchase behavior\n\nbehaviorIndicator = list(purchaseIndicator) + list(otherBrandIndicator) + ['maxBrandIndicator']\nprint(behaviorIndicator)\n\n['No. of Brands', 'Brand Runs', 'Total Volume', 'No. of  Trans', 'Value', 'Trans / Brand Runs', 'Vol/Tran', 'Avg. Price ', 'Others 999', 'maxBrandIndicator']\n\n\n\nNormalizing the data and definition of helper functions\n\n# Normalize the data\nbathSoap_df_norm = (bathSoap_df - bathSoap_df.mean())/bathSoap_df.std()\n\ndef clusterSizes(kmeans):\n    return pd.Series(kmeans.labels_).value_counts().sort_index()\n\n\ndef clusterCenters(kmeans, indicator):\n    return bathSoap_df_norm[indicator].groupby(kmeans.labels_).mean()\n\n\n\nTwo clusters\n\nclusters = KMeans(n_clusters=2, random_state=1).fit(bathSoap_df_norm[behaviorIndicator])\nprint(clusterSizes(clusters))\nclusterCenters(clusters, behaviorIndicator)\n\n0    317\n1    283\ndtype: int64\n\n\n\n\n\n\n\n\n\nNo. of Brands\nBrand Runs\nTotal Volume\nNo. of Trans\nValue\nTrans / Brand Runs\nVol/Tran\nAvg. Price\nOthers 999\nmaxBrandIndicator\n\n\n\n\n0\n0.483611\n0.632865\n0.158222\n0.522115\n0.306960\n-0.261283\n-0.285383\n0.279689\n0.488928\n-0.588026\n\n\n1\n-0.541712\n-0.708898\n-0.177232\n-0.584843\n-0.343838\n0.292674\n0.319669\n-0.313291\n-0.547669\n0.658673\n\n\n\n\n\n\n\nComment: The two clusters are well-separated on everything, except transaction volume. - Cluster 0 (n=317) is high activity & value, with low loyalty. - Cluster 1 (n=283) is the reverse.\n(“Value” here is the meaning attached to the variable - total dollar value of purchases, not some broader meaning.) Note: Due to the randomization element in the k-means process, different runs can produce different cluster results.\n\nclusters = KMeans(n_clusters=3, random_state=1).fit(bathSoap_df_norm[behaviorIndicator])\nprint(clusterSizes(clusters))\nclusterCenters(clusters, behaviorIndicator)\n\n0    166\n1    259\n2    175\ndtype: int64\n\n\n\n\n\n\n\n\n\nNo. of Brands\nBrand Runs\nTotal Volume\nNo. of Trans\nValue\nTrans / Brand Runs\nVol/Tran\nAvg. Price\nOthers 999\nmaxBrandIndicator\n\n\n\n\n0\n0.950737\n1.099320\n0.635975\n1.082139\n0.767309\n-0.254805\n-0.190209\n0.127343\n0.254809\n-0.477311\n\n\n1\n-0.275933\n-0.215651\n-0.532365\n-0.415564\n-0.454992\n-0.247321\n-0.273599\n0.228323\n0.599030\n-0.532205\n\n\n2\n-0.493460\n-0.723619\n0.184632\n-0.411452\n-0.054460\n0.607736\n0.585353\n-0.458712\n-1.128269\n1.240427\n\n\n\n\n\n\n\nComment: - Cluster 0 (n=166) is not at all loyal, favoring many brands, and of high value. - Cluster 1 (n=259) is also not very loyal, but may be of the least interest since its customers have the lowest value. - Cluster 2 (n=175) is highly loyal, favoring main brands and bigger individual purchases, with middling overall value."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.6 Segmenting Consumers of Bath Soap.html#clusters-based-on-basis-for-purchase",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.6 Segmenting Consumers of Bath Soap.html#clusters-based-on-basis-for-purchase",
    "title": "Case 21.6 Segmenting Consumers of Bath Soap",
    "section": "Clusters based on “basis for purchase”",
    "text": "Clusters based on “basis for purchase”\nThe variables used are: Pur Vol No Promo - %, Pur Vol Promo 6 %, Pur Vol Other Promo %, all price categories, selling propositions 5 and 14 (most people seemed to be responding to one or the other of these promotions/propositions).\n\npurchaseBasisIndicator = list(withinPromotionIndicator) + list(priceCategoryIndicator) \npurchaseBasisIndicator.extend(['PropCat 5', 'PropCat 14'])\nprint(purchaseBasisIndicator)\n\n['Pur Vol No Promo - %', 'Pur Vol Promo 6 %', 'Pur Vol Other Promo %', 'Pr Cat 1', 'Pr Cat 2', 'Pr Cat 3', 'Pr Cat 4', 'PropCat 5', 'PropCat 14']\n\n\n\nclusters = KMeans(n_clusters=2, random_state=1).fit(bathSoap_df_norm[purchaseBasisIndicator])\nprint(clusterSizes(clusters))\nclusterCenters(clusters, purchaseBasisIndicator)\n\n0     77\n1    523\ndtype: int64\n\n\n\n\n\n\n\n\n\nPur Vol No Promo - %\nPur Vol Promo 6 %\nPur Vol Other Promo %\nPr Cat 1\nPr Cat 2\nPr Cat 3\nPr Cat 4\nPropCat 5\nPropCat 14\n\n\n\n\n0\n0.205410\n-0.402586\n0.179079\n-0.791774\n-1.134101\n2.387450\n-0.334659\n-1.105730\n2.391835\n\n\n1\n-0.030242\n0.059272\n-0.026365\n0.116571\n0.166971\n-0.351498\n0.049271\n0.162794\n-0.352144\n\n\n\n\n\n\n\nComment: The two clusters are well-separated across most variables. - Cluster 0 (n=77) is notable for its responsiveness to price category 3 and selling proposition 14 coupled with aversion to price categories 1 and 2, and selling proposition 5. - Cluster 1 (n=523) shows a less clear profile\n\nclusters = KMeans(n_clusters=3, random_state=1).fit(bathSoap_df_norm[purchaseBasisIndicator])\nprint(clusterSizes(clusters))\nclusterCenters(clusters, purchaseBasisIndicator)\n\n0     74\n1    429\n2     97\ndtype: int64\n\n\n\n\n\n\n\n\n\nPur Vol No Promo - %\nPur Vol Promo 6 %\nPur Vol Other Promo %\nPr Cat 1\nPr Cat 2\nPr Cat 3\nPr Cat 4\nPropCat 5\nPropCat 14\n\n\n\n\n0\n0.256679\n-0.434717\n0.135482\n-0.802892\n-1.152356\n2.432767\n-0.352060\n-1.136908\n2.436529\n\n\n1\n0.349656\n-0.287933\n-0.208460\n0.062932\n0.258829\n-0.336154\n-0.042901\n0.195635\n-0.335935\n\n\n2\n-1.742234\n1.605076\n0.818594\n0.334189\n-0.265602\n-0.369225\n0.458320\n0.002103\n-0.373063\n\n\n\n\n\n\n\nComment: - Cluster 0 (n=74) has the same profile as cluster 0 in the two cluster case. - Cluster 1 (n=429) corresponds mainly to cluster 1 in the two cluster case. - Cluster 2 (n=97) needs promostions, likes price categories 1 and 4, and is not responsive to the two selling propositions."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.6 Segmenting Consumers of Bath Soap.html#clusters-based-on-all-of-the-above-variables",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.6 Segmenting Consumers of Bath Soap.html#clusters-based-on-all-of-the-above-variables",
    "title": "Case 21.6 Segmenting Consumers of Bath Soap",
    "section": "Clusters based on all of the above variables",
    "text": "Clusters based on all of the above variables\n\ncombinedIndicator = behaviorIndicator + purchaseBasisIndicator \nclusters = KMeans(n_clusters=2, random_state=1).fit(bathSoap_df_norm[combinedIndicator])\nprint(clusterSizes(clusters))\nclusterCenters(clusters, combinedIndicator).transpose()\n\n0    528\n1     72\ndtype: int64\n\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\nNo. of Brands\n0.076539\n-0.561285\n\n\nBrand Runs\n0.107685\n-0.789690\n\n\nTotal Volume\n-0.017671\n0.129590\n\n\nNo. of Trans\n0.057712\n-0.423219\n\n\nValue\n0.070433\n-0.516510\n\n\nTrans / Brand Runs\n-0.143673\n1.053602\n\n\nVol/Tran\n-0.074915\n0.549376\n\n\nAvg. Price\n0.178255\n-1.307201\n\n\nOthers 999\n0.171574\n-1.258207\n\n\nmaxBrandIndicator\n-0.193782\n1.421067\n\n\nPur Vol No Promo - %\n-0.026127\n0.191600\n\n\nPur Vol Promo 6 %\n0.056392\n-0.413540\n\n\nPur Vol Other Promo %\n-0.029475\n0.216152\n\n\nPr Cat 1\n0.109357\n-0.801955\n\n\nPr Cat 2\n0.160177\n-1.174631\n\n\nPr Cat 3\n-0.332833\n2.440774\n\n\nPr Cat 4\n0.044785\n-0.328422\n\n\nPropCat 5\n0.155090\n-1.137330\n\n\nPropCat 14\n-0.333261\n2.443912\n\n\n\n\n\n\n\nComment: The two clusters are separated on almost all variables, Value being an important exception.\n\ndef clusterDemographics(kmeans):\n    return bathSoap_df[demographicIndicators].groupby(kmeans.labels_).mean()\n\nclusterDemographics(clusters).transpose()\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\nSEC\n2.375000\n3.416667\n\n\nFEH\n2.049242\n2.041667\n\n\nMT\n8.250000\n7.652778\n\n\nSEX\n1.763258\n1.555556\n\n\nAGE\n3.246212\n2.972222\n\n\nEDU\n4.268939\n2.388889\n\n\nHS\n4.198864\n4.138889\n\n\nCHILD\n3.202652\n3.458333\n\n\nCS\n0.937500\n0.888889\n\n\nAffluence Index\n18.159091\n8.666667\n\n\n\n\n\n\n\nCluster 1 (n=72) is the more loyal, with lower socioeconomic status, educational level, and affluence.\n\ndef display_side_by_side(*args):\n    html_str = ''.join(df.to_html() for df in args)\n    display_html(html_str.replace('table','table style=\"display:inline\"'),raw=True)\n\n\ncombinedIndicator = behaviorIndicator + purchaseBasisIndicator \nclusters = KMeans(n_clusters=3, random_state=1).fit(bathSoap_df_norm[combinedIndicator])\nprint(clusterSizes(clusters))\ndisplay_side_by_side(clusterCenters(clusters, combinedIndicator).transpose(),\n                     clusterDemographics(clusters).transpose())\n\n0     72\n1    248\n2    280\ndtype: int64\n\n\n\n  \n    \n      \n      0\n      1\n      2\n    \n  \n  \n    \n      No. of Brands\n      -0.508532\n      -0.346872\n      0.437995\n    \n    \n      Brand Runs\n      -0.761636\n      -0.474887\n      0.616463\n    \n    \n      Total Volume\n      0.137098\n      0.043343\n      -0.073643\n    \n    \n      No. of  Trans\n      -0.390544\n      -0.412782\n      0.466032\n    \n    \n      Value\n      -0.513577\n      -0.042678\n      0.169863\n    \n    \n      Trans / Brand Runs\n      1.004705\n      -0.018763\n      -0.241734\n    \n    \n      Vol/Tran\n      0.520628\n      0.373779\n      -0.464937\n    \n    \n      Avg. Price\n      -1.310393\n      -0.196607\n      0.511096\n    \n    \n      Others 999\n      -1.248331\n      -0.225977\n      0.521151\n    \n    \n      maxBrandIndicator\n      1.396638\n      0.288194\n      -0.614393\n    \n    \n      Pur Vol No Promo - %\n      0.196872\n      0.283258\n      -0.301510\n    \n    \n      Pur Vol Promo 6 %\n      -0.413540\n      -0.299317\n      0.371448\n    \n    \n      Pur Vol Other Promo %\n      0.207402\n      -0.083541\n      0.020661\n    \n    \n      Pr Cat 1\n      -0.797347\n      -0.451972\n      0.605350\n    \n    \n      Pr Cat 2\n      -1.204639\n      0.550190\n      -0.177547\n    \n    \n      Pr Cat 3\n      2.470385\n      -0.293348\n      -0.375420\n    \n    \n      Pr Cat 4\n      -0.327801\n      0.178150\n      -0.073498\n    \n    \n      PropCat 5\n      -1.126918\n      0.559757\n      -0.206006\n    \n    \n      PropCat 14\n      2.473743\n      -0.294432\n      -0.375323\n    \n  \n\n  \n    \n      \n      0\n      1\n      2\n    \n  \n  \n    \n      SEC\n      3.416667\n      2.641129\n      2.139286\n    \n    \n      FEH\n      2.041667\n      2.008065\n      2.085714\n    \n    \n      MT\n      7.652778\n      7.979839\n      8.489286\n    \n    \n      SEX\n      1.555556\n      1.693548\n      1.825000\n    \n    \n      AGE\n      3.000000\n      3.229839\n      3.253571\n    \n    \n      EDU\n      2.347222\n      3.802419\n      4.692857\n    \n    \n      HS\n      4.125000\n      4.193548\n      4.207143\n    \n    \n      CHILD\n      3.486111\n      3.262097\n      3.142857\n    \n    \n      CS\n      0.888889\n      0.899194\n      0.971429\n    \n    \n      Affluence Index\n      8.694444\n      14.895161\n      21.042857\n    \n  \n\n\n\nComment: - Cluster 0: (n=72) Highly loyal, low value, highly responsive to price category 3 and selling proposition 14. - Cluster 1: (n=252) Responsive to price category 2 and selling proposition 5, otherwise somewhat middling. - Cluster 2: (n=278) Low brand loyalty, responsive to price category 1\n\ncombinedIndicator = behaviorIndicator + purchaseBasisIndicator \nclusters = KMeans(n_clusters=4, random_state=1).fit(bathSoap_df_norm[combinedIndicator])\nprint(clusterSizes(clusters))\ndisplay_side_by_side(clusterCenters(clusters, combinedIndicator).transpose(),\n                     clusterDemographics(clusters).transpose())\n\n0    109\n1    201\n2     70\n3    220\ndtype: int64\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n    \n  \n  \n    \n      No. of Brands\n      -0.414643\n      0.837833\n      -0.583893\n      -0.374254\n    \n    \n      Brand Runs\n      -0.318502\n      0.973787\n      -0.800569\n      -0.477157\n    \n    \n      Total Volume\n      -0.651976\n      0.290887\n      0.083386\n      0.030727\n    \n    \n      No. of  Trans\n      -0.426788\n      0.824516\n      -0.433421\n      -0.403947\n    \n    \n      Value\n      -0.218712\n      0.388624\n      -0.555616\n      -0.069913\n    \n    \n      Trans / Brand Runs\n      -0.140179\n      -0.299278\n      1.037226\n      0.012857\n    \n    \n      Vol/Tran\n      -0.458972\n      -0.360995\n      0.513840\n      0.393724\n    \n    \n      Avg. Price\n      1.358148\n      0.063462\n      -1.318253\n      -0.311438\n    \n    \n      Others 999\n      0.514018\n      0.368089\n      -1.264136\n      -0.188747\n    \n    \n      maxBrandIndicator\n      -0.418273\n      -0.552509\n      1.421065\n      0.259871\n    \n    \n      Pur Vol No Promo - %\n      0.187409\n      -0.477946\n      0.210216\n      0.276930\n    \n    \n      Pur Vol Promo 6 %\n      -0.175746\n      0.584703\n      -0.436005\n      -0.308403\n    \n    \n      Pur Vol Other Promo %\n      -0.084062\n      0.038058\n      0.214271\n      -0.061299\n    \n    \n      Pr Cat 1\n      1.641899\n      0.045993\n      -0.797226\n      -0.601845\n    \n    \n      Pr Cat 2\n      -0.818168\n      0.140268\n      -1.220714\n      0.665620\n    \n    \n      Pr Cat 3\n      -0.479027\n      -0.267756\n      2.495499\n      -0.312055\n    \n    \n      Pr Cat 4\n      -0.406199\n      0.079000\n      -0.336964\n      0.236292\n    \n    \n      PropCat 5\n      -0.358174\n      -0.078526\n      -1.142977\n      0.612878\n    \n    \n      PropCat 14\n      -0.472507\n      -0.270906\n      2.498321\n      -0.313305\n    \n  \n\n  \n    \n      \n      0\n      1\n      2\n      3\n    \n  \n  \n    \n      SEC\n      1.779817\n      2.368159\n      3.400000\n      2.690909\n    \n    \n      FEH\n      1.669725\n      2.233831\n      2.042857\n      2.068182\n    \n    \n      MT\n      6.733945\n      9.024876\n      7.671429\n      8.281818\n    \n    \n      SEX\n      1.477064\n      1.920398\n      1.542857\n      1.763636\n    \n    \n      AGE\n      3.165138\n      3.323383\n      3.014286\n      3.200000\n    \n    \n      EDU\n      4.165138\n      4.626866\n      2.357143\n      3.986364\n    \n    \n      HS\n      3.045872\n      4.681592\n      3.914286\n      4.400000\n    \n    \n      CHILD\n      3.550459\n      3.044776\n      3.514286\n      3.159091\n    \n    \n      CS\n      0.743119\n      1.044776\n      0.871429\n      0.940909\n    \n    \n      Affluence Index\n      18.018349\n      20.930348\n      8.542857\n      15.650000\n    \n  \n\n\n\nComment: - Cluster 0 (n=109) is characterized by low volume, low loyalty, and sensitivity to promotions and price (responsive to cat. 1, unresponsive to 2 and 3), and unmoved by selling proposition. Demographically, it is affluent, of high socio-economic status, and has relatively small family size. - Cluster 1 (n=201) is distinguished mostly by the purchase behavior variables - it has low brand loyalty together with high value, volume and frequency. The brand switching seems to be intrinsic - this group is not particularly responsive to promotions, pricing or selling propositions. Demographically it is relatively affluent and educated. - Cluster 2 (n=70) stands out in both groups of variables - it has high loyalty, low value and price per purchase, and very differential response to price (unresponsive to categories 1, 2 and 4, highly responsive to category 3), and selling proposition (unresponsive to #5, highly responsive to #14). Demographically it has low affluence and education. - Cluster 3 (n=220) is a “gray” cluster, it is not characterized by very extreme/distinctive values across all variables, but is responsive to price category 2 and selling proposition 5 (similar to cluster 1 in the 3-cluster analysis). Demographically it is relatively affluent and educated."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.6 Segmenting Consumers of Bath Soap.html#whats-next",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-21-CaseStudies/Case 21.6 Segmenting Consumers of Bath Soap.html#whats-next",
    "title": "Case 21.6 Segmenting Consumers of Bath Soap",
    "section": "What’s next?",
    "text": "What’s next?\nMany data mining algorithms are iterative in an mathematical sense - iteration is used to find a good, if not best, solution. The modeling process itself is also iterative. In initial exploration, we do not seek the perfect model, merely something to get started. Results are assessed, and we typically continue with a modified approach.\nSeveral steps can be explored next to improve predictive performance:\n\nSome of the demographic categorical variables may not have much value being treated as is, as ordered categorical variables. They could be reviewed and turned into binary dummies.\nInstead of using a two-cluster model, a multi-cluster model could be used in hopes of deriving more distinguishable clusters. The non-success clusters could then be consolidated. For example, cluster #2 in the 4-cluster model is similar to our cluster 1 (“success”) in the 2-cluster model, only more sharply defined.\nDemographic predictors could be added to the original clustering process.\nThe clustering process, which includes a randomization component that yields variability in resulting clusters, can be repeated, to ensure that the cluster labels reflect some degree of stability. Repetition should show some clustering results that are consistent across various runs. Choosing for your labels a clustering result that is very inconsistent with the others could mean that you are labeling your market segments according to a chance fluke.\nIn the real world, going beyond the parameters of this case study, CRISA would probably work with the client to add the client’s own purchase data to the model to improve it over time."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-08-ProbSolutions-NB.html",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-08-ProbSolutions-NB.html",
    "title": "Chapter 8: The Naive Bayes Classifier (NB)",
    "section": "",
    "text": "2019-2020 Galit Shmueli, Peter C. Bruce, Peter Gedeck\n\nData Mining for Business Analytics: Concepts, Techniques, and Applications in Python (First Edition) Galit Shmueli, Peter C. Bruce, Peter Gedeck, and Nitin R. Patel. 2019.\nDate: 2020-03-08\nPython Version: 3.8.2 Jupyter Notebook Version: 5.6.1\nPackages: - dmba: 0.0.12 - numpy: 1.18.1 - pandas: 1.0.1 - scikit-learn: 0.22.2\nThe assistance from Mr. Kuber Deokar and Ms. Anuja Kulkarni in preparing these solutions is gratefully acknowledged.\n# Import required packages for this chapter\nfrom pathlib import Path\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom dmba import classificationSummary\n\n%matplotlib inline\n# Working directory:\n#\n# We assume that data are kept in the same directory as the notebook. If you keep your \n# data in a different folder, replace the argument of the `Path`\nDATA = Path('.')\n# and then load data using \n#\n# pd.read_csv(DATA / ‘filename.csv’)"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-08-ProbSolutions-NB.html#data-preparation",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-08-ProbSolutions-NB.html#data-preparation",
    "title": "Chapter 8: The Naive Bayes Classifier (NB)",
    "section": "Data Preparation",
    "text": "Data Preparation\nRemove all unnecessary columns from the dataset and convert Online and CreditCard to categories. Split the data into training (60%), and validation (40%) sets (use random_state=1).\n\n# Load the data\nbank_df = pd.read_csv(DATA / 'UniversalBank.csv')\n\n# Consider only the required variables and reorder the columns at the same time\nbank_df = bank_df[['Online', 'CreditCard', 'Personal Loan']]\nbank_df.Online = bank_df.Online.astype('category')\nbank_df.CreditCard = bank_df.CreditCard.astype('category')\nbank_df.head()\n\n\n\n\n\n\n\n\nOnline\nCreditCard\nPersonal Loan\n\n\n\n\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n\n\n2\n0\n0\n0\n\n\n3\n0\n0\n0\n\n\n4\n0\n1\n0\n\n\n\n\n\n\n\nSplit dataset into training and validation sets.\n\ntrain_df, valid_df = train_test_split(bank_df, test_size=0.4, random_state=1)\nprint('Training Set:', train_df.shape, 'Validation Set:', valid_df.shape)\n\nTraining Set: (3000, 3) Validation Set: (2000, 3)\n\n\n8.1.a Create a pivot table for the training data with Online as a column variable, CC as a row variable, and Loan as a secondary row variable. The values inside the table should convey the count. Use the pandas dataframe methods melt() and pivot().\nAnswer:\n\n# pivot table for training data\ntrain_df.pivot_table(index=['CreditCard', 'Personal Loan'],\n                    columns=['Online'], aggfunc=len)\n\n\n\n\n\n\n\n\nOnline\n0\n1\n\n\nCreditCard\nPersonal Loan\n\n\n\n\n\n\n0\n0\n792\n1117\n\n\n1\n73\n126\n\n\n1\n0\n327\n477\n\n\n1\n39\n49\n\n\n\n\n\n\n\n8.1.b. Consider the task of classifying a customer who owns a bank credit card and is actively using online banking services. Looking at the pivot table, what is the probability that this customer will accept the loan offer? (This is the probability of loan acceptance (Loan = 1) conditional on having a bank credit card (CC = 1) and being an active user of online banking services (Online = 1)).\nAnswer:\nUse the pivot table created in 8.1.b. for the answer\nThere are 477 + 49 = 526 records where online = 1 and cc = 1. 46 of them accept the loan, so the conditional probability is 49/526 = 0.0932\n\np11 = 49 / (477 + 49)\nprint('Count based probability P(Loan = 1|CC = 1, Online = 1) = ', p11)\n\nCount based probability P(Loan = 1|CC = 1, Online = 1) =  0.09315589353612168\n\n\n8.1.c. Create two separate pivot tables for the training data. One will have Loan (rows) as a function of Online (columns) and the other will have Loan (rows) as a function of CC.\nAnswer\nPivot table for Loan (rows) as a function of Online (columns). Here we can use the pivot_table method of the pandas data frame.\n\npredictors = ['CreditCard', 'Online']\n\nprint(train_df['Personal Loan'].value_counts() / len(train_df))\nprint()\n\nfor predictor in predictors:\n    # construct the frequency table\n    df = train_df[['Personal Loan', predictor]]\n    freqTable = df.pivot_table(index='Personal Loan', columns=predictor, aggfunc=len)\n\n    # divide each row by the sum of the row to get conditional probabilities\n    propTable = freqTable.apply(lambda x: x / sum(x), axis=1)\n    print(propTable)\n    print()\n\n0    0.904333\n1    0.095667\nName: Personal Loan, dtype: float64\n\nCreditCard            0         1\nPersonal Loan                    \n0              0.703649  0.296351\n1              0.693380  0.306620\n\nOnline                0         1\nPersonal Loan                    \n0              0.412459  0.587541\n1              0.390244  0.609756\n\n\n\nCreditCard abbreviated as CC, Personal Loan abbreviated as Loan)\n8.1.d. Compute the following quantities, P(A | B) means “the probability of A given B”]:\n\n\n\nP(CC = 1 | Loan = 1) (the proportion of credit card holders among the loan acceptors)\n\n\nP(Online = 1|Loan = 1)\n\n\n\nP(Loan = 1) = the proportion of loan acceptors\n\n\n\nP(CC = 1|Loan = 0)\n\n\n\nP(Online = 1|Loan = 0)\n\n\n\nP(Loan = 0)\n\n\n\n\n\n\n\n\nUse the pivot tables created in 8.1.c.\n\n\n\nP(CreditCard = 1|Loan = 1) = 0.306620\n\n\n\nP(Online = 1|Loan = 1) = 0.609756\n\n\n\nP(Loan = 1) = 0.095667\n\n\n\nP(CC = 1|Loan = 0) = 0.296351\n\n\n\nP(Online = 1|Loan = 0) = 0.587541\n\n\n\nP(Loan = 0) = 0.904333\n\n\n\n\n\n\n\n\n8.1.e. Use the quantities computed above to compute the naive Bayes probability P(Loan = 1 j CC = 1, Online = 1).\nRefer to the naive Bayes formula (8.3) in the book.\nP(Loan=1|CC=1,Online=1) = \n   P(Loan=1) * P(CC=1|Loan=1) * P(Online=1|Loan=1) / \n   [P(Loan=1) * [P(CC=1|Loan=1) * P(Online=1|Loan=1)] + \n    P(Loan=0) * [P(CC=1|Loan=0) * P(Online=1|Loan=0)]]\n\n# P(Loan = 1) * P(CC = 1 / Loan = 1) * P(Online = 1 / Loan = 1)\np1 = 0.095667 * 0.306620 * 0.609756\n# P(Loan = 0) * P(CC = 1 / Loan = 0) * P(Online = 1 / Loan = 0)\np2 = 0.904333 * 0.296351 * 0.587541\n\nprint('Naive Bayes probability P(Loan = 1|CC = 1, Online = 1) = ', p1 / (p1 + p2))\n\nNaive Bayes probability P(Loan = 1|CC = 1, Online = 1) =  0.1020046248320646\n\n\n8.1.f. Compare this value with the one obtained from the pivot table in (b). Which is a more accurate estimate?\nThe value obtained from the crossed pivot table is the more accurate estimate, since it does not make the simplifying assumption that the probabilities (of taking a loan if you are a credit card holder and if you are an online customer) are independent. It is feasible in this case because there are few variables and few categories to consider, and thus there are ample data for all possible combinations.\n8.1.g. Which of the entries in this table are needed for computing P(Loan = 1 | CC = 1, Online = 1)? In Python, run naive Bayes on the data. Examine the model output on training data, and find the entry that corresponds to P(Loan = 1 | CC = 1, Online = 1). Compare this to the number you obtained in (e).\nIn Python, run naive Bayes on the training data. Use data points that match the condition CreditCard=1,Online=1 to find the predicted probability for P(Loan=1|CC=1,Online=1).\nChange the types of variables to categories and use one-hot-encoding for the independent variables.\n\ntrain_df = pd.get_dummies(train_df, prefix_sep='_')\ntrain_df['Personal Loan'] = train_df['Personal Loan'].astype('category')\ntrain_df.head()\n\n\n\n\n\n\n\n\nPersonal Loan\nOnline_0\nOnline_1\nCreditCard_0\nCreditCard_1\n\n\n\n\n4522\n0\n1\n0\n1\n0\n\n\n2851\n0\n0\n1\n1\n0\n\n\n2313\n0\n0\n1\n0\n1\n\n\n982\n0\n1\n0\n0\n1\n\n\n1164\n1\n0\n1\n1\n0\n\n\n\n\n\n\n\n\npredictors = ['Online_0', 'Online_1', 'CreditCard_0', 'CreditCard_1']\nnb = MultinomialNB(alpha=0.01)\nnb.fit(train_df[predictors], train_df['Personal Loan'])\n\nMultinomialNB(alpha=0.01, class_prior=None, fit_prior=True)\n\n\nPredict probabilities and check for the probability of “1” in the row where Online = 1 and CreditCard = 1\n\npredProb = nb.predict_proba(train_df.drop(columns=['Personal Loan']))\npredicted = pd.concat([train_df, pd.DataFrame(predProb, index=train_df.index)], axis=1)\n\nmatches = (predicted.Online_1 == 1) & (predicted.CreditCard_1 == 1)\npredicted[matches].head()\n\n\n\n\n\n\n\n\nPersonal Loan\nOnline_0\nOnline_1\nCreditCard_0\nCreditCard_1\n0\n1\n\n\n\n\n2313\n0\n0\n1\n0\n1\n0.897993\n0.102007\n\n\n1918\n1\n0\n1\n0\n1\n0.897993\n0.102007\n\n\n4506\n0\n0\n1\n0\n1\n0.897993\n0.102007\n\n\n586\n0\n0\n1\n0\n1\n0.897993\n0.102007\n\n\n3591\n0\n0\n1\n0\n1\n0.897993\n0.102007\n\n\n\n\n\n\n\nThis gives P(Loan=1|Online=1,CC=1) = 0.1020"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-20-ProbSolutions-TM.html",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-20-ProbSolutions-TM.html",
    "title": "Chapter 20: Text Mining",
    "section": "",
    "text": "2019-2020 Galit Shmueli, Peter C. Bruce, Peter Gedeck\n\nData Mining for Business Analytics: Concepts, Techniques, and Applications in Python (First Edition) Galit Shmueli, Peter C. Bruce, Peter Gedeck, and Nitin R. Patel. 2019.\nDate: 2020-03-08\nPython Version: 3.8.2 Jupyter Notebook Version: 5.6.1\nPackages: - dmba: 0.0.12 - matplotlib: 3.2.0 - nltk: 3.4.4 - numpy: 1.18.1 - pandas: 1.0.1 - scipy: 1.4.1 - scikit-learn: 0.22.2\nThe assistance from Mr. Kuber Deokar and Ms. Anuja Kulkarni in preparing these solutions is gratefully acknowledged.\n# import required packages for this chapter\nfrom pathlib import Path\nfrom zipfile import ZipFile\nimport math\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.metrics import pairwise\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn import linear_model\nfrom scipy.cluster.hierarchy import linkage, dendrogram, fcluster\nfrom sklearn.cluster import KMeans\nimport nltk\nfrom nltk import word_tokenize\nfrom nltk.stem.snowball import EnglishStemmer \n\nimport matplotlib.pylab as plt\n\nfrom dmba import classificationSummary\n\n%matplotlib inline\n# warning expected\n\n/Users/gedeck/opt/anaconda3/envs/dmba-notebooks/lib/python3.8/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.feature_extraction.stop_words module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.feature_extraction.text. Anything that cannot be imported from sklearn.feature_extraction.text is now part of the private API.\n  warnings.warn(message, FutureWarning)\n# Working directory:\n#\n# We assume that data are kept in the same directory as the notebook. If you keep your \n# data in a different folder, replace the argument of the `Path`\nDATA = Path('.')\n# and then load data using \n#\n# pd.read_csv(DATA / ‘filename.csv’)\n# warning expected\n# Download required files for nltk\nnltk.download('punkt')\n\n[nltk_data] Downloading package punkt to /Users/gedeck/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n\n\nTrue"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-20-ProbSolutions-TM.html#solution-20.1.a",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-20-ProbSolutions-TM.html#solution-20.1.a",
    "title": "Chapter 20: Text Mining",
    "section": "Solution 20.1.a",
    "text": "Solution 20.1.a\nIdentify 10 non-word tokens in the passage.\nPunctuation, such as: !, ;, comma. Also components of the html code like &lt;, &gt;, , img, alt, title, src, font. Also the url and the &quot."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-20-ProbSolutions-TM.html#solution-20.1.b",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-20-ProbSolutions-TM.html#solution-20.1.b",
    "title": "Chapter 20: Text Mining",
    "section": "Solution 20.1.b",
    "text": "Solution 20.1.b\nSuppose this passage constitutes a document to be classified, but you are not certain of the business goal of the classification task. Identify material (at least 20% of the terms) that, in your judgment, could be discarded fairly safely without knowing that goal.\nEverything in the top row and the bottom three rows could be discarded."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-20-ProbSolutions-TM.html#solution-20.1.c",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-20-ProbSolutions-TM.html#solution-20.1.c",
    "title": "Chapter 20: Text Mining",
    "section": "Solution 20.1.c",
    "text": "Solution 20.1.c\nSuppose the classification task is to predict whether this post requires the attention of the instructor, or whether a teaching assistant might suffice. Identify the 20% of the terms that you think might be most helpful in that task.\nThe phrases “do we need to finish project” and “where to find illustrations and demos”."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-20-ProbSolutions-TM.html#solution-20.1.d",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-20-ProbSolutions-TM.html#solution-20.1.d",
    "title": "Chapter 20: Text Mining",
    "section": "Solution 20.1.d",
    "text": "Solution 20.1.d\nWhat aspect of the passage is most problematic from the standpoint of simply using a bag-of-words approach, as opposed to an approach in which meaning is extracted?\nIt is probably the fact that, in this post, there are two voices - that of the inquirer and that of the responder. A bag of words jumbles this all together, and any meaning associated with there being two voices is lost."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-20-ProbSolutions-TM.html#solution-20.2.a",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-20-ProbSolutions-TM.html#solution-20.2.a",
    "title": "Chapter 20: Text Mining",
    "section": "Solution 20.2.a",
    "text": "Solution 20.2.a\nLoad the zipped file into Python and create a label vector.\n\n# Step 1: import and label records\ncorpus = []\nlabel = []\nwith ZipFile(DATA / 'AutoAndElectronics.zip') as rawData:\n    for info in rawData.infolist():\n        if info.is_dir(): \n            continue\n        label.append(1 if 'rec.autos' in info.filename else 0)\n        corpus.append(rawData.read(info))\n\nThere are two distinct classes, electronics (labeled as 0) and autos (labeled as 1). Reviewing some random files from these groups can give some idea of which category they pertain to."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-20-ProbSolutions-TM.html#solution-20.2.b",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-20-ProbSolutions-TM.html#solution-20.2.b",
    "title": "Chapter 20: Text Mining",
    "section": "Solution 20.2.b",
    "text": "Solution 20.2.b\nFollowing the example in this chapter, preprocess the documents.\nExplain what would be different if you did not perform the ‘stemming’ step.\n\n# Step 2: preprocessing (tokenization, stemming, and stopwords)\nclass LemmaTokenizer(object):\n    def __init__(self):\n        self.stemmer = EnglishStemmer()\n        self.stopWords = set(ENGLISH_STOP_WORDS)\n\n    def __call__(self, doc):\n        return [self.stemmer.stem(t) for t in word_tokenize(doc) \n                if t.isalpha() and t not in self.stopWords]\n\n\npreprocessor = CountVectorizer(tokenizer=LemmaTokenizer(), stop_words='english', encoding='latin1')\npreprocessedText = preprocessor.fit_transform(corpus)\n\n\n# Step 3: TF-IDF and latent semantic analysis\ntfidfTransformer = TfidfTransformer()\ntfidf = tfidfTransformer.fit_transform(preprocessedText)\n\nIf we don’t perform stemming there will be more columns in the resulting matrix. Stemming strips words down to their “stems” or “roots”, hence reducing the dimension of the resulting dataset."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-20-ProbSolutions-TM.html#solution-20.2.c",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-20-ProbSolutions-TM.html#solution-20.2.c",
    "title": "Chapter 20: Text Mining",
    "section": "Solution 20.2.c",
    "text": "Solution 20.2.c\nUse the LSA to create 10 concepts. Explain what is different about the concept matrix, as opposed to the TF-IDF matrix.\n\n# Extract 20 concepts using LSA ()\nsvd = TruncatedSVD(10)\nnormalizer = Normalizer(copy=False)\nlsa = make_pipeline(svd, normalizer)\n\nlsa_tfidf = lsa.fit_transform(tfidf)\n\nThe Term Frequency matrix gives the number (the count) of times the term appears in the document and enter this value into the corresponding row/column in the output matrix.\nTF-IDF compensates for the overall frequency of the term throughout the corpus terms that appear frequently throughout the corpus are given a low value; high values of TF-IDF occur when a relatively rare term occurs frequently in one document.\nThe concept matrix is smaller - grouping similar terms together."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-20-ProbSolutions-TM.html#solution-20.2.d",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-20-ProbSolutions-TM.html#solution-20.2.d",
    "title": "Chapter 20: Text Mining",
    "section": "Solution 20.2.d",
    "text": "Solution 20.2.d\nUsing this matrix, fit a predictive model (different from the model presented in the chapter illustration) to classify documents as autos or electronics. Compare its performance to that of the model presented in the chapter illustration.\nWe use a decision tree and a random forest classifier here.\n\n# split dataset into 60% training and 40% test set\nfrom sklearn.model_selection import train_test_split\nXtrain, Xtest, ytrain, ytest = train_test_split(lsa_tfidf, label, test_size=0.4, random_state=42)\n\n# run decision tree model on training\ndt = DecisionTreeClassifier(random_state=1)\ndt.fit(Xtrain, ytrain)\n\n# print confusion matrix and accuracty\nclassificationSummary(ytest, dt.predict(Xtest))\n\n# run random forest classifier model on training\nrf = RandomForestClassifier(n_estimators=500, random_state=1)\nrf.fit(Xtrain, ytrain)\n\n# print confusion matrix and accuracty\nclassificationSummary(ytest, rf.predict(Xtest))\n\nConfusion Matrix (Accuracy 0.9313)\n\n       Prediction\nActual   0   1\n     0 375  22\n     1  33 370\nConfusion Matrix (Accuracy 0.9613)\n\n       Prediction\nActual   0   1\n     0 388   9\n     1  22 381\n\n\nThe accuracies of the different models are:\n\nLogistic regression: 0.9563\nDecision tree: 0.9413\nRandom forest: 0.9600\n\nThe random forest classifier performs better than the logistic regression model. The decision tree classifier performs worse."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-20-ProbSolutions-TM.html#solution-20.3.a",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-20-ProbSolutions-TM.html#solution-20.3.a",
    "title": "Chapter 20: Text Mining",
    "section": "Solution 20.3.a",
    "text": "Solution 20.3.a\nExamine the term-document matrix.\n\nSolution 20.3.a.i\nIs it sparse or dense?\n\nshape = counts.shape\nprint('Term-document matrix: {0[1]} terms, {0[0]} documents'.format(shape))\nprint('  number of non-zero terms: {}'.format(counts.count_nonzero()))\nprint('  sparsity: {:.0f}%\\n'.format(100 * counts.count_nonzero() / (shape[0] * shape[1])))\n\nTerm-document matrix: 58047 terms, 4143 documents\n  number of non-zero terms: 640792\n  sparsity: 0%\n\n\n\nThe matrix is sparse\n\n\nSolution 20.3.a.ii\nFind two non-zero entries and briefly interpret their meaning, in words (you do not need to derive their calculation)\n\n# we know that 'add' occurs frequently in one of the document, but you could pick any term that has 1 or more counts \nfromTerm = 5340\ntoTerm = 5350\n# print(counts[11,1])\nindex = count_vect.get_feature_names()[fromTerm:toTerm]\npd.DataFrame(data=counts[0:20,fromTerm:toTerm].toarray().transpose(), index=index)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n\n\n\n\nadconfirm\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nadconfirmemail\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nadconfirmemailaddress\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nadconfirmemailaddressreceive\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nadd\n0\n0\n0\n0\n0\n0\n0\n3\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\naddadhdadrenalfatigueagelongevityallergyfood\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\naddax\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\naddchoice\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\naddcommentadminpage\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\naddcompare\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n\n\n\n\nprint('Document 7 - term 5344: ', counts[7, 5344])\nprint('Document 8 - term 5344: ', counts[8, 5344])\nprint('Average occurrence of term 5344 in all documents:', np.sum(counts[:, 5344]) / counts.shape[0])\npd.Series([c.toarray()[0, 0] for c in counts[:, 5344]]).hist(bins=60)\n\nDocument 7 - term 5344:  3\nDocument 8 - term 5344:  0\nAverage occurrence of term 5344 in all documents: 0.5614289162442674\n\n\n\n\n\n\n\n\n\nThe term-document matrix has a 3 in the cell at row 7 and column 5344. This means the term add occurs 3 times in document 7. This is relatively frequent as we can see from the histogram that it rarely occurs in the other documents.\nAt row 8 and column 5344, the value is 0. This meanst the term did not occur in document 8."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-20-ProbSolutions-TM.html#solution-20.3.b",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-20-ProbSolutions-TM.html#solution-20.3.b",
    "title": "Chapter 20: Text Mining",
    "section": "Solution 20.3.b",
    "text": "Solution 20.3.b\nBriefly explain the difference between the term-document matrix and the concept-document matrix. Relate the latter to what you learned in the principal components chapter (Chapter 4 Dimension Reduction).\nThe Term-Document Matrix is a matrix that indicates whether, or how often, a term (column) appears in a document (row). In the Concept - Document Matrix, we must first join together similar terms into a limited number of concepts, which are the columns. The documents will be listed down the left side of the matrix. Principle Component Analysis produces linear combinations of correlated predictor variables, and produces new predictor variables, of which a limited set can effectively replace the numerous original variables. The concept-document matrix works on the same logic using latent semantic indexing which results in combining related terms to produce concepts. This reduces the dimension of the data."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-20-ProbSolutions-TM.html#solution-20.3.c",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-20-ProbSolutions-TM.html#solution-20.3.c",
    "title": "Chapter 20: Text Mining",
    "section": "Solution 20.3.c",
    "text": "Solution 20.3.c\nUsing logistic regression, partition the data (60% training, 40% validation), and develop a model to classify the documents as relevant' ornon-relevant.’ Comment on its efficacy.\n\n# split dataset into 60% training and 40% validation set\ntrain_X, valid_X, train_y, valid_y = train_test_split(lsa_tfidf, farm_ads.relevance, test_size=0.4, random_state=42)\n\n# run logistic regression model on training\nlogit_reg = linear_model.LogisticRegression(solver='lbfgs')\nlogit_reg.fit(train_X, train_y)\n\n# print confusion matrix and accuracty\nclassificationSummary(valid_y, logit_reg.predict(valid_X), class_names=logit_reg.classes_)\n\nConfusion Matrix (Accuracy 0.7823)\n\n       Prediction\nActual  -1   1\n    -1 554 239\n     1 122 743\n\n\nThe Logistic Regression model classifies the documents (as 1’s or 0’s) with an error rate of 22%, according to the validation data confusion matrix. The goal of the model is not to select out a subset of ads most likely to be relevant, but rather to classify all ads."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-20-ProbSolutions-TM.html#solution-20.3.d",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-20-ProbSolutions-TM.html#solution-20.3.d",
    "title": "Chapter 20: Text Mining",
    "section": "Solution 20.3.d",
    "text": "Solution 20.3.d\nWhy use the concept-document matrix, and not the term-document matrix, to provide the predictor variables?\nWe use the concept-document matrix, rather than the term document matrix, to provide predictor variables, because the former reduces the dimension of the data (i.e. reduces the number of predictor variables) without sacrificing much predictive power, allowing for easier and more effective modeling."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-20-ProbSolutions-TM.html#solution-20.4.a",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-20-ProbSolutions-TM.html#solution-20.4.a",
    "title": "Chapter 20: Text Mining",
    "section": "Solution 20.4.a",
    "text": "Solution 20.4.a\nFollowing the example in this chapter, preprocess the documents, except do not create a label vector.\n\n# Step 1: import and label records\ncorpus = []\nwith ZipFile(DATA / 'AutoAndElectronics.zip') as rawData:\n    for info in rawData.infolist():\n        if info.is_dir(): \n            continue\n        corpus.append(rawData.read(info))\n\n# Step 2: preprocessing (tokenization, stemming, and stopwords)\nclass LemmaTokenizer(object):\n    def __init__(self):\n        self.stemmer = EnglishStemmer()\n        self.stopWords = set(ENGLISH_STOP_WORDS)\n\n    def __call__(self, doc):\n        return [self.stemmer.stem(t) for t in word_tokenize(doc) \n                if t.isalpha() and t not in self.stopWords]\n\npreprocessor = CountVectorizer(tokenizer=LemmaTokenizer(), stop_words='english', encoding='latin1')\npreprocessedText = preprocessor.fit_transform(corpus)\n\n# Step 3: TF-IDF and latent semantic analysis\ntfidfTransformer = TfidfTransformer()\ntfidf = tfidfTransformer.fit_transform(preprocessedText)"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-20-ProbSolutions-TM.html#solution-20.4.b",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-20-ProbSolutions-TM.html#solution-20.4.b",
    "title": "Chapter 20: Text Mining",
    "section": "Solution 20.4.b",
    "text": "Solution 20.4.b\nUse the LSA to create 10 concepts.\n\n# Extract 20 concepts using LSA ()\nsvd = TruncatedSVD(10)\nnormalizer = Normalizer(copy=False)\nlsa = make_pipeline(svd, normalizer)\n\nlsa_tfidf = lsa.fit_transform(tfidf)"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-20-ProbSolutions-TM.html#solution-20.4.c",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-20-ProbSolutions-TM.html#solution-20.4.c",
    "title": "Chapter 20: Text Mining",
    "section": "Solution 20.4.c",
    "text": "Solution 20.4.c\nBefore doing the clustering, state how many natural clusters you expect to find.\nWe can expect to find two natural clusters"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-20-ProbSolutions-TM.html#solution-20.4.d",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-20-ProbSolutions-TM.html#solution-20.4.d",
    "title": "Chapter 20: Text Mining",
    "section": "Solution 20.4.d",
    "text": "Solution 20.4.d\nPerform hierarchical clustering and inspect the dendrogram.\n\nZ = linkage(lsa_tfidf, method='average')\n\n\nfig = plt.figure(figsize=(10, 6))\nfig.subplots_adjust(bottom=0.23)\nplt.title('Hierarchical Clustering Dendrogram (Complete linkage)')\nplt.xlabel('Documents')\ndendrogram(Z, color_threshold=0.9)\nplt.axhline(y=8, color='black', linewidth=0.5, linestyle='dashed')\nplt.show()\n\n\n\n\n\n\n\n\n\nSolution 20.4.d.i\nFrom the dendrogram, how many natural clusters appear?\nIt appears there are two major clusters. The remaining documents form a number of smaller clusters.\n\n\nSolution 20.4.d.ii\nExamining the dendrogram as it branches beyond the number of main clusters, select a sub-cluster and assess its characteristics.\n\nnclusters = 20\nmembership = fcluster(Z, nclusters, criterion='maxclust')\nfor clNumber in range(1, nclusters + 1):\n    nmembers = sum(membership == clNumber)\n    recAutos = ['Newsgroups: rec.autos' in str(doc) for doc, cl in zip(corpus, membership) if cl == clNumber]\n    ratioAutos = sum(recAutos) /nmembers\n    print(f'{sum(recAutos):3d} of {nmembers:3d} : {ratioAutos:.2f} {\"rec.autos\" if ratioAutos &gt; 0.9 else \"\"}')\n\n  2 of  33 : 0.06 \n  7 of   7 : 1.00 rec.autos\n 22 of  56 : 0.39 \n 48 of  49 : 0.98 rec.autos\n 38 of  38 : 1.00 rec.autos\n  8 of  69 : 0.12 \n  0 of  41 : 0.00 \n  3 of  41 : 0.07 \n 17 of  17 : 1.00 rec.autos\n 83 of  84 : 0.99 rec.autos\n  0 of   8 : 0.00 \n 21 of  21 : 1.00 rec.autos\n 16 of  18 : 0.89 \n448 of 487 : 0.92 rec.autos\n171 of 171 : 1.00 rec.autos\n 81 of 351 : 0.23 \n  0 of   7 : 0.00 \n  3 of  45 : 0.07 \n  1 of  25 : 0.04 \n  9 of 432 : 0.02 \n\n\nFrom this analysis, we can see that the clustering groups documents from the same newsgroup most of the times well together."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-20-ProbSolutions-TM.html#solution-20.4.e",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-20-ProbSolutions-TM.html#solution-20.4.e",
    "title": "Chapter 20: Text Mining",
    "section": "Solution 20.4.e",
    "text": "Solution 20.4.e\nPerform \\(k\\)-means clustering for two clusters and report how distant and separated they are (using between-cluster distance and within cluster dispersion).\n\nkmeans = KMeans(n_clusters=2, random_state=0).fit(lsa_tfidf)\n\ncentroids = pd.DataFrame(kmeans.cluster_centers_)\nprint('Between-cluster distance: ', math.sqrt(sum(centroids.iloc[0, :] - centroids.iloc[1, :])**2))\n\nBetween-cluster distance:  0.8845828812990629\n\n\n\nwithinClusterSS = [0] * 2\nclusterCount = [0] * 2\nfor cluster, distance in zip(kmeans.labels_, kmeans.transform(lsa_tfidf)):\n    withinClusterSS[cluster] += distance[cluster]**2\n    clusterCount[cluster] += 1\nfor cluster, withClustSS in enumerate(withinClusterSS):\n    count = clusterCount[cluster]\n    withinClusterDispersion = math.sqrt(withClustSS / (count - 1))\n    print(f'Cluster {cluster} ({count} members): {withinClusterDispersion:5.2f} within cluster')\n\nCluster 0 (1087 members):  0.60 within cluster\nCluster 1 (913 members):  0.63 within cluster"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-15-ProbSolutions-CA.html",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-15-ProbSolutions-CA.html",
    "title": "Chapter 15: Cluster Analysis",
    "section": "",
    "text": "2019-2020 Galit Shmueli, Peter C. Bruce, Peter Gedeck\n\nData Mining for Business Analytics: Concepts, Techniques, and Applications in Python (First Edition) Galit Shmueli, Peter C. Bruce, Peter Gedeck, and Nitin R. Patel. 2019.\nDate: 2020-03-08\nPython Version: 3.8.2 Jupyter Notebook Version: 5.6.1\nPackages: - matplotlib: 3.2.0 - numpy: 1.18.1 - pandas: 1.0.1 - scipy: 1.4.1 - scikit-learn: 0.22.2\nThe assistance from Mr. Kuber Deokar and Ms. Anuja Kulkarni in preparing these solutions is gratefully acknowledged.\n# Import required packages for this chapter\nfrom pathlib import Path\n\nimport pandas as pd\nimport numpy as np\nfrom pandas.plotting import parallel_coordinates\nfrom sklearn import preprocessing\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import pairwise\nfrom scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n\nimport matplotlib.pylab as plt\n\n%matplotlib inline\n# Working directory:\n#\n# We assume that data are kept in the same directory as the notebook. If you keep your \n# data in a different folder, replace the argument of the `Path`\nDATA = Path('.')\n# and then load data using \n#\n# pd.read_csv(DATA / ‘filename.csv’)"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-15-ProbSolutions-CA.html#solution-15.1.a",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-15-ProbSolutions-CA.html#solution-15.1.a",
    "title": "Chapter 15: Cluster Analysis",
    "section": "Solution 15.1.a",
    "text": "Solution 15.1.a\nRemove all records with missing measurements from the dataset.\n\n# Load data\nfull_df = pd.read_csv(DATA / 'Universities.csv')\n\nprint('Before: ', full_df.shape)\nuniversities_df = full_df.dropna(axis=0)\nprint('After: ', universities_df.shape)\n\nuniversities_df.head()\n\nBefore:  (1302, 20)\nAfter:  (471, 20)\n\n\n\n\n\n\n\n\n\nCollege Name\nState\nPublic (1)/ Private (2)\n# appli. rec'd\n# appl. accepted\n# new stud. enrolled\n% new stud. from top 10%\n% new stud. from top 25%\n# FT undergrad\n# PT undergrad\nin-state tuition\nout-of-state tuition\nroom\nboard\nadd. fees\nestim. book costs\nestim. personal $\n% fac. w/PHD\nstud./fac. ratio\nGraduation rate\n\n\n\n\n0\nAlaska Pacific University\nAK\n2\n193.0\n146.0\n55.0\n16.0\n44.0\n249.0\n869.0\n7560.0\n7560.0\n1620.0\n2500.0\n130.0\n800.0\n1500.0\n76.0\n11.9\n15.0\n\n\n2\nUniversity of Alaska Southeast\nAK\n1\n146.0\n117.0\n89.0\n4.0\n24.0\n492.0\n1849.0\n1742.0\n5226.0\n2514.0\n2250.0\n34.0\n500.0\n1162.0\n39.0\n9.5\n39.0\n\n\n9\nBirmingham-Southern College\nAL\n2\n805.0\n588.0\n287.0\n67.0\n88.0\n1376.0\n207.0\n11660.0\n11660.0\n2050.0\n2430.0\n120.0\n400.0\n900.0\n74.0\n14.0\n72.0\n\n\n11\nHuntingdon College\nAL\n2\n608.0\n520.0\n127.0\n26.0\n47.0\n538.0\n126.0\n8080.0\n8080.0\n1380.0\n2540.0\n100.0\n500.0\n1100.0\n63.0\n11.4\n44.0\n\n\n21\nTalladega College\nAL\n2\n4414.0\n1500.0\n335.0\n30.0\n60.0\n908.0\n119.0\n5666.0\n5666.0\n1424.0\n1540.0\n418.0\n1000.0\n1400.0\n56.0\n15.5\n46.0"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-15-ProbSolutions-CA.html#solution-15.1.b",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-15-ProbSolutions-CA.html#solution-15.1.b",
    "title": "Chapter 15: Cluster Analysis",
    "section": "Solution 15.1.b",
    "text": "Solution 15.1.b\nFor all the continuous measurements, run hierarchical clustering using complete linkage and Euclidean distance. Make sure to normalize the measurements. From the dendrogram: How many clusters seem reasonable for describing these data?\n\n# Reduce to continuous measurements and normalize data\nreduced_df = universities_df.drop(columns=['College Name', 'State', 'Public (1)/ Private (2)'])\nuniversities_norm = (reduced_df - reduced_df.mean()) / reduced_df.std()\n\n# Hierarchical clustering using complete linkage and Euclidean distance \nZ = linkage(universities_norm, method='complete', metric='euclidean')\nfig = plt.figure(figsize=(10, 6))\ndendrogram(Z, no_labels=True, color_threshold=55)\nplt.show()\n\n\n\n\n\n\n\n\n\n# try splitting into different number of clustesr\nmemb = fcluster(Z, 2, criterion='maxclust')\nnp.unique(memb, return_counts=True)\n\n(array([1, 2], dtype=int32), array([458,  13]))\n\n\n\nnp.unique(fcluster(Z, 4, criterion='maxclust'), return_counts=True)\n\n(array([1, 2, 3, 4], dtype=int32), array([457,   1,  12,   1]))\n\n\nIf we select #clusters = 2, we have on big cluster and one small cluster. Looking at the same dendrogram, we see that 4 clusters is also a possibility, on similar grounds. The following analysis is for 2 clusters, but a similar analysis could be conducted for different number of clusters."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-15-ProbSolutions-CA.html#solution-15.1.c",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-15-ProbSolutions-CA.html#solution-15.1.c",
    "title": "Chapter 15: Cluster Analysis",
    "section": "Solution 15.1.c",
    "text": "Solution 15.1.c\nCompare the summary statistics for each cluster and describe each cluster in this context (e.g., “Universities with high tuition, low acceptance rate…”). (Hint: To obtain cluster statistics for hierarchical clustering, use the pandas method groupby(clusterlabel) together with methods such as mean or median.)\n\ncentroids = {}\nfor key, item in reduced_df.groupby(memb):\n    centroids['Cluster {}'.format(key)] = item.mean()\n\npd.DataFrame(centroids).round(2)\n\n\n\n\n\n\n\n\nCluster 1\nCluster 2\n\n\n\n\n# appli. rec'd\n2747.43\n17235.23\n\n\n# appl. accepted\n1790.03\n11678.38\n\n\n# new stud. enrolled\n683.45\n4207.00\n\n\n% new stud. from top 10%\n27.81\n35.15\n\n\n% new stud. from top 25%\n55.27\n69.08\n\n\n# FT undergrad\n3078.83\n20618.54\n\n\n# PT undergrad\n684.05\n4792.69\n\n\nin-state tuition\n9480.30\n6811.38\n\n\nout-of-state tuition\n10579.12\n10435.54\n\n\nroom\n2214.19\n2464.85\n\n\nboard\n2113.93\n2404.31\n\n\nadd. fees\n369.67\n708.38\n\n\nestim. book costs\n547.77\n584.62\n\n\nestim. personal $\n1294.43\n1928.77\n\n\n% fac. w/PHD\n72.86\n85.38\n\n\nstud./fac. ratio\n13.91\n15.98\n\n\nGraduation rate\n65.59\n64.69\n\n\n\n\n\n\n\nCluster 2 contains universities with high enrollment numbers, higher college entrance exam scores, and lower tuition fees. Cluster 1 contains universities with lower enrollment numbers and higher tuition fees. Other numbers are comparable."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-15-ProbSolutions-CA.html#solution-15.1.d",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-15-ProbSolutions-CA.html#solution-15.1.d",
    "title": "Chapter 15: Cluster Analysis",
    "section": "Solution 15.1.d",
    "text": "Solution 15.1.d\nUse the categorical measurements that were not used in the analysis (State and Private/Public) to characterize the different clusters. Is there any relationship between the clusters and the categorical information?\n\nfor key, item in universities_df.groupby(memb):\n    print('Cluster {}'.format(key))\n    print(item['Public (1)/ Private (2)'].value_counts().sort_index())\n\nCluster 1\n1    119\n2    339\nName: Public (1)/ Private (2), dtype: int64\nCluster 2\n1    9\n2    4\nName: Public (1)/ Private (2), dtype: int64\n\n\nCluster 2 is predominantly public, cluster 1 private.\n\nfor key, item in universities_df.groupby(memb):\n    print('Cluster {}'.format(key))\n    print(item.State.value_counts())\n\nCluster 1\nPA    41\nNY    37\nOH    24\nNC    23\nMA    19\nIA    18\nTX    18\nTN    15\nMO    15\nVA    15\nIN    15\nIL    14\nCA    14\nNJ    12\nMI    12\nMN    10\nCT    10\nWI     9\nSC     9\nFL     8\nGA     7\nNE     7\nVT     7\nKS     7\nOK     6\nNH     6\nME     6\nKY     6\nCO     6\nND     5\nLA     5\nMS     5\nOR     5\nAR     4\nDC     4\nAL     4\nRI     4\nSD     4\nMD     3\nNM     2\nID     2\nMT     2\nWA     2\nWV     2\nAK     2\nUT     2\nDE     2\nWY     1\nHI     1\nAZ     1\nName: State, dtype: int64\nCluster 2\nMA    3\nTX    2\nNY    1\nCA    1\nNJ    1\nPA    1\nIL    1\nMI    1\nAZ    1\nMN    1\nName: State, dtype: int64\n\n\nThere is no real geographic pattern."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-15-ProbSolutions-CA.html#solution-15.1.e",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-15-ProbSolutions-CA.html#solution-15.1.e",
    "title": "Chapter 15: Cluster Analysis",
    "section": "Solution 15.1.e",
    "text": "Solution 15.1.e\nWhat other external information can explain the contents of some or all of these clusters?\nState, not being a numeric variable, is not included in the clustering criteria. Note that southern states are under-represented in cluster 1 compared to cluster 2, and New England states are under-represented in cluster 2. Each group tends to have distinctive social and cultural norms. For example, sports receives relatively greater influence in the south than in New England. New England is known for its private college preparatory schools (particularly its elite ones) that feed students into selective (usually private and, hence, unsubsidized) universities. These factors could contribute to the higher cost and higher college entrance exam scores of cluster 1.\nRecall that all variables must be present for a school to be included in the clustering process. Note that most of the Ivy League and several other key top schools are missing - this could be because they choose not to report some of the data used in the US News rankings. Since these schools are missing in the “cluster-building” phase, it is possible that our set of clusters is incomplete. So, our set of clusters may be incomplete.\nThere may be other factors as well that could inform our clustering process."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-15-ProbSolutions-CA.html#solution-15.1.f",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-15-ProbSolutions-CA.html#solution-15.1.f",
    "title": "Chapter 15: Cluster Analysis",
    "section": "Solution 15.1.f",
    "text": "Solution 15.1.f\nConsider Tufts University, which is missing some information. Compute the Euclidean distance of this record from each of the clusters that you found above (using only the measurements that you have). Which cluster is it closest to? Impute the missing values for Tufts by taking the average of the cluster on those measurements.\n\n# Determine the cluster centroids\ncentroids = universities_norm.groupby(memb).mean()\ncentroids\n\n\n\n\n\n\n\n\n# appli. rec'd\n# appl. accepted\n# new stud. enrolled\n% new stud. from top 10%\n% new stud. from top 25%\n# FT undergrad\n# PT undergrad\nin-state tuition\nout-of-state tuition\nroom\nboard\nadd. fees\nestim. book costs\nestim. personal $\n% fac. w/PHD\nstud./fac. ratio\nGraduation rate\n\n\n\n\n1\n-0.098174\n-0.109007\n-0.106214\n-0.010969\n-0.018749\n-0.103681\n-0.073362\n0.013353\n0.000919\n-0.009701\n-0.014139\n-0.026271\n-0.006233\n-0.025678\n-0.020739\n-0.014720\n0.001361\n\n\n2\n3.458740\n3.840407\n3.741995\n0.386440\n0.660544\n3.652768\n2.584582\n-0.470427\n-0.032383\n0.341758\n0.498124\n0.925554\n0.219578\n0.904644\n0.730637\n0.518609\n-0.047960\n\n\n\n\n\n\n\n\n# Get information for 'Tufts University' and drop the non continuous variables\ntufts_university = full_df.loc[full_df['College Name'] == 'Tufts University', :]\ntufts_university = tufts_university.drop(columns=['College Name', 'State', 'Public (1)/ Private (2)'])\n\n# Apply the same normalization as for reduced_df\ntufts_norm = (tufts_university - reduced_df.mean())/reduced_df.std()\n\n# Combine with the centroids information from the clustering\ncombined = tufts_norm.append(centroids)\ncombined\n\n\n\n\n\n\n\n\n# appli. rec'd\n# appl. accepted\n# new stud. enrolled\n% new stud. from top 10%\n% new stud. from top 25%\n# FT undergrad\n# PT undergrad\nin-state tuition\nout-of-state tuition\nroom\nboard\nadd. fees\nestim. book costs\nestim. personal $\n% fac. w/PHD\nstud./fac. ratio\nGraduation rate\n\n\n\n\n475\n1.096623\n0.615893\n0.463390\n1.730988\n1.690004\n0.221677\nNaN\n1.866005\n2.116543\n1.145408\n1.425498\n0.348397\n0.313855\n-0.563089\n1.547610\n-0.939412\n1.456852\n\n\n1\n-0.098174\n-0.109007\n-0.106214\n-0.010969\n-0.018749\n-0.103681\n-0.073362\n0.013353\n0.000919\n-0.009701\n-0.014139\n-0.026271\n-0.006233\n-0.025678\n-0.020739\n-0.014720\n0.001361\n\n\n2\n3.458740\n3.840407\n3.741995\n0.386440\n0.660544\n3.652768\n2.584582\n-0.470427\n-0.032383\n0.341758\n0.498124\n0.925554\n0.219578\n0.904644\n0.730637\n0.518609\n-0.047960\n\n\n\n\n\n\n\n\n# Calculate distance matrix dropping the columns containing NaN\nd = pairwise.pairwise_distances(combined.dropna(axis=1), metric='euclidean')\npd.DataFrame(d, columns=combined.index, index=combined.index).head(5)\n\n\n\n\n\n\n\n\n475\n1\n2\n\n\n\n\n475\n0.000000\n5.061369\n7.778365\n\n\n1\n5.061369\n0.000000\n7.815593\n\n\n2\n7.778365\n7.815593\n0.000000\n\n\n\n\n\n\n\nThe distance to cluster 1 is 5.06 (normalized), and to cluster 2 it is 7.8. Tufts is closer to cluster 1. We can therefore imput the missing value for # PT undergrad in tufts_norm.\n\ntufts_norm['# PT undergrad'] = centroids.loc[1, '# PT undergrad']\ntufts_norm\n\n\n\n\n\n\n\n\n# appli. rec'd\n# appl. accepted\n# new stud. enrolled\n% new stud. from top 10%\n% new stud. from top 25%\n# FT undergrad\n# PT undergrad\nin-state tuition\nout-of-state tuition\nroom\nboard\nadd. fees\nestim. book costs\nestim. personal $\n% fac. w/PHD\nstud./fac. ratio\nGraduation rate\n\n\n\n\n475\n1.096623\n0.615893\n0.46339\n1.730988\n1.690004\n0.221677\n-0.073362\n1.866005\n2.116543\n1.145408\n1.425498\n0.348397\n0.313855\n-0.563089\n1.54761\n-0.939412\n1.456852"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-15-ProbSolutions-CA.html#solution-15.2.a",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-15-ProbSolutions-CA.html#solution-15.2.a",
    "title": "Chapter 15: Cluster Analysis",
    "section": "Solution 15.2.a",
    "text": "Solution 15.2.a\nUse only the numerical variables (1-9) to cluster the 21 firms. Justify the various choices made in conducting the cluster analysis, such as weights for different variables, the specific clustering algorithm(s) used, the number of clusters formed, and so on.\n\n# Load data, use the column Company as row names\npharma_df = pd.read_csv(DATA / 'Pharmaceuticals.csv')\npharma_df.set_index('Symbol', inplace=True)\npharma_df.head()\n\n\n\n\n\n\n\n\nName\nMarket_Cap\nBeta\nPE_Ratio\nROE\nROA\nAsset_Turnover\nLeverage\nRev_Growth\nNet_Profit_Margin\nMedian_Recommendation\nLocation\nExchange\n\n\nSymbol\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nABT\nAbbott Laboratories\n68.44\n0.32\n24.7\n26.4\n11.8\n0.7\n0.42\n7.54\n16.1\nModerate Buy\nUS\nNYSE\n\n\nAGN\nAllergan, Inc.\n7.58\n0.41\n82.5\n12.9\n5.5\n0.9\n0.60\n9.16\n5.5\nModerate Buy\nCANADA\nNYSE\n\n\nAHM\nAmersham plc\n6.30\n0.46\n20.7\n14.9\n7.8\n0.9\n0.27\n7.05\n11.2\nStrong Buy\nUK\nNYSE\n\n\nAZN\nAstraZeneca PLC\n67.63\n0.52\n21.5\n27.4\n15.4\n0.9\n0.00\n15.00\n18.0\nModerate Sell\nUK\nNYSE\n\n\nAVE\nAventis\n47.16\n0.32\n20.1\n21.8\n7.5\n0.6\n0.34\n26.81\n12.9\nModerate Buy\nFRANCE\nNYSE\n\n\n\n\n\n\n\n\npharma_data = pharma_df.drop(columns=['Name', 'Median_Recommendation', 'Location', 'Exchange'])\npharma_data.head()\n\n\n\n\n\n\n\n\nMarket_Cap\nBeta\nPE_Ratio\nROE\nROA\nAsset_Turnover\nLeverage\nRev_Growth\nNet_Profit_Margin\n\n\nSymbol\n\n\n\n\n\n\n\n\n\n\n\n\n\nABT\n68.44\n0.32\n24.7\n26.4\n11.8\n0.7\n0.42\n7.54\n16.1\n\n\nAGN\n7.58\n0.41\n82.5\n12.9\n5.5\n0.9\n0.60\n9.16\n5.5\n\n\nAHM\n6.30\n0.46\n20.7\n14.9\n7.8\n0.9\n0.27\n7.05\n11.2\n\n\nAZN\n67.63\n0.52\n21.5\n27.4\n15.4\n0.9\n0.00\n15.00\n18.0\n\n\nAVE\n47.16\n0.32\n20.1\n21.8\n7.5\n0.6\n0.34\n26.81\n12.9\n\n\n\n\n\n\n\nNormalize: normalization gives the same importance to all the variables and so is important for distance calculations.\n\npharma_norm = pharma_data.apply(preprocessing.scale, axis=0)\n\n\nfig, axes = plt.subplots(nrows=3, ncols=3, figsize=(10, 7), squeeze=False)\nfor i, method in enumerate(['single', 'complete', 'average', 'weighted', 'centroid', 'median', 'ward']):\n    ax = axes[i // 3, i % 3]\n    Z = linkage(pharma_norm, method=method)\n    dendrogram(Z, color_threshold=0, ax=ax, no_labels=True)\n    ax.set_xlabel(method)\nfig.delaxes(axes[2,1])  # remove empty subplots\nfig.delaxes(axes[2,2])\n\n\n\n\n\n\n\n\nWard clustering nicely separates the dataset into four clusters.\n\nZ = linkage(pharma_norm, method='ward')\nmemb = fcluster(Z, 4, criterion='maxclust')\n\n\n# k-means clustering\ninertia = []\nfor n_clusters in range(1, 7):\n    kmeans = KMeans(n_clusters=n_clusters, random_state=1).fit(pharma_norm)\n    inertia.append(kmeans.inertia_ / n_clusters)\ninertias = pd.DataFrame({'n_clusters': range(1, 7), 'inertia': inertia})\nax = inertias.plot(x='n_clusters', y='inertia')\nplt.xlabel('Number of clusters(k)')\nplt.ylabel('Average Within-Cluster Squared Distances')\nplt.ylim((0, 1.1 * inertias.inertia.max()))\nax.legend().set_visible(False)\nplt.show()\n\n\n\n\n\n\n\n\nThree to four clusters seem to be a good choice\n\nkmeans = KMeans(n_clusters=4, random_state=0).fit(pharma_norm)\n\nTwo factors can help us decide how many clusters to form:\n\nThe purpose of the inquiry. How many clusters are we going to want to deal with? In this case, to understand the structure of the industry in financial terms we might use basic domain knowledge to consider that size and financial performance are fundamental dimensions into which our various metrics fall. To make sense of our analysis, we will need at least two clusters, but more than 3 or 4 will probably defeat the purpose of clustering, which is to simplify matters (at least for the purposes of initial analysis).\nHow do the clusters look when trying 2, 3, 4, 5 clusters? Are they nicely separated? Do they have enough members to be useful? Do they look like natural clusters (as opposed to arbitrary decisions about cluster membership)? Normalizing the data is important to ensure that the “distance measured” accords equal weight to each variable. Without normalization, the variable with the largest scale will dominate the measure.\n\nTrying both k-means and hierarchical clustering, k-means of 3 and 4, and hierarchical clustering with Wards method give useful results. We will focus mainly on Wards clustering with k=4, but similar analysis could be made for the other choices."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-15-ProbSolutions-CA.html#solution-15.2.b",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-15-ProbSolutions-CA.html#solution-15.2.b",
    "title": "Chapter 15: Cluster Analysis",
    "section": "Solution 15.2.b",
    "text": "Solution 15.2.b\nInterpret the clusters with respect to the categorical variables used in forming the clusters.\n\ncentroids = {}\nfor key, item in pharma_data.groupby(memb):\n    centroids['Cluster {}'.format(key)] = item.mean()\n\npd.DataFrame(centroids).round(2)\n\n\n\n\n\n\n\n\nCluster 1\nCluster 2\nCluster 3\nCluster 4\n\n\n\n\nMarket_Cap\n157.02\n62.88\n26.91\n8.82\n\n\nBeta\n0.48\n0.41\n0.64\n0.62\n\n\nPE_Ratio\n22.22\n20.23\n55.63\n19.61\n\n\nROE\n44.42\n30.71\n10.10\n16.96\n\n\nROA\n17.70\n13.39\n4.20\n6.24\n\n\nAsset_Turnover\n0.95\n0.71\n0.70\n0.54\n\n\nLeverage\n0.22\n0.39\n0.32\n1.11\n\n\nRev_Growth\n18.53\n5.38\n7.00\n21.14\n\n\nNet_Profit_Margin\n19.58\n20.51\n5.13\n13.19\n\n\n\n\n\n\n\n\nCluster 1 companies are big, fast growing, profitable and not leveraged.\nCluster 2 companies are of moderate size (market cap), profitable and slow growing, not leveraged.\nCluster 3 companies are small, slow growing and of low profitability.\nCluster 4 companies are small, fast growing, profitable and highly leveraged"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-15-ProbSolutions-CA.html#solution-15.2.c",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-15-ProbSolutions-CA.html#solution-15.2.c",
    "title": "Chapter 15: Cluster Analysis",
    "section": "Solution 15.2.c",
    "text": "Solution 15.2.c\nIs there a pattern in the clusters with respect to the numerical variables (10-12)? (those not used in forming the clusters).\n\npd.DataFrame(pd.Series(item.Median_Recommendation.value_counts(), name=f'Cluster {key}') \n             for key, item in pharma_df.groupby(memb))\n\n\n\n\n\n\n\n\nHold\nModerate Buy\nModerate Sell\nStrong Buy\n\n\n\n\nCluster 1\n2.0\n2.0\nNaN\nNaN\n\n\nCluster 2\n4.0\n1.0\n2.0\nNaN\n\n\nCluster 3\n2.0\n1.0\nNaN\nNaN\n\n\nCluster 4\n1.0\n3.0\n2.0\n1.0\n\n\n\n\n\n\n\nWith respect to buy/hold recommendations, clusters 2 and 4 are the interesting ones, characterized by mixed recommendations for each. Cluster 4, for example, has recommendations ranging from “moderate sell” to “strong buy”.\n\npd.DataFrame(pd.Series(item.Location.value_counts(), name=f'Cluster {key}') \n             for key, item in pharma_df.groupby(memb))\n\n\n\n\n\n\n\n\nUS\nUK\nSWITZERLAND\nGERMANY\nCANADA\nIRELAND\nFRANCE\n\n\n\n\nCluster 1\n3.0\n1.0\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\nCluster 2\n5.0\n1.0\n1.0\nNaN\nNaN\nNaN\nNaN\n\n\nCluster 3\n1.0\nNaN\nNaN\n1.0\n1.0\nNaN\nNaN\n\n\nCluster 4\n4.0\n1.0\nNaN\nNaN\nNaN\n1.0\n1.0\n\n\n\n\n\n\n\nWith respect to country, the various countries are fairly well spread out across the 4 clusters, though cluster 2 has 5 US firms.\n\npd.DataFrame(pd.Series(item.Exchange.value_counts(), name=f'Cluster {key}') \n             for key, item in pharma_df.groupby(memb))\n\n\n\n\n\n\n\n\nNYSE\nNASDAQ\nAMEX\n\n\n\n\nCluster 1\n4.0\nNaN\nNaN\n\n\nCluster 2\n7.0\nNaN\nNaN\n\n\nCluster 3\n3.0\nNaN\nNaN\n\n\nCluster 4\n5.0\n1.0\n1.0\n\n\n\n\n\n\n\nWith respect to exchange, there are only two non-NYSE firms in the data set, and they are in cluster 4, one of the “small firms” cluster. (This is consistent with the nature of the AMEX and NASDAQ, which typically host smaller firms than the NYSE.)"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-15-ProbSolutions-CA.html#solution-15.2.d",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-15-ProbSolutions-CA.html#solution-15.2.d",
    "title": "Chapter 15: Cluster Analysis",
    "section": "Solution 15.2.d",
    "text": "Solution 15.2.d\nProvide an appropriate name for each cluster using any or all of the variables in the dataset.\nSee PA3 week 2 discussion task\n\nCluster 1: The big boys, financial stars\nCluster 2: International establishment with decent financial performance.\nCluster 3: Slow growth mid-size\nCluster 4: Fast growth small-cap"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-15-ProbSolutions-CA.html#solution-15.3.a",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-15-ProbSolutions-CA.html#solution-15.3.a",
    "title": "Chapter 15: Cluster Analysis",
    "section": "Solution 15.3.a",
    "text": "Solution 15.3.a\nApply hierarchical clustering to the data using Euclidean distance to the normalized measurements. Compare the dendrograms from single linkage and complete linkage, and look at cluster centroids. Comment on the structure of the clusters and on their stability. (Hint: To obtain cluster centroids for hierarchical clustering, compute the average values of each cluster members, using groupby() with the cluster centers followed by mean: dataframe.groupby(clusterlabel).mean().)\n\n# normalize data\ntemp = cereals_nonmissing.drop(columns=['name', 'mfr', 'type'])\ncereals_norm = (temp - temp.mean())/temp.std()\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 7), squeeze=False)\nfor i, (method, threshold) in enumerate([('single', 3.5), ('complete', 7)]):\n    ax = axes[0, i]\n    Z = linkage(cereals_norm, method=method, metric='euclidean')\n    dendrogram(Z, color_threshold=threshold, ax=ax, no_labels=True)\n    ax.set_xlabel(method + ' linkage')\n\n\n\n\n\n\n\n\n\n# redo clustering after removing data points from start and end of dataset\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 7), squeeze=False)\nfor i, (method, threshold) in enumerate([('single', 3.5), ('complete', 7)]):\n    ax = axes[0, i]\n    Z = linkage(cereals_norm.iloc[4:-4,:], method=method, metric='euclidean')\n    dendrogram(Z, color_threshold=threshold, ax=ax, no_labels=True)\n    ax.set_xlabel(method + ' linkage')\n\n\n\n\n\n\n\n\nClusters appear to be not well stabilized as removing some data resulted in different dendrograms."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-15-ProbSolutions-CA.html#solution-15.3.b",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-15-ProbSolutions-CA.html#solution-15.3.b",
    "title": "Chapter 15: Cluster Analysis",
    "section": "Solution 15.3.b",
    "text": "Solution 15.3.b\nWhich method leads to the most insightful or meaningful clusters?\nComplete linkage leads to more insightful clusters."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-15-ProbSolutions-CA.html#solution-15.3.c",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-15-ProbSolutions-CA.html#solution-15.3.c",
    "title": "Chapter 15: Cluster Analysis",
    "section": "Solution 15.3.c",
    "text": "Solution 15.3.c\nChoose one of the methods. How many clusters would you use? What distance is used for this cutoff? (Look at the dendrogram.)\nWe would use complete linkage method, cutting the dendrogram at around 7 to get 5 clusters."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-15-ProbSolutions-CA.html#solution-15.3.d",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-15-ProbSolutions-CA.html#solution-15.3.d",
    "title": "Chapter 15: Cluster Analysis",
    "section": "Solution 15.3.d",
    "text": "Solution 15.3.d\nThe elementary public schools would like to choose a set of cereals to include in their daily cafeterias. Every day a different cereal is offered, but all cereals should support a healthy diet. For this goal, you are requested to find a cluster of “healthy cereals.” Should the data be normalized? If not, how should they be used in the cluster analysis?\n\nZ = linkage(cereals_norm, method='ward', metric='euclidean')\nmemb = fcluster(Z, 5, criterion='maxclust')\n\ncentroids = {}\nfor key, item in cereals_nonmissing.groupby(memb):\n    centroids['Cluster {}'.format(key)] = item.mean()\n\npd.DataFrame(centroids).round(2)\n\n\n\n\n\n\n\n\nCluster 1\nCluster 2\nCluster 3\nCluster 4\nCluster 5\n\n\n\n\ncalories\n63.33\n124.00\n110.95\n82.22\n103.81\n\n\nprotein\n4.00\n3.15\n1.52\n2.44\n2.71\n\n\nfat\n0.67\n1.95\n1.00\n0.11\n0.52\n\n\nsodium\n176.67\n155.00\n172.38\n1.67\n226.19\n\n\nfiber\n11.00\n3.10\n0.57\n2.11\n1.67\n\n\ncarbo\n6.67\n13.95\n12.62\n15.33\n18.48\n\n\nsugars\n3.67\n9.35\n11.29\n2.33\n3.33\n\n\npotass\n310.00\n151.50\n45.95\n90.56\n73.81\n\n\nvitamins\n25.00\n31.25\n25.00\n11.11\n39.29\n\n\nshelf\n3.00\n2.90\n1.67\n2.00\n2.10\n\n\nweight\n1.00\n1.17\n1.00\n0.87\n1.00\n\n\ncups\n0.39\n0.69\n0.89\n0.85\n0.93\n\n\nrating\n73.84\n38.26\n28.85\n63.02\n46.47\n\n\n\n\n\n\n\nThe most deleterious attributes of breakfast cereals, particularly those for children, are high levels of salt and sugar. Cluster 4 is the most health cluster in this respect."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-15-ProbSolutions-CA.html#solution-15.4.a",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-15-ProbSolutions-CA.html#solution-15.4.a",
    "title": "Chapter 15: Cluster Analysis",
    "section": "Solution 15.4.a",
    "text": "Solution 15.4.a\nApply hierarchical clustering with Euclidean distance and Ward’s method. Make sure to normalize the data first. How many clusters appear?\n\nairlines_df = pd.read_csv(DATA / 'EastWestAirlinesCluster.csv')\nairlines_df = airlines_df.set_index(\"ID#\")\nairlines_df.head()\n\n\n\n\n\n\n\n\nBalance\nQual_miles\ncc1_miles\ncc2_miles\ncc3_miles\nBonus_miles\nBonus_trans\nFlight_miles_12mo\nFlight_trans_12\nDays_since_enroll\nAward?\n\n\nID#\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n28143\n0\n1\n1\n1\n174\n1\n0\n0\n7000\n0\n\n\n2\n19244\n0\n1\n1\n1\n215\n2\n0\n0\n6968\n0\n\n\n3\n41354\n0\n1\n1\n1\n4123\n4\n0\n0\n7034\n0\n\n\n4\n14776\n0\n1\n1\n1\n500\n1\n0\n0\n6952\n0\n\n\n5\n97752\n0\n4\n1\n1\n43300\n26\n2077\n4\n6935\n1\n\n\n\n\n\n\n\n\nairlines_norm = (airlines_df - airlines_df.mean())/airlines_df.std()\nairlines_norm.head()\n\n\n\n\n\n\n\n\nBalance\nQual_miles\ncc1_miles\ncc2_miles\ncc3_miles\nBonus_miles\nBonus_trans\nFlight_miles_12mo\nFlight_trans_12\nDays_since_enroll\nAward?\n\n\nID#\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n-0.451084\n-0.186275\n-0.769482\n-0.09823\n-0.062759\n-0.702698\n-1.103926\n-0.328562\n-0.362123\n1.395280\n-0.766823\n\n\n2\n-0.539389\n-0.186275\n-0.769482\n-0.09823\n-0.062759\n-0.701001\n-0.999801\n-0.328562\n-0.362123\n1.379784\n-0.766823\n\n\n3\n-0.319991\n-0.186275\n-0.769482\n-0.09823\n-0.062759\n-0.539185\n-0.791550\n-0.328562\n-0.362123\n1.411744\n-0.766823\n\n\n4\n-0.583726\n-0.186275\n-0.769482\n-0.09823\n-0.062759\n-0.689200\n-1.103926\n-0.328562\n-0.362123\n1.372037\n-0.766823\n\n\n5\n0.239648\n-0.186275\n1.409295\n-0.09823\n-0.062759\n1.082986\n1.499207\n1.154788\n0.692404\n1.363805\n1.303755\n\n\n\n\n\n\n\n\nZ = linkage(airlines_norm, method='ward', metric='euclidean')\ndendrogram(Z, color_threshold=100, no_labels=True)\nplt.show()\n\n\n\n\n\n\n\n\nThe dataset can be split into two clusters."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-15-ProbSolutions-CA.html#solution-15.4.b",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-15-ProbSolutions-CA.html#solution-15.4.b",
    "title": "Chapter 15: Cluster Analysis",
    "section": "Solution 15.4.b",
    "text": "Solution 15.4.b\nWhat would happen if the data were not normalized?\nNormalizing the data is important to ensure that the “distance measured” accords equal weight to each variable - without normalization, the variable with the largest scale will dominate the measure. Here the variables that have the largest scale are Balance, Bonus_trans, Fligt_miles_12mo, and Days_since_enroll. These variables will dominate the measure if we don’t standardize the data."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-15-ProbSolutions-CA.html#solution-15.4.c",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-15-ProbSolutions-CA.html#solution-15.4.c",
    "title": "Chapter 15: Cluster Analysis",
    "section": "Solution 15.4.c",
    "text": "Solution 15.4.c\nCompare the cluster centroid to characterize the different clusters, and try to give each cluster a label.\n\nmemb = fcluster(linkage(airlines_norm, method='ward', metric='euclidean'), 2, criterion='maxclust')\ncentroids = {}\nfor key, item in airlines_df.groupby(memb):\n    centroids[key] = item.mean()\n    print('Cluster {}: size {}'.format(key, len(item)))\npd.DataFrame(centroids).transpose()\n\nCluster 1: size 2489\nCluster 2: size 1510\n\n\n\n\n\n\n\n\n\nBalance\nQual_miles\ncc1_miles\ncc2_miles\ncc3_miles\nBonus_miles\nBonus_trans\nFlight_miles_12mo\nFlight_trans_12\nDays_since_enroll\nAward?\n\n\n\n\n1\n46718.863399\n9.274407\n1.242266\n1.023303\n1.00000\n5037.792688\n7.091201\n221.167135\n0.700281\n3772.785858\n0.188027\n\n\n2\n117912.886093\n366.377483\n3.406623\n1.000000\n1.03245\n37101.439735\n19.037086\n853.826490\n2.483444\n4688.513245\n0.670861\n\n\n\n\n\n\n\nObserving the cluster centroids we can see that Cluster 1 has lower values on virtually all variables. We can think of Cluster 1 as “occasional and minimal flyers” and Cluster 2 as “frequent and loyal travelers.”"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-15-ProbSolutions-CA.html#solution-15.4.d",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-15-ProbSolutions-CA.html#solution-15.4.d",
    "title": "Chapter 15: Cluster Analysis",
    "section": "Solution 15.4.d",
    "text": "Solution 15.4.d\nTo check the stability of the clusters, remove a random 5% of the data (by taking a random sample of 95% of the records), and repeat the analysis. Does the same picture emerge?\n\n# we repeat the random sampling three times\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(10, 7))\nfor ax in axes:\n    Z = linkage(airlines_norm.sample(frac=0.95), method='ward', metric='euclidean')\n    dendrogram(Z, color_threshold=100, ax=ax, no_labels=True)\nplt.show()\n\n\n\n\n\n\n\n\nThe dendrogram looks a bit different, but the same conclusions result: clusters occur naturally at 2 clusters, and the characteristics of the resulting clusters are similar to those in the clusters found in the entire data set."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-15-ProbSolutions-CA.html#solution-15.4.e",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-15-ProbSolutions-CA.html#solution-15.4.e",
    "title": "Chapter 15: Cluster Analysis",
    "section": "Solution 15.4.e",
    "text": "Solution 15.4.e\nUse \\(k\\)-means clustering with the number of clusters that you found above. Does the same picture emerge?\n\nkmeans = KMeans(n_clusters=2, random_state=0).fit(airlines_norm)\n\n# Compare the cluster membership between k-means and hierarchical clustering\npd.crosstab(memb, kmeans.labels_)\n\n\n\n\n\n\n\ncol_0\n0\n1\n\n\nrow_0\n\n\n\n\n\n\n1\n2436\n53\n\n\n2\n261\n1249\n\n\n\n\n\n\n\nThe cluster membership from the two methods is comparable.\n\ncentroids = {}\nfor key, item in airlines_df.groupby(kmeans.labels_):\n    centroids[key] = item.mean()\n    print('Cluster {}: size {}'.format(key, len(item)))\npd.DataFrame(centroids).transpose()\n\nCluster 0: size 2697\nCluster 1: size 1302\n\n\n\n\n\n\n\n\n\nBalance\nQual_miles\ncc1_miles\ncc2_miles\ncc3_miles\nBonus_miles\nBonus_trans\nFlight_miles_12mo\nFlight_trans_12\nDays_since_enroll\nAward?\n\n\n\n\n0\n45041.650352\n89.232481\n1.310716\n1.016685\n1.000371\n5421.404894\n7.363737\n215.211346\n0.635521\n3720.834631\n0.206897\n\n\n1\n132760.658986\n257.798771\n3.610599\n1.009985\n1.036866\n41429.117512\n20.380952\n967.233487\n2.902458\n4942.417819\n0.708909\n\n\n\n\n\n\n\n\ncentroids = pd.DataFrame(kmeans.cluster_centers_, columns=airlines_norm.columns)\ncentroids['cluster'] = ['Cluster {}'.format(i) for i in centroids.index]\n\nplt.figure(figsize=(10,6))\nfig.subplots_adjust(right=3)\nax = parallel_coordinates(centroids, class_column='cluster', colormap='Dark2', linewidth=5)\nplt.legend(loc='center left', bbox_to_anchor=(0.95, 0.5))\nplt.xlim(-0.5,7.5)\nplt.show()"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-15-ProbSolutions-CA.html#solution-15.4.f",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-15-ProbSolutions-CA.html#solution-15.4.f",
    "title": "Chapter 15: Cluster Analysis",
    "section": "Solution 15.4.f",
    "text": "Solution 15.4.f\nWhich clusters would you target for offers, and what types of offers would you target to customers in that cluster?\nFor the “minimal, non-frequent” flyers, two types of offers might be used. 1. Offers to liquidate the mileage, to remove it as a liability (e.g., offers to purchase magazine subscriptions) 2. Offers for special mileage bonuses if a number of segments or miles is flown in a limited period of time, in case some of these flyers are regular customers of other airlines, or new flyers, in hopes that some of them will become more “invested” in East-West.\nFor the frequent, loyal flyer we might\n\nOffer luxury goods in conjunction with partners (high end vacations, exclusive real estate, etc.) – frequent flyers are likely to be relatively prosperous (compared to non-frequent flyers)."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-19-ProbSolutions-SNA.html",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-19-ProbSolutions-SNA.html",
    "title": "Chapter 19: Social Network Analytics",
    "section": "",
    "text": "2019-2020 Galit Shmueli, Peter C. Bruce, Peter Gedeck\n\nData Mining for Business Analytics: Concepts, Techniques, and Applications in Python (First Edition) Galit Shmueli, Peter C. Bruce, Peter Gedeck, and Nitin R. Patel. 2019.\nDate: 2020-03-08\nPython Version: 3.8.2 Jupyter Notebook Version: 5.6.1\nPackages: - matplotlib: 3.2.0 - networkx: 2.4 - pandas: 1.0.1\nThe assistance from Mr. Kuber Deokar and Ms. Anuja Kulkarni in preparing these solutions is gratefully acknowledged.\n# import required packages for this chapter\nfrom pathlib import Path\n\nimport pandas as pd\nimport networkx as nx\n\nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n# Working directory:\n#\n# We assume that data are kept in the same directory as the notebook. If you keep your \n# data in a different folder, replace the argument of the `Path`\nDATA = Path('.')\n# and then load data using \n#\n# pd.read_csv(DATA / ‘filename.csv’)"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-19-ProbSolutions-SNA.html#solution-19.1.a",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-19-ProbSolutions-SNA.html#solution-19.1.a",
    "title": "Chapter 19: Social Network Analytics",
    "section": "Solution 19.1.a",
    "text": "Solution 19.1.a\nProduce a network plot for this network.\n\ndf = pd.DataFrame([('A', 'B'), ('A', 'C'), ('B', 'C'), ('C', 'D'), ('D', 'E')], columns=['from', 'to'])\nG = nx.from_pandas_edgelist(df, 'from', 'to')\n\nnx.draw(G, with_labels=True, node_color='skyblue')\nplt.show()"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-19-ProbSolutions-SNA.html#solution-19.1.b",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-19-ProbSolutions-SNA.html#solution-19.1.b",
    "title": "Chapter 19: Social Network Analytics",
    "section": "Solution 19.1.b",
    "text": "Solution 19.1.b\nWhat node(s) would need to be removed from the network for the remaining nodes to constitute a clique?\nNodes D and E would need to be removed from the graph for the remaining nodes to constitute a clique."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-19-ProbSolutions-SNA.html#solution-19.1.c",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-19-ProbSolutions-SNA.html#solution-19.1.c",
    "title": "Chapter 19: Social Network Analytics",
    "section": "Solution 19.1.c",
    "text": "Solution 19.1.c\nWhat is the degree for node A?\n\nG.degree()\n\nDegreeView({'A': 2, 'B': 2, 'C': 3, 'D': 2, 'E': 1})\n\n\nThe degree for node A is 2"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-19-ProbSolutions-SNA.html#solution-19.1.d",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-19-ProbSolutions-SNA.html#solution-19.1.d",
    "title": "Chapter 19: Social Network Analytics",
    "section": "Solution 19.1.d",
    "text": "Solution 19.1.d\nWhich node(s) have the lowest degree?\nNode E has lowest degree, i.e. 1."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-19-ProbSolutions-SNA.html#solution-19.1.e",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-19-ProbSolutions-SNA.html#solution-19.1.e",
    "title": "Chapter 19: Social Network Analytics",
    "section": "Solution 19.1.e",
    "text": "Solution 19.1.e\nTabulate the degree distribution for this network.\n\npd.DataFrame(G.degree())\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n0\nA\n2\n\n\n1\nB\n2\n\n\n2\nC\n3\n\n\n3\nD\n2\n\n\n4\nE\n1"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-19-ProbSolutions-SNA.html#solution-19.1.e-1",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-19-ProbSolutions-SNA.html#solution-19.1.e-1",
    "title": "Chapter 19: Social Network Analytics",
    "section": "Solution 19.1.e",
    "text": "Solution 19.1.e\nIs this network connected?\nThe network is connected as a path can be found between any pair of nodes."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-19-ProbSolutions-SNA.html#solution-19.1.e-2",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-19-ProbSolutions-SNA.html#solution-19.1.e-2",
    "title": "Chapter 19: Social Network Analytics",
    "section": "Solution 19.1.e",
    "text": "Solution 19.1.e\nCalculate the betweenness centrality for nodes A and C.\n\nprint('Betweenness', nx.betweenness_centrality(G))\n\nBetweenness {'A': 0.0, 'B': 0.0, 'C': 0.6666666666666666, 'D': 0.5, 'E': 0.0}\n\n\nThe betweenness centrality for node A is 0. None of the shortest paths between the other nodes passes through A.\nFor node C, the unnormalized betweenness is 4. The number of all possible shortest paths is 4*3/2 = 6 and therefore the betweenness is 0.66."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-19-ProbSolutions-SNA.html#solution-19.1.e-3",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-19-ProbSolutions-SNA.html#solution-19.1.e-3",
    "title": "Chapter 19: Social Network Analytics",
    "section": "Solution 19.1.e",
    "text": "Solution 19.1.e\nCalculate the density of the network.\n\nprint('Density', nx.density(G))\n\nDensity 0.5"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-19-ProbSolutions-SNA.html#solution-19.2.a",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-19-ProbSolutions-SNA.html#solution-19.2.a",
    "title": "Chapter 19: Social Network Analytics",
    "section": "Solution 19.2.a",
    "text": "Solution 19.2.a\nBy what percent has the number of nodes increased?\nIf we add two new nodes the total nodes in the network will be 7. The number of nodes in the original network is 5. To calculate % increase in nodes, first calculate the difference between the two numbers, i.e. 7 - 5 = 2, which\nis the “increase”. Then divide the increase by the original number of nodes and multiply the answer by 100:\n\nprint('% increase in number of nodes = 2/5 * 100 = ', 2 / 5 * 100)\n\n% increase in number of nodes = 2/5 * 100 =  40.0"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-19-ProbSolutions-SNA.html#solution-19.2.b",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-19-ProbSolutions-SNA.html#solution-19.2.b",
    "title": "Chapter 19: Social Network Analytics",
    "section": "Solution 19.2.b",
    "text": "Solution 19.2.b\nBy what percent has the number of possible edges increased?\nFor an undirected network, the number of maximum edges is given by\nn(n-1)/2\nwhere n is the number of nodes in the network.\nLet’s calculate maximum number of edges in each of old and new network:\nNumber of maximum edges in the old network with 5 nodes\n = 5 (5-1) / 2 \n = 20 / 2 \n = 10\nNumber of maximum edges in the new network with 7 nodes\n = 7 (7-1) / 2 \n = 42 / 2 \n = 21\nTherefore, the percent increase in the number of maximum edges after adding two new nodes is\n = ((21 - 10) / 10) * 100\n = (11/10) * 100\n = 110"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-19-ProbSolutions-SNA.html#solution-19.2.c",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-19-ProbSolutions-SNA.html#solution-19.2.c",
    "title": "Chapter 19: Social Network Analytics",
    "section": "Solution 19.2.c",
    "text": "Solution 19.2.c\nSuppose the new node has a typical (median) number of connections. What will happen to network density?\nIf the new node has typical number of connections, the density of the network will be reduced.\nConsider our old and new networks for the illustration. The typical (median) number of connections for each node in the old network is 2. Suppose our new network is resulted from the addition of two new nodes (each with number of connections 2) in the old network. So new network will have 9 edges and 7 nodes. As calculated above, the density of the old network = 0.5. Let’s calculate density of our new network\nDensity of new network with 7 nodes and 9 edges\n = 9/(7(7-1)/2) = 9 / 21 = 0.43."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-19-ProbSolutions-SNA.html#solution-19.2.d",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-19-ProbSolutions-SNA.html#solution-19.2.d",
    "title": "Chapter 19: Social Network Analytics",
    "section": "Solution 19.2.d",
    "text": "Solution 19.2.d\nComment on comparing densities in networks of different sizes.\nAs you add nodes to a network, in order to maintain the same density of the network the new nodes must have MORE than the typical number of connections of the existing network nodes. Therefore, in two networks with the same density, one large and one small, the nodes in the larger network will be more connected than those in the smaller."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-19-ProbSolutions-SNA.html#solution-19.2.e",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-19-ProbSolutions-SNA.html#solution-19.2.e",
    "title": "Chapter 19: Social Network Analytics",
    "section": "Solution 19.2.e",
    "text": "Solution 19.2.e\nTabulate the degree distribution for this network.\n\ndf = pd.DataFrame([('A', 'B'), ('A', 'C'), ('B', 'C'), ('C', 'D'), ('D', 'E'), \n                   ('A', 'G'), ('C', 'G'), ('D', 'F')], columns=['from', 'to'])\nG = nx.from_pandas_edgelist(df, 'from', 'to')\n\nnx.draw(G, with_labels=True, node_color='skyblue')\nplt.show()\n\n\n\n\n\n\n\n\n\npd.DataFrame(G.degree())\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n0\nA\n3\n\n\n1\nB\n2\n\n\n2\nC\n4\n\n\n3\nD\n3\n\n\n4\nE\n1\n\n\n5\nG\n2\n\n\n6\nF\n1"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-19-ProbSolutions-SNA.html#solution-19.3.a",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-19-ProbSolutions-SNA.html#solution-19.3.a",
    "title": "Chapter 19: Social Network Analytics",
    "section": "Solution 19.3.a",
    "text": "Solution 19.3.a\nUsing the number of common neighbors score, predict the next link to form (that is, suggest which new link has the best chance of success).\nThe next link to form is either between C and D or A and E since each of them has two common neighbors."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-19-ProbSolutions-SNA.html#solution-19.3.b",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-19-ProbSolutions-SNA.html#solution-19.3.b",
    "title": "Chapter 19: Social Network Analytics",
    "section": "Solution 19.3.b",
    "text": "Solution 19.3.b\nUsing the shortest path score, identify the link that is least likely to form.\nThe least likely link to form is between F and G since the shortest path score is 5 which is the largest in the network."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-10-ProbSolutions-LR.html",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Publisher/Class Activity/ProblemSolution_python/Ch-10-ProbSolutions-LR.html",
    "title": "Chapter 10: Logistic Regression (LR)",
    "section": "",
    "text": "2019-2020 Galit Shmueli, Peter C. Bruce, Peter Gedeck\n\nData Mining for Business Analytics: Concepts, Techniques, and Applications in Python (First Edition) Galit Shmueli, Peter C. Bruce, Peter Gedeck, and Nitin R. Patel. 2019.\nDate: 2020-03-09\nPython Version: 3.8.2 Jupyter Notebook Version: 5.6.1\nPackages: - dmba: 0.0.12 - matplotlib: 3.2.0 - numpy: 1.18.1 - pandas: 1.0.1 - scikit-learn: 0.22.2\nThe assistance from Mr. Kuber Deokar and Ms. Anuja Kulkarni in preparing these solutions is gratefully acknowledged.\n\n\n# Import required packages for this chapter\nfrom pathlib import Path\nimport math\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nfrom sklearn.metrics import accuracy_score\n\nimport matplotlib.pylab as plt\n\nfrom dmba import classificationSummary\nfrom dmba import AIC_score\nfrom dmba import backward_elimination, forward_selection, stepwise_selection\n\n%matplotlib inline\n\n\n# Working directory:\n#\n# We assume that data are kept in the same directory as the notebook. If you keep your \n# data in a different folder, replace the argument of the `Path`\nDATA = Path('.')\n# and then load data using \n#\n# pd.read_csv(DATA / ‘filename.csv’)\n\n\nProblem 10.1 Financial Condition of Banks.\nThe file Banks.csv includes data on a sample of 20 banks. The “Financial Condition” column records the judgement of an expert on the financial condition of each bank. This outcome variable takes one of two possible values – weak or strong– according to the financial condition of the bank. The predictors are two ratios used in the financial analysis of banks: TotLns&Lses/Assets is the ratio of total loans and leases to total assets and TotExp/Assets is the ratio of total expenses to total assets. The target is to use the two ratios for classifying the financial condition of a new bank.\nRun a logistic regression model (on the entire dataset) that model the status of a bank as a function of the two financial measures provided. Specify the success class as weak (this is similar to creating a dummy that is 1 for financially weak banks and 0 otherwise), and use the default cutoff value of 0.5.\n\n# load the data\nbank_df = pd.read_csv(DATA / 'banks.csv')\n\n# cheack few records\nbank_df.head()\n\n\n\n\n\n\n\n\nObs\nFinancial Condition\nTotCap/Assets\nTotExp/Assets\nTotLns&Lses/Assets\n\n\n\n\n0\n1\n1\n9.7\n0.12\n0.65\n\n\n1\n2\n1\n1.0\n0.11\n0.62\n\n\n2\n3\n1\n6.9\n0.09\n1.02\n\n\n3\n4\n1\n5.8\n0.10\n0.67\n\n\n4\n5\n1\n4.3\n0.11\n0.69\n\n\n\n\n\n\n\n\n# pretify the variable names\nbank_df.columns = [c.replace(' ', '_') for c in bank_df.columns]\nbank_df.columns = [c.replace('&', '') for c in bank_df.columns]\nbank_df.columns = [c.replace('/', '_') for c in bank_df.columns]\n\n# drop unwanted variables\nbank_df.drop(columns = ['Obs','TotCap_Assets'], inplace = True)\nbank_df.head()\n\n\n\n\n\n\n\n\nFinancial_Condition\nTotExp_Assets\nTotLnsLses_Assets\n\n\n\n\n0\n1\n0.12\n0.65\n\n\n1\n1\n0.11\n0.62\n\n\n2\n1\n0.09\n1.02\n\n\n3\n1\n0.10\n0.67\n\n\n4\n1\n0.11\n0.69\n\n\n\n\n\n\n\n\n# fit a logistic regression (set penalty=l2 and C=1e42 to avoid regularization)\nlogit_reg = LogisticRegression(penalty=\"l2\", C=1e42, solver='liblinear')\ny = bank_df['Financial_Condition']\nX = bank_df.drop(columns=['Financial_Condition'])\nlogit_reg.fit(X, y)\n\nLogisticRegression(C=1e+42, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n                   multi_class='auto', n_jobs=None, penalty='l2',\n                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n                   warm_start=False)\n\n\n\n# print coefficients and AIC measures\nprint('intercept ', logit_reg.intercept_[0])\nprint(pd.DataFrame({'coeff': logit_reg.coef_[0]}, index=X.columns).transpose())\nprint('AIC', AIC_score(y, logit_reg.predict(X), df = len(X.columns) + 1))\n\nintercept  -14.720832806179043\n       TotExp_Assets  TotLnsLses_Assets\ncoeff      89.832567           8.371267\nAIC 18.705839468305996\n\n\n10.1.a. Write the estimated equation that associates the financial condition of a bank with its two predictors in three formats:\n10.1.a.i. The logit as a function of the predictors\nAnswer:\nlogit = -14.7208 + (89.8326 * TotExp_Assets) + (8.3713 * TotLnsLses_Assets)\n10.1.a.ii. The odds as a function of the predictors\nAnswer\nOdds = e^(logit) = e^(-14.7208 + (89.8326 * TotExp_Assets) + (8.3713 * TotLns&Lses_Assets)\n10.1.a.ii. The probability as a function of the predictors\nAnswer:\np = (1 + Exp[-(-14.7208 + (89.8326 * TotExp/Assets) + (8.3713 * TotLns&Lses/Assets)])^-1\n10.1.b. Consider a new bank whose total loans and leases/assets ratio = 0.6 and total expenses/assets ratio = 0.11. From your logistic regression model, estimate the following four quantities for this bank (use Python to do all the intermediate calculations; show your final answers to four decimal places): the logit, the odds, the probability of being financially weak, and the classification of the bank (use cutoff = 0.5).\n\n# new record\nnew_bank = pd.DataFrame(\n    [[0.11, 0.6]],\n    columns=['TotExp_Assets', 'TotLns_Lses_Assets'])\nnew_bank\n\n\n\n\n\n\n\n\nTotExp_Assets\nTotLns_Lses_Assets\n\n\n\n\n0\n0.11\n0.6\n\n\n\n\n\n\n\n\n# calculate logit, odds and probability of being financially weak\nlogit_new = -14.7210 + (89.8339 * new_bank.TotExp_Assets) + (8.3713 * new_bank.TotLns_Lses_Assets)\nprint(\"logit = \", logit_new)\nodds = math.exp(-logit_new)\nprint(\"odds = \", odds)\nprob = 1/(1+odds)\nprint(\"prob = \", prob)\n\nlogit =  0    0.183509\ndtype: float64\nodds =  0.8323443846047233\nprob =  0.5457489369367221\n\n\nProbability that the new bank is 0.5457 and therefore the predicted class for this new bank is 1, or “financially week”.\n10.1.c. The cutoff value of 0.5 is used in conjunction with the probability of being financially weak. Compute the threshold that should be used if we want to make a classification based on the odds of being financially weak, and the threshold for the corresponding logit.\nAnswer:\nCutoff value of p=0.5.\nOdds = (p) / (1-p) = (0.5) / (1-0.5) = 1\nIf odds &gt; 1 then classify financial status as “weak” (otherwise classify as “strong”).\nLogit = ln (odds) = ln (1) = 0\nIf Logit &gt; 0 then classify financial status as “weak” (otherwise, classify it as “strong”)\nTherefore, a cutoff of 0.5 on the probability of being weak is equivalent to a threshold of 1 on the odds of being weak, and to a threshold of 0 on the logit.\n10.1.d. Interpret the estimated coefficient for the total loans & leases to total assets ratio (TotLns&Lses/Assets) in terms of the odds of being financially weak.\nAnswer:\nA positive coefficient in the logit model translates into a coefficient larger than 1 in the odds model. In the logit model, the estimated coefficient for total expenses-to-assets ratio is 8.37. In the odds models, the coefficient is e^8.37 = 4316. This means that an increase of a unit in total loans and leases-to-assets is associated with an increase in the odds of being financially weak by a factor of 4321.\n10.1.e. When a bank that is in poor financial condition is misclassified as financially strong, the misclassification cost is much higher than when a financially strong bank is misclassified as weak. To minimize the expected cost of misclassification, should the cutoff value for classification (which is currently at 0.5) be increased or decreased?\nAnswer:\nIn order to minimize the expected cost of misclassification in this case, we need to decrease the cutoff.\n\n\nProblem 10.2 Identifying Good System Administrators.\nA management consultant is studying the roles played by experience and training in a system administrator’s ability to complete a set of tasks in a specified amount of time. In particular, she is interested in discriminating between administrators who are able to complete given tasks within a specified time and those who are not. Data are collected on the performance of 75 randomly selected administrators. They are stored in the file SystemAdministrators.csv.\nThe variable Experience measures months of full-time system administrator experience, while Training measures the number of relevant training credits. The outcome variable Completed is either Yes or No, according to whether or not the administrator completed the tasks.\n\n# load the data and review some records\nadmin_df = pd.read_csv(DATA / 'SystemAdministrators.csv')\nadmin_df.head()\nadmin_df\n\n\n\n\n\n\n\n\nExperience\nTraining\nCompleted task\n\n\n\n\n0\n10.9\n4\nYes\n\n\n1\n9.9\n4\nYes\n\n\n2\n10.4\n6\nYes\n\n\n3\n13.7\n6\nYes\n\n\n4\n9.4\n8\nYes\n\n\n...\n...\n...\n...\n\n\n70\n5.6\n4\nNo\n\n\n71\n5.9\n8\nNo\n\n\n72\n6.4\n6\nNo\n\n\n73\n3.8\n4\nNo\n\n\n74\n5.3\n4\nNo\n\n\n\n\n75 rows × 3 columns\n\n\n\n\n# preetify the variable names\nadmin_df.columns = [c.replace(' ', '_') for c in admin_df.columns]\nadmin_df.columns\n\nIndex(['Experience', 'Training', 'Completed_task'], dtype='object')\n\n\n\n# convert values of response variable from Yes/No to 0/1\nadmin_df.loc[admin_df.Completed_task == 'Yes', 'Completed_task'] = 1\nadmin_df.loc[admin_df.Completed_task == 'No', 'Completed_task'] = 0\n\n10.2.a. Create a scatter plot of Experience vs. Training using color or symbol to distinguish programmers who completed the task from those who did not complete it. Which predictor(s) appear(s) potentially useful for classifying task completion?\nAnswer:\n\n# plot Experience vs. Training \nfig, ax = plt.subplots()\n\nsubset = admin_df.loc[admin_df['Completed_task']== 0]\nax.scatter(subset.Training, subset.Experience, marker='o', label='No')\n\nsubset1 = admin_df.loc[admin_df['Completed_task']== 1]\nax.scatter(subset1.Training, subset1.Experience, marker='D', label='Yes')\n\nplt.xlabel('Training')  # set x-axis label\nplt.ylabel('Experience')  # set y-axis label\n    \nhandles, labels = ax.get_legend_handles_labels()\nax.legend(handles, labels)\n\n\n\n\n\n\n\n\nFrom the scatterplot we can observe that programmers who completed the task tend to have more experience. Training, however, does not play much a role in task completion. Therefore, the predictor Experience appears more useful for classifying task completion.\n10.2.b. Run a logistic regression model withboth predictors using the entire dataset as training data. Among those who completd the task, what is the percentage of programmers incorrectly classified as failing to complete the task?\n\n# fit a logistic regression (set penalty=l2 and C=1e42 to avoid regularization)\nadmin_df.Completed_task = admin_df.Completed_task.astype('category')\nlogit_reg = LogisticRegression(penalty=\"l2\", C=1e42, solver='liblinear')\ny = admin_df['Completed_task']\nX = admin_df.drop(columns=['Completed_task'])\nclasses = list(y.cat.categories)\nlogit_reg.fit(X, y)\n\nLogisticRegression(C=1e+42, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n                   multi_class='auto', n_jobs=None, penalty='l2',\n                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n                   warm_start=False)\n\n\n\n# predicted y\ny_pred = logit_reg.predict(X)\n\n\n# Confusion matrix\nclassificationSummary(y, y_pred, class_names=classes)\n\nConfusion Matrix (Accuracy 0.9067)\n\n       Prediction\nActual  0  1\n     0 58  2\n     1  5 10\n\n\nAmong those who completed the task (15), 5 are misclassified as failing to complete the tast. So the percentage of programmers incorrectly classified as failing to complete task is 5/15 = 33.33%.\n10.2.c. To decrease the percentage in part (b), should the cutoff probability be increased or decreased?\nAnswer:\nlowering the cutoff will lead to a reduced (or equal) classification error rate for truly completed records.\n10.2.d. How much experience must be accumulated by a programmer with 4 years of training before his or her estimated probability of completing the task exceeds 0.5?\nAnswer:\nThe cutoff of 0.5 on the probability of task completion is equivalent to a threshold of 0 on the logit. We therefore write the estimated logit equation, equate it to 0 and set Training=4.\nlet’s say p = 0.6\nLogit = ln [(p) / (1-p)] = 0.4055 = -10.9794 + 1.1267Experience + 0.1805Training.\nWe plug Training = 4 and solve for Experience.\n9.464 months experience must be accumulated by a programmer with 4 years of education before his/her estimated probability of completing the task exceeds 50%.\n\n\nProblem 10.3 Sales of Riding Mowers.\nA company that manufactures riding mowers wants to identify the best sales prospects for an intensive sales campaign. In particular, the manufacturer is interested in classifying households as prospective owners or nonowners on the basis of Income (in $1000s) and Lot Size (in 1000 ft2). The marketing expert looked at a random sample of 24 households, given in the file RidingMowers.csv. Use all the data to fit a logistic regression of ownership on the two predictors.\n\n# load the data and review few records\nmower_df = pd.read_csv(DATA/'RidingMowers.csv')\nmower_df.head()\n\n\n\n\n\n\n\n\nIncome\nLot_Size\nOwnership\n\n\n\n\n0\n60.0\n18.4\nOwner\n\n\n1\n85.5\n16.8\nOwner\n\n\n2\n64.8\n21.6\nOwner\n\n\n3\n61.5\n20.8\nOwner\n\n\n4\n87.0\n23.6\nOwner\n\n\n\n\n\n\n\n10.3.a. What percentage of households in the study were owners of a riding mower?\n\n# count of owners in the data\nprint(mower_df['Ownership'].value_counts() / len(mower_df))\n\nOwner       0.5\nNonowner    0.5\nName: Ownership, dtype: float64\n\n\n50% of households were owners of riding mowers.\n10.3.b. Create a scatter plot of Income vs. Lot Size using color or symbol to distinguish owners from nonowners. From the scatter plot, which class seems to have a higher average income, owners or nonowners?\nAnswer:\n\n# scater plot of Income vs. Lot Size \nfig, ax = plt.subplots()\n\nsubset = mower_df.loc[mower_df['Ownership']== 'Owner']\nax.scatter(subset.Lot_Size, subset.Income, marker='o', label='Owner')\n\nsubset1 = mower_df.loc[mower_df['Ownership']== 'Nonowner']\nax.scatter(subset1.Lot_Size, subset1.Income, marker='D', label='Nonowner')\n\nplt.xlabel('Lot_Size')  # set x-axis label\nplt.ylabel('Income')  # set y-axis label\n    \nhandles, labels = ax.get_legend_handles_labels()\nax.legend(handles, labels)\n\n\n\n\n\n\n\n\nFrom the scatterplot it appears that “owners” tend to have a higher average income.\n10.3.c. Among nonowners, what is the percentage of households classified correctly?\n\n# fit a logistic regression (set penalty=l2 and C=1e42 to avoid regularization)\n# change values of response variable from Owner/Nonowner to 1/0\nmower_df['Ownership'] = [1 if v == 'Owner' else 0 for v in mower_df['Ownership']]\n# convert response variable to type categorical\nmower_df.Ownership = mower_df.Ownership.astype('category')\n\nlogit_reg = LogisticRegression(penalty=\"l2\", C=1e42, solver='liblinear')\ny = mower_df['Ownership']\nX = mower_df.drop(columns=['Ownership'])\nclasses = list(y.cat.categories)\nlogit_reg.fit(X, y)\n\nLogisticRegression(C=1e+42, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n                   multi_class='auto', n_jobs=None, penalty='l2',\n                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n                   warm_start=False)\n\n\n\n# print coefficients\nprint('intercept ', logit_reg.intercept_[0])\nprint(pd.DataFrame({'coeff': logit_reg.coef_[0]}, index=X.columns).transpose())\n\nintercept  -25.870471875170026\n         Income  Lot_Size\ncoeff  0.110611  0.961144\n\n\n\n# predicted y\ny_pred = logit_reg.predict(X)\n\n\n# Confusion matrix\nclassificationSummary(y, y_pred, class_names=classes)\n\nConfusion Matrix (Accuracy 0.8333)\n\n       Prediction\nActual  0  1\n     0 10  2\n     1  2 10\n\n\n83.33% (10 out of 12) nononwers are classified correctly.\n10.3.d. To increase the percentage of correctly classified nonowners, should the cutoff probability be increased or decreased?\nAnswer:\nFrom the classification confusion matrix shown in (c) we can see that 10 of the 12 non-owners are correctly classified. If we increase the cutoff (i.e. make it harder to be classified as an owner), we can reduce the number of non-owners misclassified as owners, and improve the classification performance with non-owners. (Of course, this will be at the cost of misclassifying more of the owners as non-owners.)\n10.3.e. What are the odds that a household with a $60K income and a lot size of 20,000 ft2 is an owner?\nAnswer:\n\n# new household\nIncome = 60\nLot_Size = 20\nodds = math.exp(-25.870471875168775 + 0.110611 * Income + 0.961144 * Lot_Size)\nodds\n\n0.9891276606350936\n\n\n10.3.f. What is the classification of a household with a $60K income and a lot size of 20,000 ft2? Use cutoff = 0.5.\n\n# logit for new household\nlogit = -25.870471875168775 + 0.110611 * Income + 0.961144 * Lot_Size\nodds = math.exp(logit)\nprob = 1/(1+odds)\nprob\n\n0.5027329415753625\n\n\nProbability that the new household is 0.50 and thus the predicted class for this new household is 0, or “non-owner”.\n10.3.g. What is the minimum income that a household with 16,000 ft2 lot size should have before it is classified as an owner?\nThe cutoff of 0.5 on the probability of owner is equivalent to a threshold of 0 on the logit. We therefore write the estimated logit equation, equate it to 0 and set Lot_Size=16.\n0 = -25.870471875168775 + 0.110611Income + 0.961144Lot_Size.\nWe plug Lot_Size = 16 and solve for Income.\n\nIncome = (25.870471875168775 - (0.961144*16)) / 0.110611\nIncome\n\n94.85645980208817\n\n\nThe minimum income needed for this household to be classified as “owner” is $94.8368K.\n\n\nProblem 10.4 Competitive Auctions on eBay.com.\nThe file eBayAuctions.csv contains information on 1972 auctions transacted on eBay.com during May–June 2004. The goal is to use these data to build a model that will distinguish competitive auctions from noncompetitive ones. A competitive auction is defined as an auction with at least two bids placed on the item being auctioned. The data include variables that describe the item (auction category), the seller (his or her eBay rating), and the auction terms that the seller selected (auction duration, opening price, currency, day of week of auction close). In addition, we have the price at which the auction closed. The goal is to predict whether or not an auction of interest will be competitive.\nData preprocessing. Create dummy variables for the categorical predictors. These include Category (18 categories), Currency (USD, GBP, Euro), EndDay (Monday–Sunday), and Duration (1, 3, 5, 7, or 10 days).\n10.4.a. Create pivot tables for the mean of the binary outcome (Competitive?) as a function of the various categorical variables (use the original variables, not the dummies). Use the information in the tables to reduce the number of dummies that will be used in the model. For example, categories that appear most similar with respect to the distribution of competitive auctions could be combined.\nAnswer:\n\n# load the data\nebay_df = pd.read_csv(DATA / 'eBayAuctions.csv')\nebay_df.columns\n\nIndex(['Category', 'currency', 'sellerRating', 'Duration', 'endDay',\n       'ClosePrice', 'OpenPrice', 'Competitive?'],\n      dtype='object')\n\n\n\n# prettify variable names\nebay_df.columns = [c.replace('?', '') for c in ebay_df.columns]\nebay_df.columns\n\nIndex(['Category', 'currency', 'sellerRating', 'Duration', 'endDay',\n       'ClosePrice', 'OpenPrice', 'Competitive'],\n      dtype='object')\n\n\n\n# check the variable types\nebay_df.dtypes\n\nCategory         object\ncurrency         object\nsellerRating      int64\nDuration          int64\nendDay           object\nClosePrice      float64\nOpenPrice       float64\nCompetitive       int64\ndtype: object\n\n\n\n# pivot table of Competitive and Category\npivot1 = pd.pivot_table(ebay_df, index= 'Category', values= \"Competitive\",\n               aggfunc= [np.mean])\npivot1\n\n\n\n\n\n\n\n\nmean\n\n\n\nCompetitive\n\n\nCategory\n\n\n\n\n\nAntique/Art/Craft\n0.564972\n\n\nAutomotive\n0.353933\n\n\nBooks\n0.500000\n\n\nBusiness/Industrial\n0.666667\n\n\nClothing/Accessories\n0.504202\n\n\nCoins/Stamps\n0.297297\n\n\nCollectibles\n0.577406\n\n\nComputer\n0.666667\n\n\nElectronics\n0.800000\n\n\nEverythingElse\n0.235294\n\n\nHealth/Beauty\n0.171875\n\n\nHome/Garden\n0.656863\n\n\nJewelry\n0.365854\n\n\nMusic/Movie/Game\n0.602978\n\n\nPhotography\n0.846154\n\n\nPottery/Glass\n0.350000\n\n\nSportingGoods\n0.725806\n\n\nToys/Hobbies\n0.529915\n\n\n\n\n\n\n\n\n# pivot table of Competitive and \npivot2 = pd.pivot_table(ebay_df, index= 'currency', values= \"Competitive\",\n               aggfunc= [np.mean])\npivot2\n\n\n\n\n\n\n\n\nmean\n\n\n\nCompetitive\n\n\ncurrency\n\n\n\n\n\nEUR\n0.551595\n\n\nGBP\n0.687075\n\n\nUS\n0.519350\n\n\n\n\n\n\n\n\n# pivot table of Competitive and Category\npivot3 = pd.pivot_table(ebay_df, index= 'endDay', values= \"Competitive\",\n               aggfunc= [np.mean])\npivot3\n\n\n\n\n\n\n\n\nmean\n\n\n\nCompetitive\n\n\nendDay\n\n\n\n\n\nFri\n0.466899\n\n\nMon\n0.673358\n\n\nSat\n0.427350\n\n\nSun\n0.485207\n\n\nThu\n0.603960\n\n\nTue\n0.532164\n\n\nWed\n0.480000\n\n\n\n\n\n\n\nThe categories “Business/Industrials” and “Computers” have same average of the binary variable “Competitive.” This means that the percentage of competitive auctions in each of these two categories is equal. Hence we can combine the two categories into a single category called “Computers.” Similarly, the average of “competitive” for the categories “Antique/Art/Craft” and “Collectibles” are almost same. Therefore, we can combine them into a single category called “collectibles”. The auction ending days “Sunday” and “Friday” have approximately equal averages for “competitive”. So we can combine them into a single category called “Sun_Fri”.\n\nebay_df.loc[ebay_df.endDay == 'Sun', 'endDay'] = 'Sun_Fri'\nebay_df.loc[ebay_df.endDay == 'Fri', 'endDay'] = 'Sun_Fri'\n\nebay_df.loc[ebay_df.Category == \"Business/Industrial\", 'Category'] = 'Computer'\nebay_df.loc[ebay_df.Category == \"Antique/Art/Craft\", 'Category'] = 'Collectibles'\n\n\n# pivot table of Competitive and Category\npivot1 = pd.pivot_table(ebay_df, index= 'Category', values= \"Competitive\",\n               aggfunc= [np.mean])\npivot1\n\n\n\n\n\n\n\n\nmean\n\n\n\nCompetitive\n\n\nCategory\n\n\n\n\n\nAutomotive\n0.353933\n\n\nBooks\n0.500000\n\n\nClothing/Accessories\n0.504202\n\n\nCoins/Stamps\n0.297297\n\n\nCollectibles\n0.572115\n\n\nComputer\n0.666667\n\n\nElectronics\n0.800000\n\n\nEverythingElse\n0.235294\n\n\nHealth/Beauty\n0.171875\n\n\nHome/Garden\n0.656863\n\n\nJewelry\n0.365854\n\n\nMusic/Movie/Game\n0.602978\n\n\nPhotography\n0.846154\n\n\nPottery/Glass\n0.350000\n\n\nSportingGoods\n0.725806\n\n\nToys/Hobbies\n0.529915\n\n\n\n\n\n\n\n\n# pivot table of Competitive and Category\npivot3 = pd.pivot_table(ebay_df, index= 'endDay', values= \"Competitive\",\n               aggfunc= [np.mean])\npivot3\n\n\n\n\n\n\n\n\nmean\n\n\n\nCompetitive\n\n\nendDay\n\n\n\n\n\nMon\n0.673358\n\n\nSat\n0.427350\n\n\nSun_Fri\n0.476800\n\n\nThu\n0.603960\n\n\nTue\n0.532164\n\n\nWed\n0.480000\n\n\n\n\n\n\n\n10.4.b. Split the data into training (60%) and validation (40%) datasets. Run a logistic model with all predictors with a cutoff of 0.5.\nAnswer:\nSplit the data into training (60%) and validation (40%) datasets. Use the random seed 202.\n\n# change variable types to appropriate ones\nebay_df['Category'] = ebay_df['Category'].astype('category')\nebay_df['currency'] = ebay_df['currency'].astype('category')\nebay_df['endDay'] = ebay_df['endDay'].astype('category')\n\n# convert categorical variables into indicator and drop the first column of each of them\nebay_df = pd.get_dummies(ebay_df, prefix_sep='_')\nebay_df.drop(columns=['Category_Automotive', 'currency_EUR', 'endDay_Mon'], inplace=True)\n\n\n# predictor and response variables\nebay_df['Competitive'] = ebay_df['Competitive'].astype('category')\noutcome = 'Competitive'\npredictors = list(ebay_df.columns)\npredictors.remove(outcome)\ny = ebay_df[outcome]\nX = ebay_df[predictors]\n\n\n# split the data into training and validation sets\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.40, random_state=202)\n\n\n# fit a logistic regression (set penalty=l2 and C=1e42 to avoid regularization)\nlogit_reg = LogisticRegression(penalty=\"l2\", C=1e42, solver='liblinear')\nclasses = list(y.cat.categories)\nlogit_reg.fit(X_train, y_train)\n\nLogisticRegression(C=1e+42, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n                   multi_class='auto', n_jobs=None, penalty='l2',\n                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n                   warm_start=False)\n\n\n\n# print coefficients\nprint('intercept ', logit_reg.intercept_[0])\nprint(pd.DataFrame({'coeff': logit_reg.coef_[0]}, index=X.columns).transpose())\n\nintercept  -0.16233820301376703\n       sellerRating  Duration  ClosePrice  OpenPrice  Category_Books  \\\ncoeff     -0.000037 -0.009712    0.084273  -0.098255       -0.030295   \n\n       Category_Clothing/Accessories  Category_Coins/Stamps  \\\ncoeff                      -0.525984              -0.962637   \n\n       Category_Collectibles  Category_Computer  Category_Electronics  ...  \\\ncoeff                0.43238           0.501414              1.162978  ...   \n\n       Category_Pottery/Glass  Category_SportingGoods  Category_Toys/Hobbies  \\\ncoeff                -0.17042                0.485073               0.706662   \n\n       currency_GBP  currency_US  endDay_Sat  endDay_Sun_Fri  endDay_Thu  \\\ncoeff      1.174816     0.305728    -1.05426       -0.711331   -0.724128   \n\n       endDay_Tue  endDay_Wed  \ncoeff   -0.724097   -0.838984  \n\n[1 rows x 26 columns]\n\n\n\n# confusion matrix\n# predicted y for validation set\nval_pred = logit_reg.predict(X_valid)\n# Confusion matrix\nclassificationSummary(y_valid, val_pred, class_names=classes)\n\nConfusion Matrix (Accuracy 0.7592)\n\n       Prediction\nActual   0   1\n     0 289  74\n     1 116 310\n\n\n10.4.c. If we want to predict at the start of an auction whether it will be competitive, we cannot use the information on the closing price. Run a logistic model with all predictors as above, excluding price. How does this model compare to the full model with respect to predictive accuracy?\nAnswer:\n\n# drop the predictor 'ClosePrice'\nX_train = X_train.drop(columns=['ClosePrice'])\nX_valid = X_valid.drop(columns=['ClosePrice'])\n\n\n# fit a logistic regression (set penalty=l2 and C=1e42 to avoid regularization)\nlogit_reg = LogisticRegression(penalty=\"l2\", C=1e42, solver='liblinear')\nlogit_reg.fit(X_train, y_train)\n\nLogisticRegression(C=1e+42, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n                   multi_class='auto', n_jobs=None, penalty='l2',\n                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n                   warm_start=False)\n\n\n\n# print coefficients\nprint('intercept ', logit_reg.intercept_[0])\nprint(pd.DataFrame({'coeff': logit_reg.coef_[0]}, index=X_train.columns).transpose())\n\nintercept  1.239216727766818\n       sellerRating  Duration  OpenPrice  Category_Books  \\\ncoeff      -0.00004 -0.080785  -0.007146       -0.220607   \n\n       Category_Clothing/Accessories  Category_Coins/Stamps  \\\ncoeff                       0.285479              -0.815631   \n\n       Category_Collectibles  Category_Computer  Category_Electronics  \\\ncoeff               0.442175            0.98236              1.540873   \n\n       Category_EverythingElse  ...  Category_Pottery/Glass  \\\ncoeff                -1.887026  ...               -0.332043   \n\n       Category_SportingGoods  Category_Toys/Hobbies  currency_GBP  \\\ncoeff                1.358508               0.648472      0.958213   \n\n       currency_US  endDay_Sat  endDay_Sun_Fri  endDay_Thu  endDay_Tue  \\\ncoeff    -0.095301   -1.239433       -0.954428   -1.012888   -0.740699   \n\n       endDay_Wed  \ncoeff    -0.91522  \n\n[1 rows x 25 columns]\n\n\n\n# confusion matrix\n# predicted y for validation set\nval_pred = logit_reg.predict(X_valid)\n# Confusion matrix\nclassificationSummary(y_valid, val_pred, class_names=classes)\n\n# Keep track of results\naccuracies = {}\naccuracies['full model'] = [accuracy_score(y_train, logit_reg.predict(X_train)), accuracy_score(y_valid, val_pred)]\n\nConfusion Matrix (Accuracy 0.6312)\n\n       Prediction\nActual   0   1\n     0 213 150\n     1 141 285\n\n\nThe overall error rate for the full model (1-0.7605 = 23.95%) is much lower than that for the model excluding “close price” (36.63%). Hence the model excluding “close price” has lower predictive accuracy compared to the full model.\n10.4.d. Interpret the meaning of the coefficient for closing price. Does closing price have a practical significance? Is it statistically significant for predicting competitiveness of auctions? (Use a 10% significance level.)\nAnswer:\nThe coefficient for “close price” is 0.084431. This positive coefficient means that higher closing prices are associated with a higher probability of the auction being competitive.\n\n# odds coefficient for close price\nmath.exp(0.084431)\n\n1.088097762896088\n\n\nLooking at the odds coefficient (1.1), we see that every $1 increase in the closing price increases the odds of the auction being competitive by a factor of 1.1. We will not use ClosePrice in the remaining questions since closing price won’t be available at the time making predictions.\n10.4.e. Use stepwise regression as described in chapter 6.4 to find the model with the best fit to the training data (highest accuracy). Which predictors are used?\n\ndef train_model(variables):\n    if len(variables) == 0:\n        return None\n    model = LogisticRegression(penalty=\"l2\", C=1e42, solver='liblinear')\n    \n    model.fit(X_train[list(variables)], y_train)\n    return model\n\ndef score_model(model, variables):\n    if len(variables) == 0:\n        return 0\n    y_pred = model.predict(X_train[list(variables)])\n    # we negate as score is optimized to be as low as possible\n    return -accuracy_score(y_train, y_pred)\n\nallVariables = X_train.columns\nbest_fitting, var_fitting = stepwise_selection(allVariables, train_model, \n                                score_model, verbose=True)\nprint(sorted(var_fitting))\n\n\n# Confusion matrix\ny_pred = best_fitting.predict(X_valid[list(var_fitting)])\nclassificationSummary(y_valid, y_pred, class_names=classes)\n\naccuracies['best fitting'] = [accuracy_score(y_train, best_fitting.predict(X_train[list(var_fitting)])), \n                              accuracy_score(y_valid, y_pred)]\n\nVariables: sellerRating, Duration, OpenPrice, Category_Books, Category_Clothing/Accessories, Category_Coins/Stamps, Category_Collectibles, Category_Computer, Category_Electronics, Category_EverythingElse, Category_Health/Beauty, Category_Home/Garden, Category_Jewelry, Category_Music/Movie/Game, Category_Photography, Category_Pottery/Glass, Category_SportingGoods, Category_Toys/Hobbies, currency_GBP, currency_US, endDay_Sat, endDay_Sun_Fri, endDay_Thu, endDay_Tue, endDay_Wed\nStart: score=0.00, constant\nStep: score=-0.57, add endDay_Sat\nStep: score=-0.60, add endDay_Sun_Fri\nStep: score=-0.61, add Category_Electronics\nStep: score=-0.62, add Category_Toys/Hobbies\nStep: score=-0.63, add Category_Jewelry\nStep: score=-0.63, add Category_Computer\nStep: score=-0.64, add Category_Health/Beauty\nStep: score=-0.67, add OpenPrice\nStep: score=-0.67, add endDay_Tue\nStep: score=-0.67, add sellerRating\nStep: score=-0.67, unchanged None\n['Category_Computer', 'Category_Electronics', 'Category_Health/Beauty', 'Category_Jewelry', 'Category_Toys/Hobbies', 'OpenPrice', 'endDay_Sat', 'endDay_Sun_Fri', 'endDay_Tue', 'sellerRating']\nConfusion Matrix (Accuracy 0.6274)\n\n       Prediction\nActual   0   1\n     0 196 167\n     1 127 299\n\n\n10.4.f. Use stepwise regression to find the model with the highest accuracy on the validation data. Which predictors are used?\nAnswer:\n\ndef score_model(model, variables):\n    if len(variables) == 0:\n        return 0\n    y_pred = model.predict(X_valid[list(variables)])\n    # we negate as score is optimized to be as low as possible\n    return -accuracy_score(y_valid, y_pred)\n\nallVariables = X_train.columns\nbest_predictive, var_predictive = stepwise_selection(allVariables, train_model, \n                                score_model, verbose=True)\nprint(sorted(var_predictive))\n\n# Confusion matrix\ny_pred = best_predictive.predict(X_valid[list(var_predictive)])\nclassificationSummary(y_valid, y_pred, class_names=classes)\n\naccuracies['best predictive'] = [accuracy_score(y_train, best_predictive.predict(X_train[list(var_predictive)])), \n                                 accuracy_score(y_valid, y_pred)]\n\nVariables: sellerRating, Duration, OpenPrice, Category_Books, Category_Clothing/Accessories, Category_Coins/Stamps, Category_Collectibles, Category_Computer, Category_Electronics, Category_EverythingElse, Category_Health/Beauty, Category_Home/Garden, Category_Jewelry, Category_Music/Movie/Game, Category_Photography, Category_Pottery/Glass, Category_SportingGoods, Category_Toys/Hobbies, currency_GBP, currency_US, endDay_Sat, endDay_Sun_Fri, endDay_Thu, endDay_Tue, endDay_Wed\nStart: score=0.00, constant\nStep: score=-0.57, add OpenPrice\nStep: score=-0.58, add Category_Music/Movie/Game\nStep: score=-0.60, add Category_SportingGoods\nStep: score=-0.63, add Category_Electronics\nStep: score=-0.64, add sellerRating\nStep: score=-0.65, add Category_EverythingElse\nStep: score=-0.65, unchanged None\n['Category_Electronics', 'Category_EverythingElse', 'Category_Music/Movie/Game', 'Category_SportingGoods', 'OpenPrice', 'sellerRating']\nConfusion Matrix (Accuracy 0.6489)\n\n       Prediction\nActual   0   1\n     0 163 200\n     1  77 349\n\n\n10.4.g. What is the danger of using the best predictive model that you found?\nAnswer:\n\npd.DataFrame(accuracies, index=['training', 'validation']).transpose()\n\n\n\n\n\n\n\n\ntraining\nvalidation\n\n\n\n\nfull model\n0.655959\n0.631179\n\n\nbest fitting\n0.672866\n0.627376\n\n\nbest predictive\n0.641589\n0.648923\n\n\n\n\n\n\n\nThe model that was optimized for performance on the training set (best-fitting) has the highest accuracy. However, it also has the smallest accuracy on the validation set. One way to evaluate the degree of overfitting is to compare the error rates of the training and validation sets. The large difference in training set error and validation error (where training set error is smaller than validation set) indicates that the best-fitting model may be overfit to the training data.\nThe best-predictive model (optimized for accuracy on the validation set) on the other hand, has a small difference between training and validation accuracy. Here, the accuracy on the validation set is even higher than for the training set. This can be an indication that the model is biased towards the validation set.\nTo avoid bias towards training or validation set, it is best to use cross validation.\n10.4.h. Explain how and why the best-fitting model and the best predictive models are the same or different.\nWhile finding the best-fitted model means fitting the training data set (using measures like accuracy), the best predictive model is one that has highest prediction accuracy with the validation data.\n10.4.i. Use regularized logistic regression with L1 penalty on the training data. Compare its selected predictors and classification performance to the best-fitting and best predictive models.\n\nlogit_reg_L1 = LogisticRegressionCV(cv=5, random_state=0, Cs=20, max_iter=500)\nlogit_reg_L1.fit(X_train, y_train)\n\n# Confusion matrix\nclassificationSummary(logit_reg_L1.predict(X_valid), val_pred, class_names=classes)\n\n\naccuracies['regularized'] = [accuracy_score(y_train, logit_reg_L1.predict(X_train)), \n                             accuracy_score(y_valid, logit_reg_L1.predict(X_valid))]\n\nConfusion Matrix (Accuracy 0.8834)\n\n       Prediction\nActual   0   1\n     0 289  27\n     1  65 408\n\n\n\npd.DataFrame(accuracies, index=['training', 'validation']).transpose()\n\n\n\n\n\n\n\n\ntraining\nvalidation\n\n\n\n\nfull model\n0.655959\n0.631179\n\n\nbest fitting\n0.672866\n0.627376\n\n\nbest predictive\n0.641589\n0.648923\n\n\nregularized\n0.657650\n0.648923\n\n\n\n\n\n\n\nRegularization leads to a model that has good accuracy on both the training and validation set.\n\ncv_result = pd.DataFrame({\n    'Cs': logit_reg_L1.Cs_,\n    'cv_score': np.mean(logit_reg_L1.scores_[1], axis=0),\n    \n})\ncv_result.plot.line(x='Cs', y='cv_score', logx=True)\n\n\n\n\n\n\n\n\n\n# print coefficients\nprint('intercept ', logit_reg.intercept_[0])\ncoefficients = pd.DataFrame({\n    'unregularized': logit_reg.coef_[0], \n    'best_fitting': 0,\n    'best_predictive': 0,\n    'L1 regularized': logit_reg_L1.coef_[0],\n    }, index=X_train.columns)\ncoefficients.loc[var_fitting, 'best_fitting'] = best_fitting.coef_.ravel()\ncoefficients.loc[var_predictive, 'best_predictive'] = best_predictive.coef_.ravel()\ncoefficients\n\nintercept  1.239216727766818\n\n\n\n\n\n\n\n\n\nunregularized\nbest_fitting\nbest_predictive\nL1 regularized\n\n\n\n\nsellerRating\n-0.000040\n-0.000012\n-0.000020\n-0.000036\n\n\nDuration\n-0.080785\n0.000000\n0.000000\n-0.085876\n\n\nOpenPrice\n-0.007146\n-0.009041\n-0.009020\n-0.007419\n\n\nCategory_Books\n-0.220607\n0.000000\n0.000000\n-0.437808\n\n\nCategory_Clothing/Accessories\n0.285479\n0.000000\n0.000000\n-0.038999\n\n\nCategory_Coins/Stamps\n-0.815631\n0.000000\n0.000000\n-0.450466\n\n\nCategory_Collectibles\n0.442175\n0.000000\n0.000000\n0.442809\n\n\nCategory_Computer\n0.982360\n0.639226\n0.000000\n0.567203\n\n\nCategory_Electronics\n1.540873\n1.128596\n1.039034\n1.125243\n\n\nCategory_EverythingElse\n-1.887026\n0.000000\n-0.479445\n-0.455946\n\n\nCategory_Health/Beauty\n-1.728272\n-2.077200\n0.000000\n-1.426092\n\n\nCategory_Home/Garden\n0.619485\n0.000000\n0.000000\n0.464107\n\n\nCategory_Jewelry\n-0.301259\n-0.817193\n0.000000\n-0.649345\n\n\nCategory_Music/Movie/Game\n0.503799\n0.000000\n0.586934\n0.386435\n\n\nCategory_Photography\n1.282746\n0.000000\n0.000000\n0.236325\n\n\nCategory_Pottery/Glass\n-0.332043\n0.000000\n0.000000\n-0.168076\n\n\nCategory_SportingGoods\n1.358508\n0.000000\n1.173969\n1.271161\n\n\nCategory_Toys/Hobbies\n0.648472\n0.143210\n0.000000\n0.608254\n\n\ncurrency_GBP\n0.958213\n0.000000\n0.000000\n0.244927\n\n\ncurrency_US\n-0.095301\n0.000000\n0.000000\n-0.048313\n\n\nendDay_Sat\n-1.239433\n-1.033639\n0.000000\n-1.201499\n\n\nendDay_Sun_Fri\n-0.954428\n-0.749508\n0.000000\n-0.791568\n\n\nendDay_Thu\n-1.012888\n0.000000\n0.000000\n-0.186941\n\n\nendDay_Tue\n-0.740699\n-0.599479\n0.000000\n-0.445839\n\n\nendDay_Wed\n-0.915220\n0.000000\n0.000000\n-0.470326\n\n\n\n\n\n\n\nIn general, we can see that for cases where both the best_fitting and the best_predictive model exclude a predictor, the coefficient in the regularized model is also closer to zero compared to the unregularized model.\n10.4.j. If the major objective is accurate classification, what cutoff value should be used?\n\n# Determine accuracy for different cutoff values between 0 and 1\ndata = []\nfor cutoff in [v / 1000 for v in range(1001)]:\n    data.append({'cutoff': cutoff, \n                 'accuracy': accuracy_score(y_valid, logit_reg_L1.predict_proba(X_valid)[:, 1] &gt; cutoff)})\ndata = pd.DataFrame(data)\nax = data.plot.line(x='cutoff', y='accuracy')\nax.axvline(x=0.5)\n\ndata.iloc[data['accuracy'].argmax() - 1:data['accuracy'].argmax() + 2, :]\n\n\n\n\n\n\n\n\ncutoff\naccuracy\n\n\n\n\n505\n0.505\n0.651458\n\n\n506\n0.506\n0.655260\n\n\n507\n0.507\n0.653992\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe highest accuracy is achieved with the default cutoff close to 0.5\n10.4.k. Based on these data, what auction settings set by the seller (duration, opening price, ending day, currency) would you recommend as being most likely to lead to a competitive auction?"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/Lecture_Note/chapter2/Chapter2_Note.html#classification",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/Lecture_Note/chapter2/Chapter2_Note.html#classification",
    "title": "[Lecture Note] Chapter2: Overview of the Data Mining Process",
    "section": "Classification",
    "text": "Classification\n\nClassification is the most basic form of data analysis.\n\ne.g., application of a loan, credit card transaction\n\nTo examine data where the classification is\n\nunknown or\nwill occur in the future\n\nSimilar to classification is develop rules"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/Lecture_Note/chapter2/Chapter2_Note.html#prediction",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/Lecture_Note/chapter2/Chapter2_Note.html#prediction",
    "title": "[Lecture Note] Chapter2: Overview of the Data Mining Process",
    "section": "Prediction",
    "text": "Prediction\n\nPrediction is to predict the value of a numerical variable (e.g., amount of purchase) rather than a class (e.g.,purchaser or nonpurchaser).\nthe value of a continuous variable."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/Lecture_Note/chapter2/Chapter2_Note.html#assiciation-rules-and-recommendation-systems",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/Lecture_Note/chapter2/Chapter2_Note.html#assiciation-rules-and-recommendation-systems",
    "title": "[Lecture Note] Chapter2: Overview of the Data Mining Process",
    "section": "Assiciation Rules and Recommendation Systems",
    "text": "Assiciation Rules and Recommendation Systems\n\nWhat goes with what\nAssociation rules, or affinity analysis, is designed to find such general associations patterns between items in large databases.\n\ngrocery stores: product placement, weekly promotional offers, bundling products.\nhospital database: which symptom is followed by what other symptom to help predict future symptoms for returning patients.\nOnline recommendation systems: collaborative filtering\n\n\n\n\n\n\n\n\nCollaborative filtering\n\n\n\nCollaborative filteringa method that uses individual users’ preferences and tastes given their historic purchase, rating, browsing, or any other measurable behavior indicative of preference, as well as other users’ history.\n\n\n\n\n\n\n\n\nAssociation rules vs Collaborative filtering\n\n\n\nIn contrast to association rules that generate rules general to an entire population, collaborative filtering generates “what goes with what” at the individual user level. Hence, collaborative filtering is used in many recommendation systems that aim to deliver personalized recommendations to users with a wide range of preferences."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/Lecture_Note/chapter2/Chapter2_Note.html#predictive-analytics",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/Lecture_Note/chapter2/Chapter2_Note.html#predictive-analytics",
    "title": "[Lecture Note] Chapter2: Overview of the Data Mining Process",
    "section": "Predictive Analytics",
    "text": "Predictive Analytics\n\nClassification, prediction, and to some extent, association rules and collaborative filtering constitute the analytical methods employed in predictive analytics.\nThe term predictive analytics is sometimes used to also include data pattern identification methods such as clustering."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/Lecture_Note/chapter2/Chapter2_Note.html#data-reduction-and-dimension-reduction",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/Lecture_Note/chapter2/Chapter2_Note.html#data-reduction-and-dimension-reduction",
    "title": "[Lecture Note] Chapter2: Overview of the Data Mining Process",
    "section": "Data Reduction and Dimension Reduction",
    "text": "Data Reduction and Dimension Reduction\n\nData mining algorithms is often improved\n\nwhen the number of variables is limited, and\nwhen large numbers of records can be grouped into homogeneous groups.\n\nFor example,\n\nrather than dealing with thousands of product types, an analyst might wish to group them into a smaller number of groups and build separate models for each group.\nOr a marketer might want to classify customers into different “personas,” and must therefore group customers into homogeneous groups to define the personas.\n\nThis process of consolidating a large number of records (or cases) into a smaller set is termed data reduction. Methods for reducing the number of cases are often called clustering.\nReducing the number of variables is typically called dimension reduction.\n\nDimension reduction is a common initial step before deploying data mining methods, intended to improve predictive power, manageability, and inter-pretability."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/Lecture_Note/chapter2/Chapter2_Note.html#data-exploration-and-visualization",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/Lecture_Note/chapter2/Chapter2_Note.html#data-exploration-and-visualization",
    "title": "[Lecture Note] Chapter2: Overview of the Data Mining Process",
    "section": "Data Exploration and Visualization",
    "text": "Data Exploration and Visualization\n\nExploration is aimed at\n\nunderstanding the global landscape of the data, and\ndetecting unusual values.\n\nExploration is used for data cleaning and manipulation as well as for visual discovery and hypothesis generation.\nMethods for exploring data include looking at various data aggregations and summaries\n(Both numerically and graphically) looking at each variable separately as well as looking at relationships among variables.\nThe purpose is to discover patterns and exceptions.\nData Visualization or Visual Analytics - Exploration by creating charts and dashboards\nFor numerical variables,\n\nwe use histograms and boxplots to learn about the distribution of their values,\n\nto detect outliers (extreme observations), and to\nfind other information that is relevant to the analysis task.\n\n\nFor categorical variables,\n\nwe use bar charts. We can also look at scatter plots of pairs of numerical variables\n\nto learn about possible relationships,\nthe type of relationship,\nto detect outliers.\n\n\nVisualization can be greatly enhanced by adding features such as color and interactive navigation."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/Lecture_Note/chapter2/Chapter2_Note.html#supervised-and-unsupervised-learning",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/Lecture_Note/chapter2/Chapter2_Note.html#supervised-and-unsupervised-learning",
    "title": "[Lecture Note] Chapter2: Overview of the Data Mining Process",
    "section": "Supervised and Unsupervised Learning",
    "text": "Supervised and Unsupervised Learning\n\nA fundamental distinction among data mining techniques is between supervised and unsupervised methods.\n\n\nSupervised Learning Algorithms\n\nSupervised learning algorithms are those used in classification and prediction.\n\nWe must have data available in which the value of the outcome of interest (e.g., purchase or no purchase) is known, “labeled data”\nThese training data are the data from which the classification or prediction algorithm “learns,” or is “trained,” about the relationship between predictor variables and the outcome variable.\nOnce the algorithm has learned from the training data, it is then applied to another sample of labeled data (the validation data) where the outcome is known but initially hidden, to see how well it does in comparison to other models.\nIf many different models are being tried out, it is prudent to save a third sample, which also includes known outcomes (the test data) to use with the model finally selected to predict how well it will do.\nThe model can then be used to classify or predict the outcome of interest in new cases where the outcome is unknown.\n\n\n\n\n\n\n\n\nLinear Regression as Supervised Machine Learning Algorithm\n\n\n\n\nSimple linear regression is an example of a supervised learning algorithm (although rarely called that in the introductory statistics course where you probably first encountered it).\n\nThe Y variable is the (known) outcome variable and the X variable is a predictor variable.\nA regression line is drawn to minimize the sum of squared deviations between the actual Y values and the values predicted by this line.\nThe regression line can now be used to predict Y values for new values of X for which we do not know the Y value.\n\n\n\n\n\n\nUnsupervised Learning Algorithms\n\nUnsupervised learning algorithms are those used where there is no outcome variable to predict or classify.\nHence, there is no “learning” from cases where such an outcome variable is known.\nAssociation rules, dimension reduction methods, and clustering techniques are all unsupervised learning methods.\nSupervised and unsupervised methods are sometimes used in conjunction.\n\n\n\n\n\n\n\nNote\n\n\n\nFor example, unsupervised clustering methods are used to separate loan applicants into several risk-level groups. Then, supervised algorithms are applied separately to each risk-level group for predicting propensity of loan default."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/Lecture_Note/chapter2/Chapter2_Note.html#data-mining-steps",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/Lecture_Note/chapter2/Chapter2_Note.html#data-mining-steps",
    "title": "[Lecture Note] Chapter2: Overview of the Data Mining Process",
    "section": "Data Mining Steps",
    "text": "Data Mining Steps\n\nDevelop an understanding of the purpose of the data mining project. How will the stakeholder use the results? Who will be affected by the results? Will the analysis be a one-shot effort or an ongoing procedure?\nObtain the dataset to be used in the analysis. This often involves sampling from a large database to capture records to be used in an analysis. How well this sample reflects the records of interest affects the ability of the data mining results to generalize to records outside of this sample. It may also involve pulling together data from different databases or sources The databases could be internal (e.g., past purchases made by customers) or external (credit ratings). While data mining deals with very large databases, usually the analysis to be done requires only thousands or tens of thousands of records.\nExplore, clean, and preprocess the data. This step involves verifying that the data are in reasonable condition. How should missing data be handled? Are the values in a reasonable range, given what you would expect for each variable? Are there obvious outliers? The data are reviewed graphically: for example, a matrix of scatterplots showing the relationship of each variable with every other variable. We also need to ensure consistency in the definitions of fields, units of measurement, time periods, and so on. In this step, new variables are also typically created from existing ones. For example, “duration” can be computed from start and end dates.\nReduce the data dimension, if necessary. Dimension reduction can involve operations such as eliminating unneeded variables, transforming variables (e.g., turning “money spent” into “spent &gt; $100” vs. “spent ≤ $100”), and creating new variables (e.g., a variable that records whether at least one of several products was purchased). Make sure that you know what each variable means and whether it is sensible to include it in the model.\nDetermine the data mining task. (classification, prediction, clustering, etc.). This involves translating the general question or problem of Step 1 into a more specific data mining question.\nPartition the data (for supervised tasks). If the task is supervised (classification or prediction), randomly partition the dataset into three parts: training, validation, and test datasets.\nChoose the data mining techniques to be used. (regression, neural nets, hierarchical clustering, etc.).\nUse algorithms to perform the task. This is typically an iterative process—trying multiple variants, and often using multiple variants of the same algorithm (choosing different variables or settings within the algorithm). Where appropriate, feedback from the algorithm’s performance on validation data is used to refine the settings.\nInterpret the results of the algorithms. This involves making a choice as to the best algorithm to deploy, and where possible, testing the final choice on the test data to get an idea as to how well it will perform. (Recall that each algorithm may also be tested on the validation data for tuning purposes; in this way, the validation data become a part of the fitting process and are likely to underestimate the error in the deployment of the model that is finally chosen.)\nDeploy the model. This step involves integrating the model into operational systems and running it on real records to produce decisions or actions. For example, the model might be applied to a purchased list of possible customers, and the action might be “include in the mailing if the predicted amount of purchase is &gt; $10.” A key step here is “scoring” the new records, or using the chosen model to predict the outcome value (“score”) for each new record."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/Lecture_Note/chapter2/Chapter2_Note.html#semma",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/Lecture_Note/chapter2/Chapter2_Note.html#semma",
    "title": "[Lecture Note] Chapter2: Overview of the Data Mining Process",
    "section": "SEMMA",
    "text": "SEMMA\nThe foregoing steps encompass the steps in SEMMA, a methodology developed by the software company SAS:\n\nSample: Take a sample from the dataset; partition into training, validation, and test datasets.\nExplore: Examine the dataset statistically and graphically.\nModify: Transform the variables and impute missing values.\nModel: Fit predictive models (e.g., regression tree, neural network).\nAssess: Compare models using a validation dataset.\n\n\nIBM SPSS Modeler (previously SPSS-Clementine) has a similar methodology, termed CRISP-DM (CRoss-Industry Standard Process for Data Mining). All these frameworks include the same main steps involved in predictive modeling."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/Lecture_Note/chapter2/Chapter2_Note.html#other-models",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/Lecture_Note/chapter2/Chapter2_Note.html#other-models",
    "title": "[Lecture Note] Chapter2: Overview of the Data Mining Process",
    "section": "Other Models",
    "text": "Other Models\n\nKDD Model: Knowledge Discovery in Databases (KDD) is a systematic process that seeks to identify valid, novel, potentially useful, and ultimately understandable patterns from large amounts of data. In simpler terms, it’s about transforming raw data into valuable knowledge.\nCRISP-DM: CRISP-DM stands for Cross-Industry Standard Process for Data Mining. It is a cyclical process that provides a structured approach to planning, organizing, and implementing a data mining project. The process consists of six major phases: Business Understanding, Data Understanding, Data, Preparation, Modeling, Evaluation, Deployment"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/Lecture_Note/chapter2/Chapter2_Note.html#organization-of-datasets",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/Lecture_Note/chapter2/Chapter2_Note.html#organization-of-datasets",
    "title": "[Lecture Note] Chapter2: Overview of the Data Mining Process",
    "section": "Organization of Datasets",
    "text": "Organization of Datasets"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/Lecture_Note/chapter2/Chapter2_Note.html#predicting-home-values-in-the-west-roxbury-neighborhood",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/Lecture_Note/chapter2/Chapter2_Note.html#predicting-home-values-in-the-west-roxbury-neighborhood",
    "title": "[Lecture Note] Chapter2: Overview of the Data Mining Process",
    "section": "Predicting Home Values in the West Roxbury Neighborhood",
    "text": "Predicting Home Values in the West Roxbury Neighborhood"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/Lecture_Note/chapter2/Chapter2_Note.html#loading-and-looking-at-the-data-in-r",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/Lecture_Note/chapter2/Chapter2_Note.html#loading-and-looking-at-the-data-in-r",
    "title": "[Lecture Note] Chapter2: Overview of the Data Mining Process",
    "section": "Loading and Looking at the Data in R",
    "text": "Loading and Looking at the Data in R"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/Lecture_Note/chapter2/Chapter2_Note.html#sampling-from-a-database",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/Lecture_Note/chapter2/Chapter2_Note.html#sampling-from-a-database",
    "title": "[Lecture Note] Chapter2: Overview of the Data Mining Process",
    "section": "Sampling from a Database",
    "text": "Sampling from a Database"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/Lecture_Note/chapter2/Chapter2_Note.html#oversampling-rare-events-in-classification-tasks",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/Lecture_Note/chapter2/Chapter2_Note.html#oversampling-rare-events-in-classification-tasks",
    "title": "[Lecture Note] Chapter2: Overview of the Data Mining Process",
    "section": "Oversampling Rare Events in Classification Tasks",
    "text": "Oversampling Rare Events in Classification Tasks"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/Lecture_Note/chapter2/Chapter2_Note.html#preprocessing-and-cleaning-the-data",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/Lecture_Note/chapter2/Chapter2_Note.html#preprocessing-and-cleaning-the-data",
    "title": "[Lecture Note] Chapter2: Overview of the Data Mining Process",
    "section": "Preprocessing and Cleaning the Data",
    "text": "Preprocessing and Cleaning the Data"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/Lecture_Note/chapter2/Chapter2_Note.html#overfitting",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/Lecture_Note/chapter2/Chapter2_Note.html#overfitting",
    "title": "[Lecture Note] Chapter2: Overview of the Data Mining Process",
    "section": "Overfitting",
    "text": "Overfitting"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/Lecture_Note/chapter2/Chapter2_Note.html#creation-and-use-of-data-partitions",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/Lecture_Note/chapter2/Chapter2_Note.html#creation-and-use-of-data-partitions",
    "title": "[Lecture Note] Chapter2: Overview of the Data Mining Process",
    "section": "Creation and Use of Data Partitions",
    "text": "Creation and Use of Data Partitions"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/Lecture_Note/chapter2/Chapter2_Note.html#more-to-read",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/Lecture_Note/chapter2/Chapter2_Note.html#more-to-read",
    "title": "[Lecture Note] Chapter2: Overview of the Data Mining Process",
    "section": "More to Read",
    "text": "More to Read"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/Lecture_Note/chapter2/Chapter2_Note.html#assignment",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/Lecture_Note/chapter2/Chapter2_Note.html#assignment",
    "title": "[Lecture Note] Chapter2: Overview of the Data Mining Process",
    "section": "Assignment",
    "text": "Assignment\n\nWhat to do\nRequirement\n\nPDF format\nfile name should be include your student id and name\n\nstuID_name_title.pdf (e.g. 1111111_ChungilChae_SelfIntroduction.pdf)\n\n\nDue date\n\nby DATE 11:59PM\nNO LATE SUBMISSION ALLOWED!!!!"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#short-video-introduction",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#short-video-introduction",
    "title": "CHAPTER 1 Introduction",
    "section": "SHORT VIDEO INTRODUCTION",
    "text": "SHORT VIDEO INTRODUCTION"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#business-analytics",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#business-analytics",
    "title": "CHAPTER 1 Introduction",
    "section": "Business Analytics",
    "text": "Business Analytics\n\nBusiness Analytics (BA) involves using quantitative data to aid decision-making, with its applications and interpretations varying across different organizations.\n\nFor example, a British tabloid once utilized it to test which images on their website, like cats or dogs, garnered more views.\nIn contrast, the Washington Post uses analytics to target specific influential audiences, such as defense contractors, by tracking reader behaviors like time of day and subscription details."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#who-uses-predictive-analytics",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#who-uses-predictive-analytics",
    "title": "CHAPTER 1 Introduction",
    "section": "Who Uses Predictive Analytics?",
    "text": "Who Uses Predictive Analytics?\n\nThe integration of predictive analytics into various sectors has significantly enhanced organizational capabilities due to the growing availability of data. Key examples include:\n\n\nCredit Scoring:\n\nCredit scoring utilizes predictive modeling to assess an individual’s likelihood of repaying debts. Rather than being an arbitrary measure, it derives from historical data analysis to forecast future repayment behaviors. This established method helps financial institutions determine creditworthiness efficiently."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#data-mining-is",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#data-mining-is",
    "title": "CHAPTER 1 Introduction",
    "section": "Data mining is",
    "text": "Data mining is\n\nIn the context of this book, data mining extends beyond simple counting, descriptive statistics, and rule-based methods, venturing into more sophisticated realms of business analytics.\nWhile data visualization serves as an introductory tool in advanced analytics, the primary focus of the book is on deeper, more complex data analytics tools.\nThese include both statistical and machine-learning techniques designed to automate and enhance decision-making processes, with a strong emphasis on prediction at an individual level."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#overview",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#overview",
    "title": "CHAPTER 1 Introduction",
    "section": "Overview",
    "text": "Overview\n\nThe analytics field is expanding rapidly in both the scope of its applications and the prevalence of its use across organizations. This growth has led to considerable inconsistencies and overlaps in terminology.\nFor example, “data mining” is defined differently by different groups:\n\nthe general public may view it as an invasive search of personal data;\na major consulting firm might focus on identifying trends in historical data under a “data mining department,” while relegating more advanced predictive modeling to an “advanced analytics department.”"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#data-mining-and-big-data",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#data-mining-and-big-data",
    "title": "CHAPTER 1 Introduction",
    "section": "Data mining and Big Data",
    "text": "Data mining and Big Data\n\nData mining and Big Data are inextricably linked, with the landscape of Big Data defined by the “four V’s”: volume, velocity, variety, and veracity. These characteristics outline the challenges and opportunities presented by modern data sets:\n\nVolume addresses the sheer amount of data.\nVelocity pertains to the rapid rate at which data is generated and updated.\nVariety indicates the different types of data being collected, from numerical to textual and beyond.\nVeracity highlights the unreliability and organic nature of data generation processes, which often lack the rigorous controls found in traditional data collection."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#data-science-and-data-engineering",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#data-science-and-data-engineering",
    "title": "CHAPTER 1 Introduction",
    "section": "Data Science and Data Engineering",
    "text": "Data Science and Data Engineering"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#overview-1",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#overview-1",
    "title": "CHAPTER 1 Introduction",
    "section": "Overview",
    "text": "Overview\n\nAs explored in this book, and indeed across the broader literature on data mining, there is a plethora of methods available for prediction and classification.\nThe coexistence of these diverse methods often raises questions regarding their relative merits and applicabilities.\nThe reason for this variety is that each method comes with its own set of strengths and weaknesses, making them suitable for different scenarios depending on several factors.\nThese factors include the size of the dataset, the types of patterns present in the data, adherence to the assumptions underlying each method, the level of noise in the data, and the specific objectives of the analysis."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#terms",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#terms",
    "title": "CHAPTER 1 Introduction",
    "section": "Terms",
    "text": "Terms\n\nDue to the hybrid origins and interdisciplinary nature of data mining, the terminology used by its practitioners often varies depending on their background in fields like machine learning (artificial intelligence) or statistics. Here’s a list of commonly used terms in data mining, along with descriptions of how they might be referred to in different fields:\n\n\nAlgorithm : A specific procedure for implementing a data mining technique, such as a classification tree or discriminant analysis. Attribute : Also referred to as Predictor."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#order-of-topics",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#order-of-topics",
    "title": "CHAPTER 1 Introduction",
    "section": "Order of Topics",
    "text": "Order of Topics\nThe book is structured into eight parts, each focusing on distinct aspects and applications of data mining:\n\nPart I (Chapters 1–2): This section provides a broad introduction to data mining, outlining its key components and the fundamental concepts underpinning the field. It serves as the foundational groundwork for the more detailed explorations that follow.\nPart II (Chapters 3–4): Here, the focus shifts to the preliminary stages of data analysis, specifically on data exploration and dimension reduction. These chapters help readers understand how to streamline complex datasets into more manageable and interpretable forms."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#more-to-read",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#more-to-read",
    "title": "CHAPTER 1 Introduction",
    "section": "More to Read",
    "text": "More to Read"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#assignment",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#assignment",
    "title": "CHAPTER 1 Introduction",
    "section": "Assignment",
    "text": "Assignment\n\n\n\nWhat to do\n\n\n\nRequirement\n\nPDF format\nfile name should be include your student id and name\n\nstuID_name_title.pdf (e.g. 1111111_ChungilChae_SelfIntroduction.pdf)\n\n\nDue date\n\nby DATE 11:59PM\nNO LATE SUBMISSION ALLOWED!!!!"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter7/chapter7.html#short-video-introduction",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter7/chapter7.html#short-video-introduction",
    "title": "TITLE",
    "section": "SHORT VIDEO INTRODUCTION",
    "text": "SHORT VIDEO INTRODUCTION"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter7/chapter7.html#a-1",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter7/chapter7.html#a-1",
    "title": "TITLE",
    "section": "a",
    "text": "a\na"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter7/chapter7.html#more-to-read",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter7/chapter7.html#more-to-read",
    "title": "TITLE",
    "section": "More to Read",
    "text": "More to Read"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter7/chapter7.html#assignment",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter7/chapter7.html#assignment",
    "title": "TITLE",
    "section": "Assignment",
    "text": "Assignment\n\n\n\nWhat to do\n\n\n\nRequirement\n\nPDF format\nfile name should be include your student id and name\n\nstuID_name_title.pdf (e.g. 1111111_ChungilChae_SelfIntroduction.pdf)\n\n\nDue date\n\nby DATE 11:59PM\nNO LATE SUBMISSION ALLOWED!!!!"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter9/chapter9.html#short-video-introduction",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter9/chapter9.html#short-video-introduction",
    "title": "TITLE",
    "section": "SHORT VIDEO INTRODUCTION",
    "text": "SHORT VIDEO INTRODUCTION"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter9/chapter9.html#a-1",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter9/chapter9.html#a-1",
    "title": "TITLE",
    "section": "a",
    "text": "a\na"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter9/chapter9.html#more-to-read",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter9/chapter9.html#more-to-read",
    "title": "TITLE",
    "section": "More to Read",
    "text": "More to Read"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter9/chapter9.html#assignment",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter9/chapter9.html#assignment",
    "title": "TITLE",
    "section": "Assignment",
    "text": "Assignment\n\n\n\nWhat to do\n\n\n\nRequirement\n\nPDF format\nfile name should be include your student id and name\n\nstuID_name_title.pdf (e.g. 1111111_ChungilChae_SelfIntroduction.pdf)\n\n\nDue date\n\nby DATE 11:59PM\nNO LATE SUBMISSION ALLOWED!!!!"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter10/chapter10.html#short-video-introduction",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter10/chapter10.html#short-video-introduction",
    "title": "TITLE",
    "section": "SHORT VIDEO INTRODUCTION",
    "text": "SHORT VIDEO INTRODUCTION"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter10/chapter10.html#a-1",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter10/chapter10.html#a-1",
    "title": "TITLE",
    "section": "a",
    "text": "a\na"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter10/chapter10.html#more-to-read",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter10/chapter10.html#more-to-read",
    "title": "TITLE",
    "section": "More to Read",
    "text": "More to Read"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter10/chapter10.html#assignment",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter10/chapter10.html#assignment",
    "title": "TITLE",
    "section": "Assignment",
    "text": "Assignment\n\n\n\nWhat to do\n\n\n\nRequirement\n\nPDF format\nfile name should be include your student id and name\n\nstuID_name_title.pdf (e.g. 1111111_ChungilChae_SelfIntroduction.pdf)\n\n\nDue date\n\nby DATE 11:59PM\nNO LATE SUBMISSION ALLOWED!!!!"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter21/chapter21.html#short-video-introduction",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter21/chapter21.html#short-video-introduction",
    "title": "TITLE",
    "section": "SHORT VIDEO INTRODUCTION",
    "text": "SHORT VIDEO INTRODUCTION"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter21/chapter21.html#a-1",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter21/chapter21.html#a-1",
    "title": "TITLE",
    "section": "a",
    "text": "a\na"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter21/chapter21.html#more-to-read",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter21/chapter21.html#more-to-read",
    "title": "TITLE",
    "section": "More to Read",
    "text": "More to Read"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter21/chapter21.html#assignment",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter21/chapter21.html#assignment",
    "title": "TITLE",
    "section": "Assignment",
    "text": "Assignment\n\n\n\nWhat to do\n\n\n\nRequirement\n\nPDF format\nfile name should be include your student id and name\n\nstuID_name_title.pdf (e.g. 1111111_ChungilChae_SelfIntroduction.pdf)\n\n\nDue date\n\nby DATE 11:59PM\nNO LATE SUBMISSION ALLOWED!!!!"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter18/chapter18.html#short-video-introduction",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter18/chapter18.html#short-video-introduction",
    "title": "TITLE",
    "section": "SHORT VIDEO INTRODUCTION",
    "text": "SHORT VIDEO INTRODUCTION"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter18/chapter18.html#a-1",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter18/chapter18.html#a-1",
    "title": "TITLE",
    "section": "a",
    "text": "a\na"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter18/chapter18.html#more-to-read",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter18/chapter18.html#more-to-read",
    "title": "TITLE",
    "section": "More to Read",
    "text": "More to Read"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter18/chapter18.html#assignment",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter18/chapter18.html#assignment",
    "title": "TITLE",
    "section": "Assignment",
    "text": "Assignment\n\n\n\nWhat to do\n\n\n\nRequirement\n\nPDF format\nfile name should be include your student id and name\n\nstuID_name_title.pdf (e.g. 1111111_ChungilChae_SelfIntroduction.pdf)\n\n\nDue date\n\nby DATE 11:59PM\nNO LATE SUBMISSION ALLOWED!!!!"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter16/chapter16.html#short-video-introduction",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter16/chapter16.html#short-video-introduction",
    "title": "TITLE",
    "section": "SHORT VIDEO INTRODUCTION",
    "text": "SHORT VIDEO INTRODUCTION"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter16/chapter16.html#a-1",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter16/chapter16.html#a-1",
    "title": "TITLE",
    "section": "a",
    "text": "a\na"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter16/chapter16.html#more-to-read",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter16/chapter16.html#more-to-read",
    "title": "TITLE",
    "section": "More to Read",
    "text": "More to Read"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter16/chapter16.html#assignment",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter16/chapter16.html#assignment",
    "title": "TITLE",
    "section": "Assignment",
    "text": "Assignment\n\n\n\nWhat to do\n\n\n\nRequirement\n\nPDF format\nfile name should be include your student id and name\n\nstuID_name_title.pdf (e.g. 1111111_ChungilChae_SelfIntroduction.pdf)\n\n\nDue date\n\nby DATE 11:59PM\nNO LATE SUBMISSION ALLOWED!!!!"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#short-video-introduction",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#short-video-introduction",
    "title": "Chapter2: Overview of the Data Mining Process",
    "section": "SHORT VIDEO INTRODUCTION",
    "text": "SHORT VIDEO INTRODUCTION"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#classification",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#classification",
    "title": "Chapter2: Overview of the Data Mining Process",
    "section": "Classification",
    "text": "Classification\n\nClassification is the most basic form of data analysis.\n\ne.g., application of a loan, credit card transaction\n\nTo examine data where the classification is\n\nunknown or\nwill occur in the future\n\nSimilar to classification is develop rules"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#prediction",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#prediction",
    "title": "Chapter2: Overview of the Data Mining Process",
    "section": "Prediction",
    "text": "Prediction\n\nPrediction is to predict the value of a numerical variable (e.g., amount of purchase) rather than a class (e.g.,purchaser or nonpurchaser).\nthe value of a continuous variable."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#assiciation-rules-and-recommendation-systems",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#assiciation-rules-and-recommendation-systems",
    "title": "Chapter2: Overview of the Data Mining Process",
    "section": "Assiciation Rules and Recommendation Systems",
    "text": "Assiciation Rules and Recommendation Systems\n\n\nWhat goes with what\nAssociation rules, or affinity analysis, is designed to find such general associations patterns between items in large databases.\n\ngrocery stores: product placement, weekly promotional offers, bundling products.\nhospital database: which symptom is followed by what other symptom to help predict future symptoms for returning patients.\nOnline recommendation systems: collaborative filtering\n\n\n\n\n\n\n\n\nCollaborative filtering\n\n\nCollaborative filteringa method that uses individual users’ preferences and tastes given their historic purchase, rating, browsing, or any other measurable behavior indicative of preference, as well as other users’ history.\n\n\n\n\n\n\n\n\n\nAssociation rules vs Collaborative filtering\n\n\nIn contrast to association rules that generate rules general to an entire population, collaborative filtering generates “what goes with what” at the individual user level. Hence, collaborative filtering is used in many recommendation systems that aim to deliver personalized recommendations to users with a wide range of preferences."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#predictive-analytics",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#predictive-analytics",
    "title": "Chapter2: Overview of the Data Mining Process",
    "section": "Predictive Analytics",
    "text": "Predictive Analytics\n\nClassification, prediction, and to some extent, association rules and collaborative filtering constitute the analytical methods employed in predictive analytics.\nThe term predictive analytics is sometimes used to also include data pattern identification methods such as clustering."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#data-reduction-and-dimension-reduction",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#data-reduction-and-dimension-reduction",
    "title": "Chapter2: Overview of the Data Mining Process",
    "section": "Data Reduction and Dimension Reduction",
    "text": "Data Reduction and Dimension Reduction\n\n\nData mining algorithms is often improved\n\nwhen the number of variables is limited, and\nwhen large numbers of records can be grouped into homogeneous groups.\n\nFor example,\n\nrather than dealing with thousands of product types, an analyst might wish to group them into a smaller number of groups and build separate models for each group.\nOr a marketer might want to classify customers into different “personas,” and must therefore group customers into homogeneous groups to define the personas.\n\nThis process of consolidating a large number of records (or cases) into a smaller set is termed data reduction. Methods for reducing the number of cases are often called clustering.\nReducing the number of variables is typically called dimension reduction.\n\nDimension reduction is a common initial step before deploying data mining methods, intended to improve predictive power, manageability, and inter-pretability."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#data-exploration-and-visualization",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#data-exploration-and-visualization",
    "title": "Chapter2: Overview of the Data Mining Process",
    "section": "Data Exploration and Visualization",
    "text": "Data Exploration and Visualization\n\n\nExploration is aimed at\n\nunderstanding the global landscape of the data, and\ndetecting unusual values.\n\nExploration is used for data cleaning and manipulation as well as for visual discovery and hypothesis generation.\nMethods for exploring data include looking at various data aggregations and summaries\n(Both numerically and graphically) looking at each variable separately as well as looking at relationships among variables.\nThe purpose is to discover patterns and exceptions.\nData Visualization or Visual Analytics - Exploration by creating charts and dashboards"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#supervised-and-unsupervised-learning",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#supervised-and-unsupervised-learning",
    "title": "Chapter2: Overview of the Data Mining Process",
    "section": "Supervised and Unsupervised Learning",
    "text": "Supervised and Unsupervised Learning"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#data-mining-steps",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#data-mining-steps",
    "title": "Chapter2: Overview of the Data Mining Process",
    "section": "Data Mining Steps",
    "text": "Data Mining Steps\n\n\nDevelop an understanding of the purpose of the data mining project.\nObtain the dataset to be used in the analysis.\nExplore, clean, and preprocess the data.\nReduce the data dimension, if necessary.\nDetermine the data mining task.\nPartition the data (for supervised tasks).\nChoose the data mining techniques to be used.\nUse algorithms to perform the task.\nInterpret the results of the algorithms.\nDeploy the model.\n\n\n\n\nDevelop an understanding of the purpose of the data mining project. How will the stakeholder use the results? Who will be affected by the results? Will the analysis be a one-shot effort or an ongoing procedure?\nObtain the dataset to be used in the analysis. This often involves sampling from a large database to capture records to be used in an analysis. How well this sample reflects the records of interest affects the ability of the data mining results to generalize to records outside of this sample. It may also involve pulling together data from different databases or sources The databases could be internal (e.g., past purchases made by customers) or external (credit ratings). While data mining deals with very large databases, usually the analysis to be done requires only thousands or tens of thousands of records.\nExplore, clean, and preprocess the data. This step involves verifying that the data are in reasonable condition. How should missing data be handled? Are the values in a reasonable range, given what you would expect for each variable? Are there obvious outliers? The data are reviewed graphically: for example, a matrix of scatterplots showing the relationship of each variable with every other variable. We also need to ensure consistency in the definitions of fields, units of measurement, time periods, and so on. In this step, new variables are also typically created from existing ones. For example, “duration” can be computed from start and end dates.\nReduce the data dimension, if necessary. Dimension reduction can involve operations such as eliminating unneeded variables, transforming variables (e.g., turning “money spent” into “spent &gt; $100” vs. “spent ≤ $100”), and creating new variables (e.g., a variable that records whether at least one of several products was purchased). Make sure that you know what each variable means and whether it is sensible to include it in the model.\nDetermine the data mining task. (classification, prediction, clustering, etc.). This involves translating the general question or problem of Step 1 into a more specific data mining question.\nPartition the data (for supervised tasks). If the task is supervised (classification or prediction), randomly partition the dataset into three parts: training, validation, and test datasets.\nChoose the data mining techniques to be used. (regression, neural nets, hierarchical clustering, etc.).\nUse algorithms to perform the task. This is typically an iterative process—trying multiple variants, and often using multiple variants of the same algorithm (choosing different variables or settings within the algorithm). Where appropriate, feedback from the algorithm’s performance on validation data is used to refine the settings.\nInterpret the results of the algorithms. This involves making a choice as to the best algorithm to deploy, and where possible, testing the final choice on the test data to get an idea as to how well it will perform. (Recall that each algorithm may also be tested on the validation data for tuning purposes; in this way, the validation data become a part of the fitting process and are likely to underestimate the error in the deployment of the model that is finally chosen.)\nDeploy the model. This step involves integrating the model into operational systems and running it on real records to produce decisions or actions. For example, the model might be applied to a purchased list of possible customers, and the action might be “include in the mailing if the predicted amount of purchase is &gt; $10.” A key step here is “scoring” the new records, or using the chosen model to predict the outcome value (“score”) for each new record."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#semma",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#semma",
    "title": "Chapter2: Overview of the Data Mining Process",
    "section": "SEMMA",
    "text": "SEMMA\n\nThe foregoing steps encompass the steps in SEMMA, a methodology developed by the software company SAS:\n\nSample: Take a sample from the dataset; partition into training, validation, and test datasets.\nExplore: Examine the dataset statistically and graphically.\nModify: Transform the variables and impute missing values.\nModel: Fit predictive models (e.g., regression tree, neural network).\nAssess: Compare models using a validation dataset.\n\n\nIBM SPSS Modeler (previously SPSS-Clementine) has a similar methodology, termed CRISP-DM (CRoss-Industry Standard Process for Data Mining). All these frameworks include the same main steps involved in predictive modeling."
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#other-models",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#other-models",
    "title": "Chapter2: Overview of the Data Mining Process",
    "section": "Other Models",
    "text": "Other Models\n\nKDD Model: Knowledge Discovery in Databases (KDD) is a systematic process that seeks to identify valid, novel, potentially useful, and ultimately understandable patterns from large amounts of data. In simpler terms, it’s about transforming raw data into valuable knowledge.\nCRISP-DM: CRISP-DM stands for Cross-Industry Standard Process for Data Mining. It is a cyclical process that provides a structured approach to planning, organizing, and implementing a data mining project. The process consists of six major phases: Business Understanding, Data Understanding, Data, Preparation, Modeling, Evaluation, Deployment"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#more-to-read",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#more-to-read",
    "title": "Chapter2: Overview of the Data Mining Process",
    "section": "More to Read",
    "text": "More to Read"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#assignment",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#assignment",
    "title": "Chapter2: Overview of the Data Mining Process",
    "section": "Assignment",
    "text": "Assignment\n\n\n\nWhat to do\n\n\n\nRequirement\n\nPDF format\nfile name should be include your student id and name\n\nstuID_name_title.pdf (e.g. 1111111_ChungilChae_SelfIntroduction.pdf)\n\n\nDue date\n\nby DATE 11:59PM\nNO LATE SUBMISSION ALLOWED!!!!"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter4/chapter4.html#short-video-introduction",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter4/chapter4.html#short-video-introduction",
    "title": "TITLE",
    "section": "SHORT VIDEO INTRODUCTION",
    "text": "SHORT VIDEO INTRODUCTION"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter4/chapter4.html#a-1",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter4/chapter4.html#a-1",
    "title": "TITLE",
    "section": "a",
    "text": "a\na"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter4/chapter4.html#more-to-read",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter4/chapter4.html#more-to-read",
    "title": "TITLE",
    "section": "More to Read",
    "text": "More to Read"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter4/chapter4.html#assignment",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter4/chapter4.html#assignment",
    "title": "TITLE",
    "section": "Assignment",
    "text": "Assignment\n\n\n\nWhat to do\n\n\n\nRequirement\n\nPDF format\nfile name should be include your student id and name\n\nstuID_name_title.pdf (e.g. 1111111_ChungilChae_SelfIntroduction.pdf)\n\n\nDue date\n\nby DATE 11:59PM\nNO LATE SUBMISSION ALLOWED!!!!"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter14/chapter14.html#short-video-introduction",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter14/chapter14.html#short-video-introduction",
    "title": "TITLE",
    "section": "SHORT VIDEO INTRODUCTION",
    "text": "SHORT VIDEO INTRODUCTION"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter14/chapter14.html#a-1",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter14/chapter14.html#a-1",
    "title": "TITLE",
    "section": "a",
    "text": "a\na"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter14/chapter14.html#more-to-read",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter14/chapter14.html#more-to-read",
    "title": "TITLE",
    "section": "More to Read",
    "text": "More to Read"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter14/chapter14.html#assignment",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter14/chapter14.html#assignment",
    "title": "TITLE",
    "section": "Assignment",
    "text": "Assignment\n\n\n\nWhat to do\n\n\n\nRequirement\n\nPDF format\nfile name should be include your student id and name\n\nstuID_name_title.pdf (e.g. 1111111_ChungilChae_SelfIntroduction.pdf)\n\n\nDue date\n\nby DATE 11:59PM\nNO LATE SUBMISSION ALLOWED!!!!"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter12/chapter12.html#short-video-introduction",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter12/chapter12.html#short-video-introduction",
    "title": "TITLE",
    "section": "SHORT VIDEO INTRODUCTION",
    "text": "SHORT VIDEO INTRODUCTION"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter12/chapter12.html#a-1",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter12/chapter12.html#a-1",
    "title": "TITLE",
    "section": "a",
    "text": "a\na"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter12/chapter12.html#more-to-read",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter12/chapter12.html#more-to-read",
    "title": "TITLE",
    "section": "More to Read",
    "text": "More to Read"
  },
  {
    "objectID": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter12/chapter12.html#assignment",
    "href": "Course/MGS3701/03_Lecture_Note/LectureNote_Chad/PPT/chapter12/chapter12.html#assignment",
    "title": "TITLE",
    "section": "Assignment",
    "text": "Assignment\n\n\n\nWhat to do\n\n\n\nRequirement\n\nPDF format\nfile name should be include your student id and name\n\nstuID_name_title.pdf (e.g. 1111111_ChungilChae_SelfIntroduction.pdf)\n\n\nDue date\n\nby DATE 11:59PM\nNO LATE SUBMISSION ALLOWED!!!!"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 03 - Data visualization.html",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 03 - Data visualization.html",
    "title": "Chapter 3: Data Visualization",
    "section": "",
    "text": "2019 Galit Shmueli, Peter C. Bruce, Peter Gedeck\n\nCode included in\nData Mining for Business Analytics: Concepts, Techniques, and Applications in Python (First Edition) Galit Shmueli, Peter C. Bruce, Peter Gedeck, and Nitin R. Patel. 2019.\nPandas provides a number of basic plotting capabilities. Matplotlib however gives you more control over details of the visualisation. The Pandas plot methods return an axes object, which can also be used to modify the visualisation using basic matplotlib commands."
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 03 - Data visualization.html#import-required-packages",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 03 - Data visualization.html#import-required-packages",
    "title": "Chapter 3: Data Visualization",
    "section": "Import required packages",
    "text": "Import required packages\n\nimport os\nimport calendar\nfrom pathlib import Path\nimport numpy as np\nimport networkx as nx\nimport pandas as pd\nfrom pandas.plotting import scatter_matrix, parallel_coordinates\nimport seaborn as sns\nfrom sklearn import preprocessing\nimport matplotlib.pylab as plt\nimport dmba\n\n%matplotlib inline\n\nno display found. Using non-interactive Agg backend"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 03 - Data visualization.html#table-3.2",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 03 - Data visualization.html#table-3.2",
    "title": "Chapter 3: Data Visualization",
    "section": "Table 3.2",
    "text": "Table 3.2\n\nhousing_df = dmba.load_data('BostonHousing.csv')\n# rename CAT. MEDV column for easier data handling\nhousing_df = housing_df.rename(columns={'CAT. MEDV': 'CAT_MEDV'})\nprint(housing_df.head(9))\nhousing_df.head(9)\n\n      CRIM    ZN  INDUS  CHAS    NOX     RM    AGE     DIS  RAD  TAX  PTRATIO  \\\n0  0.00632  18.0   2.31     0  0.538  6.575   65.2  4.0900    1  296     15.3   \n1  0.02731   0.0   7.07     0  0.469  6.421   78.9  4.9671    2  242     17.8   \n2  0.02729   0.0   7.07     0  0.469  7.185   61.1  4.9671    2  242     17.8   \n3  0.03237   0.0   2.18     0  0.458  6.998   45.8  6.0622    3  222     18.7   \n4  0.06905   0.0   2.18     0  0.458  7.147   54.2  6.0622    3  222     18.7   \n5  0.02985   0.0   2.18     0  0.458  6.430   58.7  6.0622    3  222     18.7   \n6  0.08829  12.5   7.87     0  0.524  6.012   66.6  5.5605    5  311     15.2   \n7  0.14455  12.5   7.87     0  0.524  6.172   96.1  5.9505    5  311     15.2   \n8  0.21124  12.5   7.87     0  0.524  5.631  100.0  6.0821    5  311     15.2   \n\n   LSTAT  MEDV  CAT_MEDV  \n0   4.98  24.0         0  \n1   9.14  21.6         0  \n2   4.03  34.7         1  \n3   2.94  33.4         1  \n4   5.33  36.2         1  \n5   5.21  28.7         0  \n6  12.43  22.9         0  \n7  19.15  27.1         0  \n8  29.93  16.5         0  \n\n\n\n\n\n\n\n\n\nCRIM\nZN\nINDUS\nCHAS\nNOX\nRM\nAGE\nDIS\nRAD\nTAX\nPTRATIO\nLSTAT\nMEDV\nCAT_MEDV\n\n\n\n\n0\n0.00632\n18.0\n2.31\n0\n0.538\n6.575\n65.2\n4.0900\n1\n296\n15.3\n4.98\n24.0\n0\n\n\n1\n0.02731\n0.0\n7.07\n0\n0.469\n6.421\n78.9\n4.9671\n2\n242\n17.8\n9.14\n21.6\n0\n\n\n2\n0.02729\n0.0\n7.07\n0\n0.469\n7.185\n61.1\n4.9671\n2\n242\n17.8\n4.03\n34.7\n1\n\n\n3\n0.03237\n0.0\n2.18\n0\n0.458\n6.998\n45.8\n6.0622\n3\n222\n18.7\n2.94\n33.4\n1\n\n\n4\n0.06905\n0.0\n2.18\n0\n0.458\n7.147\n54.2\n6.0622\n3\n222\n18.7\n5.33\n36.2\n1\n\n\n5\n0.02985\n0.0\n2.18\n0\n0.458\n6.430\n58.7\n6.0622\n3\n222\n18.7\n5.21\n28.7\n0\n\n\n6\n0.08829\n12.5\n7.87\n0\n0.524\n6.012\n66.6\n5.5605\n5\n311\n15.2\n12.43\n22.9\n0\n\n\n7\n0.14455\n12.5\n7.87\n0\n0.524\n6.172\n96.1\n5.9505\n5\n311\n15.2\n19.15\n27.1\n0\n\n\n8\n0.21124\n12.5\n7.87\n0\n0.524\n5.631\n100.0\n6.0821\n5\n311\n15.2\n29.93\n16.5\n0"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 03 - Data visualization.html#figure-3.1",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 03 - Data visualization.html#figure-3.1",
    "title": "Chapter 3: Data Visualization",
    "section": "Figure 3.1",
    "text": "Figure 3.1\nLoad the Amtrak data and convert them to be suitable for time series analysis\n\nAmtrak_df = dmba.load_data('Amtrak.csv')\nAmtrak_df['Date'] = pd.to_datetime(Amtrak_df.Month, format='%d/%m/%Y')\nridership_ts = pd.Series(Amtrak_df.Ridership.values, index=Amtrak_df.Date)\n\n\nridership_ts.plot(ylim=[1300, 2300], legend=False, figsize=[5, 3])\nplt.xlabel('Year')  # set x-axis label\nplt.ylabel('Ridership (in 000s)')  # set y-axis label\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# alternative plot with matplotlib\nplt.plot(ridership_ts.index, ridership_ts)\nplt.xlabel('Year')  # set x-axis label\nplt.ylabel('Ridership (in 000s)')  # set y-axis label\nplt.show()\n\n\n\n\n\n\n\n\n\nhousing_df = dmba.load_data('BostonHousing.csv')\n# rename CAT. MEDV column for easier data handling\nhousing_df = housing_df.rename(columns={'CAT. MEDV': 'CAT_MEDV'})\nhousing_df.plot.scatter(x='LSTAT', y='MEDV', legend=False)\nplt.show()\n\n\n\n\n\n\n\n\n\n# Set the color of the points in the scatterplot and draw as open circles.\nfig, ax = plt.subplots()\nfig.set_size_inches(5, 3)\n\nax.scatter(housing_df.LSTAT, housing_df.MEDV, color='C2', facecolor='none')\nplt.xlabel('LSTAT')\nplt.ylabel('MEDV')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nax = housing_df.groupby('CHAS').mean().MEDV.plot(kind='bar', figsize=[5, 3])\nax.set_ylabel('Avg. MEDV')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\ndataForPlot = housing_df.groupby('CHAS').mean().MEDV\nfig, ax = plt.subplots()\nax.bar(dataForPlot.index, dataForPlot, color=['C5', 'C1'])\nax.set_xticks([0, 1])\nax.set_xlabel('CHAS')\nax.set_ylabel('Avg. MEDV')\nplt.show()\n\n\n\n\n\n\n\n\n\nhousing_df.groupby('CHAS').mean()\n\n\n\n\n\n\n\n\nCRIM\nZN\nINDUS\nNOX\nRM\nAGE\nDIS\nRAD\nTAX\nPTRATIO\nLSTAT\nMEDV\nCAT_MEDV\n\n\nCHAS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\n3.744447\n11.634820\n11.019193\n0.551817\n6.267174\n67.911677\n3.851915\n9.566879\n409.870488\n18.527176\n12.757941\n22.093843\n0.154989\n\n\n1\n1.851670\n7.714286\n12.719143\n0.593426\n6.519600\n77.500000\n3.029709\n9.314286\n386.257143\n17.491429\n11.241714\n28.440000\n0.314286\n\n\n\n\n\n\n\n\ndataForPlot = housing_df.groupby('CHAS').mean()['CAT_MEDV'] * 100\nax = dataForPlot.plot(kind='bar', figsize=[5, 3])\nax.set_ylabel('% of CAT.MEDV')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots()\nax.bar(dataForPlot.index, dataForPlot, color=['C5', 'C1'])\nax.set_xticks([0, 1])\nax.set_xlabel('CHAS')\nax.set_ylabel('% of CAT.MEDV')\n\nText(0, 0.5, '% of CAT.MEDV')"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 03 - Data visualization.html#figure-3.2",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 03 - Data visualization.html#figure-3.2",
    "title": "Chapter 3: Data Visualization",
    "section": "Figure 3.2",
    "text": "Figure 3.2\nHistogram of MEDV\n\nax = housing_df.MEDV.hist()\nax.set_xlabel('MEDV')\nax.set_ylabel('count')\n\nplt.show()\n\n\n\n\n\n\n\n\n\nfig, ax = plt.subplots()\nax.hist(housing_df.MEDV)\nax.set_axisbelow(True)  # Show the grid lines behind the histogram\nax.grid(which='major', color='grey', linestyle='--')\nax.set_xlabel('MEDV')\nax.set_ylabel('count')\nplt.show()\n\n\n\n\n\n\n\n\n\nax = housing_df.boxplot(column='MEDV', by='CHAS')\nax.set_ylabel('MEDV')\nplt.suptitle('')  # Suppress the titles\nplt.title('')\n\nplt.show()\n\n\n\n\n\n\n\n\n\ndataForPlot = [list(housing_df[housing_df.CHAS==0].MEDV), list(housing_df[housing_df.CHAS==1].MEDV)]\nfig, ax = plt.subplots()\nax.boxplot(dataForPlot)\nax.set_xticks([1, 2])\nax.set_xticklabels([0, 1])\nax.set_xlabel('CHAS')\nax.set_ylabel('MEDV')\nplt.show()"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 03 - Data visualization.html#figure-3.3",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 03 - Data visualization.html#figure-3.3",
    "title": "Chapter 3: Data Visualization",
    "section": "Figure 3.3",
    "text": "Figure 3.3\nSide by side boxplots\n\nfig, axes = plt.subplots(nrows=1, ncols=4)\nhousing_df.boxplot(column='NOX', by='CAT_MEDV', ax=axes[0])\nhousing_df.boxplot(column='LSTAT', by='CAT_MEDV', ax=axes[1])\nhousing_df.boxplot(column='PTRATIO', by='CAT_MEDV', ax=axes[2])\nhousing_df.boxplot(column='INDUS', by='CAT_MEDV', ax=axes[3])\nfor ax in axes:\n    ax.set_xlabel('CAT.MEDV')\nplt.suptitle('')  # Suppress the overall title\nplt.tight_layout()  # Increase the separation between the plots\n\nplt.show()"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 03 - Data visualization.html#figure-3.4",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 03 - Data visualization.html#figure-3.4",
    "title": "Chapter 3: Data Visualization",
    "section": "Figure 3.4",
    "text": "Figure 3.4\n\ncorr = housing_df.corr()\nsns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns)\nplt.show()\n\n\n\n\n\n\n\n\n\n# Change the colormap to a divergent scale and fix the range of the colormap\nsns.heatmap(corr, xticklabels=corr.columns, yticklabels=corr.columns, vmin=-1, vmax=1, cmap=\"RdBu\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# Include information about values\nfig, ax = plt.subplots()\nfig.set_size_inches(11, 7)\nsns.heatmap(corr, annot=True, fmt=\".1f\", cmap=\"RdBu\", center=0, ax=ax)\n\nplt.show()"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 03 - Data visualization.html#figure-3.5",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 03 - Data visualization.html#figure-3.5",
    "title": "Chapter 3: Data Visualization",
    "section": "Figure 3.5",
    "text": "Figure 3.5\nA heatmap can also be used to visualize missing values. Here, we use a sample of the NYPD Motor Vehicle Collisions Dataset. (https://data.cityofnewyork.us/Public-Safety/NYPD-Motor-Vehicle-Collisions/h9gi-nx95)\n\n# df = pd.read_csv('NYPD_Motor_Vehicle_Collisions.csv')\n# df = df.sample(1000)\n# # df.head()\n# df.to_csv('NYPD_Motor_Vehicle_Collisions_1000.csv', index=False)\n\n\ndf = dmba.load_data('NYPD_Motor_Vehicle_Collisions_1000.csv').sort_values(['DATE'])\n\n# Given a dataframe df create a copy of the array that is 0 if a field contains a value \n# and 1 for NaN\nnaInfo = np.zeros(df.shape)\nnaInfo[df.isna().values] = 1\nnaInfo = pd.DataFrame(naInfo, columns=df.columns)\n\nfig, ax = plt.subplots()\nfig.set_size_inches(13, 9)\nax = sns.heatmap(naInfo, vmin=0, vmax=1, cmap=[\"white\", \"#444444\"], cbar=False, ax=ax)\nax.set_yticks([])\n\nrect = plt.Rectangle((0, 0), naInfo.shape[1], naInfo.shape[0], linewidth=1, \n                     edgecolor='lightgrey',facecolor='none')\nrect = ax.add_patch(rect)\nrect.set_clip_on(False)\n\nplt.xticks(rotation=80)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# # Create a dataframe with missing values\n# np.random.seed(12345)\n# df = pd.DataFrame(np.random.randint(50, size=(100, 10)))\n# df.where(df!=max(df), np.nan, inplace=True)\n\n# # Given a dataframe df create a copy of the array that is 0 if a field contains a value and 1 for NaN\n# naInfo = np.zeros(df.shape)\n# naInfo[df.isna().values] = 1\n# ax = sns.heatmap(naInfo, vmin=0, vmax=1, cmap=[\"white\", \"red\"], cbar=False)\n\n# plt.show()"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 03 - Data visualization.html#figure-3.6",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 03 - Data visualization.html#figure-3.6",
    "title": "Chapter 3: Data Visualization",
    "section": "Figure 3.6",
    "text": "Figure 3.6\n\n# Color the points by the value of CAT.MEDV\nhousing_df.plot.scatter(x='LSTAT', y='NOX', \n                        c=['C0' if c == 1 else 'C1' for c in housing_df.CAT_MEDV])\nplt.show()\n\n\n\n\n\n\n\n\n\n# Change the rendering of the points to open circles by controlling the color\nhousing_df.plot.scatter(x='LSTAT', y='NOX', color='none',\n                        edgecolor=['C0' if c == 1 else 'C1' for c in housing_df.CAT_MEDV])\nplt.show()\n\n\n\n\n\n\n\n\nIf the data points for CAT_MEDV of 1 should be emphasized, it’s useful to draw them separately.\n\n# Plot first the data points for CAT.MEDV of 0 and then of 1\n_, ax = plt.subplots()\nfor catValue, color in (0, 'C1'), (1, 'C0'):\n    subset_df = housing_df[housing_df.CAT_MEDV == catValue]\n    ax.scatter(subset_df.LSTAT, subset_df.NOX, color='none', edgecolor=color)\nax.set_xlabel('LSTAT')\nax.set_ylabel('NOX')\nax.legend([\"CAT.MEDV 0\", \"CAT.MEDV 1\"])\n\nplt.show()"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 03 - Data visualization.html#panel-plots",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 03 - Data visualization.html#panel-plots",
    "title": "Chapter 3: Data Visualization",
    "section": "Panel plots",
    "text": "Panel plots\nCompute mean MEDV per RAD and CHAS and create two bar charts for each value of RAD\n\ndataForPlot_df = housing_df.groupby(['CHAS','RAD']).mean()['MEDV']\n# We determine all possible RAD values to use as ticks\nticks = set(housing_df.RAD)\nfor i in range(2):\n    for t in ticks.difference(dataForPlot_df[i].index):\n        dataForPlot_df.loc[(i, t)] = 0\n# reorder to rows, so that the index is sorted\ndataForPlot_df = dataForPlot_df[sorted(dataForPlot_df.index)]\n\n# Determine a common range for the y axis\nyRange = [0, max(dataForPlot_df) * 1.1] \n\nfig, axes = plt.subplots(nrows=2, ncols=1)\ndataForPlot_df[0].plot.bar(x='RAD', ax=axes[0], ylim=yRange)\ndataForPlot_df[1].plot.bar(x='RAD', ax=axes[1], ylim=yRange)\naxes[0].annotate('CHAS = 0', xy=(3.5, 45))\naxes[1].annotate('CHAS = 1', xy=(3.5, 45))\n\nplt.show()"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 03 - Data visualization.html#figure-3.7",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 03 - Data visualization.html#figure-3.7",
    "title": "Chapter 3: Data Visualization",
    "section": "Figure 3.7",
    "text": "Figure 3.7\nScatterplot matrix\n\n_ = scatter_matrix(housing_df[['CRIM', 'INDUS', 'LSTAT', 'MEDV']], figsize=(6, 6), diagonal='kde')\n\n\n\n\n\n\n\n\n\n# Add the correlation coefficient to the scatterplots above the diagonal\ndf = housing_df[['CRIM', 'INDUS', 'LSTAT', 'MEDV']]\naxes = scatter_matrix(df, alpha=0.5, figsize=(6, 6), diagonal='kde')\ncorr = df.corr().values\nfor i, j in zip(*plt.np.triu_indices_from(axes, k=1)):\n    axes[i, j].annotate(\"%.3f\" %corr[i,j], (0.8, 0.8), xycoords='axes fraction', ha='center', va='center')\n\nplt.show()"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 03 - Data visualization.html#figure-3.8",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 03 - Data visualization.html#figure-3.8",
    "title": "Chapter 3: Data Visualization",
    "section": "Figure 3.8",
    "text": "Figure 3.8\n\n# Avoid the use of scientific notation for the log axis\nplt.rcParams['axes.formatter.min_exponent'] = 4\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(7, 3))\n\n# Regular scale\nhousing_df.plot.scatter(x='CRIM', y='MEDV', ax=axes[0])\n# log scale\nax = housing_df.plot.scatter(x='CRIM', y='MEDV', logx=True, logy=True, ax=axes[1])\nax.set_yticks([5, 10, 20, 50])\nax.set_yticklabels([5, 10, 20, 50])\nplt.tight_layout()\n\nplt.show()\n\n\n\n\n\n\n\n\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(7, 3))\n\n# regular scale\nax = housing_df.boxplot(column='CRIM', by='CAT_MEDV', ax=axes[0])\nax.set_xlabel('CAT.MEDV')\nax.set_ylabel('CRIM')\n\n# log scale\nax = housing_df.boxplot(column='CRIM', by='CAT_MEDV', ax=axes[1])\nax.set_xlabel('CAT.MEDV')\nax.set_ylabel('CRIM')\nax.set_yscale('log')\n\n# suppress the title\naxes[0].get_figure().suptitle('')\nplt.tight_layout()\n\nplt.show()"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 03 - Data visualization.html#figure-3.9",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 03 - Data visualization.html#figure-3.9",
    "title": "Chapter 3: Data Visualization",
    "section": "Figure 3.9",
    "text": "Figure 3.9\n\nfig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10, 7))\n\n\nAmtrak_df = dmba.load_data('Amtrak.csv')\nAmtrak_df['Month'] = pd.to_datetime(Amtrak_df.Month, format='%d/%m/%Y')\nAmtrak_df.set_index('Month', inplace=True)\n\n# fit quadratic curve and display \nquadraticFit = np.poly1d(np.polyfit(range(len(Amtrak_df)), Amtrak_df.Ridership, 2))\nAmtrak_fit = pd.DataFrame({'fit': [quadraticFit(t) for t in range(len(Amtrak_df))]})\nAmtrak_fit.index = Amtrak_df.index\n\nax = Amtrak_df.plot(ylim=[1300, 2300], legend=False, ax=axes[0][0])\nAmtrak_fit.plot(ax=ax)\nax.set_xlabel('Year')  # set x-axis label\nax.set_ylabel('Ridership (in 000s)')  # set y-axis label\n\n# Zoom in 2-year period 1/1/1991 to 12/1/1992\nridership_2yrs = Amtrak_df.loc['1991-01-01':'1992-12-01']\nax = ridership_2yrs.plot(ylim=[1300, 2300], legend=False, ax=axes[1][0])\nax.set_xlabel('Year')  # set x-axis label\nax.set_ylabel('Ridership (in 000s)')  # set y-axis label\n\n# Average by month\nbyMonth = Amtrak_df.groupby(by=[Amtrak_df.index.month]).mean()\nax = byMonth.plot(ylim=[1300, 2300], legend=False, ax=axes[0][1])\nax.set_xlabel('Month')  # set x-axis label\nax.set_ylabel('Ridership (in 000s)')  # set y-axis label\nyticks = [-2.0,-1.75,-1.5,-1.25,-1.0,-0.75,-0.5,-0.25,0.0]\nax.set_xticks(range(1, 13))\nax.set_xticklabels([calendar.month_abbr[i] for i in range(1, 13)]);\n\n# Average by year (exclude data from 2004)\nbyYear = Amtrak_df.loc['1991-01-01':'2003-12-01'].groupby(pd.Grouper(freq='A')).mean()\nax = byYear.plot(ylim=[1300, 2300], legend=False, ax=axes[1][1])\nax.set_xlabel('Year')  # set x-axis label\nax.set_ylabel('Ridership (in 000s)')  # set y-axis label\n\nplt.tight_layout()\n\nplt.show()"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 03 - Data visualization.html#figure-3.10",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 03 - Data visualization.html#figure-3.10",
    "title": "Chapter 3: Data Visualization",
    "section": "Figure 3.10",
    "text": "Figure 3.10\n\nutilities_df = dmba.load_data('Utilities.csv')\n\nax = utilities_df.plot.scatter(x='Sales', y='Fuel_Cost', figsize=(6, 6))\npoints = utilities_df[['Sales','Fuel_Cost','Company']]\n_ = points.apply(lambda x: \n             ax.text(*x, rotation=20, horizontalalignment='left',\n                     verticalalignment='bottom', fontsize=8), axis=1)\n\nplt.show()"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 03 - Data visualization.html#figure-3.11",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 03 - Data visualization.html#figure-3.11",
    "title": "Chapter 3: Data Visualization",
    "section": "Figure 3.11",
    "text": "Figure 3.11\nUse alpha to add transparent colors\n\nuniversal_df = dmba.load_data('UniversalBank.csv')\n\nuniversal_df.plot.scatter(x='Income', y='CCAvg', \n                          c=['black' if c == 1 else 'grey' for c in universal_df['Securities Account']],\n                          ylim = (0.05, 10), alpha=0.2,\n                          logx=True, logy=True)\nplt.show()\n\n\n\n\n\n\n\n\nThe pandas plot pandas has a number of problems that can be solved using matplotlib\n\noverlapping points: add random jitter to the datapoints\ndistribution of securities account less obvious due to overplotting: separate plotting of the two sets of accounts\n\n\ndef jitter(x, factor=1):\n    \"\"\" Add random jitter to x values \"\"\"\n    sx = np.array(sorted(x))\n    delta = sx[1:] - sx[:-1]\n    minDelta = min(d for d in delta if d &gt; 0)\n    a = factor * minDelta / 5\n    return x + np.random.uniform(-a, a, len(x))\n    \n    \nsaIdx = universal_df[universal_df['Securities Account'] == 1].index\n\nplt.figure(figsize=(10,6))\nplt.scatter(jitter(universal_df.drop(saIdx).Income),\n            jitter(universal_df.drop(saIdx).CCAvg),\n            marker='o', color='grey', alpha=0.2)\nplt.scatter(jitter(universal_df.loc[saIdx].Income),\n            jitter(universal_df.loc[saIdx].CCAvg), \n            marker='o', color='red', alpha=0.2)\nplt.xlabel('Income')\nplt.ylabel('CCAvg')\nplt.ylim((0.05, 20))\naxes = plt.gca()\naxes.set_xscale(\"log\")\naxes.set_yscale(\"log\")\n\nplt.show()"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 03 - Data visualization.html#figure-3.12",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 03 - Data visualization.html#figure-3.12",
    "title": "Chapter 3: Data Visualization",
    "section": "Figure 3.12",
    "text": "Figure 3.12\n\n# Transform the axes, so that they all have the same range\nmin_max_scaler = preprocessing.MinMaxScaler()\ndataToPlot = pd.DataFrame(min_max_scaler.fit_transform(housing_df),\n                         columns=housing_df.columns)\n\nfig, axes = plt.subplots(nrows=2, ncols=1, figsize=[8, 6])\nfor i in (0, 1):\n    parallel_coordinates(dataToPlot.loc[dataToPlot.CAT_MEDV == i], \n                         'CAT_MEDV', ax=axes[i], linewidth=0.5)\n    axes[i].set_title('CAT.MEDV = {}'.format(i))\n    axes[i].set_yticklabels([])\n    axes[i].legend().set_visible(False)\n\nplt.tight_layout()  # Increase the separation between the plots\n\nplt.show()"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 03 - Data visualization.html#figure-3.14",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 03 - Data visualization.html#figure-3.14",
    "title": "Chapter 3: Data Visualization",
    "section": "Figure 3.14",
    "text": "Figure 3.14\n\nebay_df = dmba.load_data('eBayNetwork.csv')\n\nG = nx.from_pandas_edgelist(ebay_df, source='Seller', target='Bidder')\n\nisBidder = [n in set(ebay_df.Bidder) for n in G.nodes()]\npos = nx.spring_layout(G, k=0.13, iterations=60, scale=0.5)\nplt.figure(figsize=(10,10))\nnx.draw_networkx(G, pos=pos, with_labels=False,\n                 edge_color='lightgray',\n                 node_color=['gray' if bidder else 'black' for bidder in isBidder],\n                 node_size=[50 if bidder else 200 for bidder in isBidder])\nplt.axis('off')\n\nplt.show()"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 03 - Data visualization.html#figure-3.15",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 03 - Data visualization.html#figure-3.15",
    "title": "Chapter 3: Data Visualization",
    "section": "Figure 3.15",
    "text": "Figure 3.15\nThere are currently no packages in Python that provide treemaps of the same quality and flexibility as R.\nUse example from here: https://python-graph-gallery.com/treemap/\n\nimport squarify\nimport matplotlib\n\nebayTreemap = dmba.load_data('EbayTreemap.csv')\n\ngrouped = []\nfor category, df in ebayTreemap.groupby(['Category']):\n    negativeFeedback = sum(df['Seller Feedback'] &lt; 0) / len(df)\n    grouped.append({\n        'category': category, \n        'negativeFeedback': negativeFeedback, \n        'averageBid': df['High Bid'].mean()\n    })\nbyCategory = pd.DataFrame(grouped)\n\nnorm = matplotlib.colors.Normalize(vmin=byCategory.negativeFeedback.min(), vmax=byCategory.negativeFeedback.max())\ncolors = [matplotlib.cm.Blues(norm(value)) for value in byCategory.negativeFeedback]\n\nfig, ax = plt.subplots()\nfig.set_size_inches(9, 5)\n\nrenameCategories = {\n    'Business & Industrial': 'Business &\\nIndustrial',\n    'Health & Beauty': 'Health &\\nBeauty',\n    'Consumer Electronics': 'Consumer\\nElectronics',\n    'Clothing & accessories': 'Clothing &\\naccessories',\n    'Clothing shoes & accessories': 'Clothing shoes &\\naccessories'\n}\nlabels = [renameCategories.get(c, c) for c in byCategory.category]\n\nsquarify.plot(label=labels, sizes=byCategory.averageBid, color=colors, \n              ax=ax, alpha=0.6, edgecolor='grey', text_kwargs={'fontsize': 8.7})\n\nax.get_xaxis().set_ticks([])\nax.get_yaxis().set_ticks([])\n\nplt.subplots_adjust(left=0.1)\nplt.show()\n# labels"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 03 - Data visualization.html#figure-3.16",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 03 - Data visualization.html#figure-3.16",
    "title": "Chapter 3: Data Visualization",
    "section": "Figure 3.16",
    "text": "Figure 3.16\nTo run this example you need an API key for Google maps. Go to https://cloud.google.com/maps-platform/ to find out how to get one.\n\nif 'GMAPS_API_KEY' in os.environ:\n    import gmaps\n    SCstudents = dmba.load_data('SC-US-students-GPS-data-2016.csv')\n\n    gmaps.configure(api_key=os.environ['GMAPS_API_KEY'])\n    fig = gmaps.figure(center=(39.7, -105), zoom_level=3)\n    fig.add_layer(gmaps.symbol_layer(SCstudents, scale=2, \n                                     fill_color='red', stroke_color='red'))\n    fig"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 03 - Data visualization.html#figure-3.17",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 03 - Data visualization.html#figure-3.17",
    "title": "Chapter 3: Data Visualization",
    "section": "Figure 3.17",
    "text": "Figure 3.17\n\nIf you get an error message on MacOS about a missing library, set the environment variable DYLD_FALLBACK_LIBRARY_PATH before starting jupyter notebook. \nexport DYLD_FALLBACK_LIBRARY_PATH=/usr/local/lib:/lib:/usr/lib\n\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport cartopy\nimport cartopy.io.shapereader as shpreader\nimport cartopy.crs as ccrs\n\ngdp_df = dmba.load_data('gdp.csv', skiprows=4)\n\ngdp_df.rename(columns={'2015': 'GDP2015'}, inplace=True)\ngdp_df.set_index('Country Code', inplace=True)  # use the three letter country code to access rows\n\n# The file contains a column with two letter combinations, use na_filter to avoid converting\n# the combination NA into not-a-number\nhappiness_df = dmba.load_data('Veerhoven.csv', na_filter = False)\nhappiness_df.set_index('Code', inplace=True)  # use the country name to access rows\n\n\nfig = plt.figure(figsize=(7, 8))\n\nax1 = plt.subplot(2, 1, 1, projection=ccrs.PlateCarree())\nax1.set_extent([-150, 60, -25, 60])\n\nax2 = plt.subplot(2, 1, 2, projection=ccrs.PlateCarree())\nax2.set_extent([-150, 60, -25, 60])\n\n# Create a color mapper\ncmap = plt.cm.Blues_r\nnorm1 = matplotlib.colors.Normalize(vmin=happiness_df.Score.dropna().min(), \n                                    vmax=happiness_df.Score.dropna().max())\nnorm2 = matplotlib.colors.LogNorm(vmin=gdp_df.GDP2015.dropna().min(), \n                                  vmax=gdp_df.GDP2015.dropna().max())\n\nshpfilename = shpreader.natural_earth(resolution='110m',\n                                      category='cultural',\n                                      name='admin_0_countries')\nreader = shpreader.Reader(shpfilename)\ncountries = reader.records()\nfor country in countries:\n    countryCode = country.attributes['ADM0_A3']\n    if countryCode in gdp_df.index:\n        ax2.add_geometries([country.geometry], ccrs.PlateCarree(),\n                           facecolor=cmap(norm2(gdp_df.loc[countryCode].GDP2015)))\n    # check various attributes to find the matching two-letter combinations\n    nation = country.attributes['POSTAL'] \n    if nation not in happiness_df.index:\n        nation = country.attributes['ISO_A2']\n    if nation not in happiness_df.index:\n        nation = country.attributes['WB_A2']\n    if nation not in happiness_df.index and country.attributes['NAME'] == 'Norway':\n        nation = 'NO'\n    if nation in happiness_df.index:\n        ax1.add_geometries([country.geometry], ccrs.PlateCarree(),\n                           facecolor=cmap(norm1(happiness_df.loc[nation].Score)))\n        \nax2.set_title(\"GDP 2015\")\nsm = plt.cm.ScalarMappable(norm=norm2, cmap=cmap)\nsm._A = []\ncb = plt.colorbar(sm, ax=ax2)\ncb.set_ticks([1e8, 1e9, 1e10, 1e11, 1e12, 1e13])\n\nax1.set_title(\"Happiness\")\nsm = plt.cm.ScalarMappable(norm=norm1, cmap=cmap)\nsm._A = []\ncb = plt.colorbar(sm, ax=ax1)\ncb.set_ticks([3, 4, 5, 6, 7, 8])\n\n\nplt.show()\n\n/opt/conda/lib/python3.9/site-packages/cartopy/io/__init__.py:241: DownloadWarning: Downloading: https://naciscdn.org/naturalearth/110m/cultural/ne_110m_admin_0_countries.zip\n  warnings.warn('Downloading: {}'.format(url), DownloadWarning)"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 06 - Multiple linear regression.html",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 06 - Multiple linear regression.html",
    "title": "Chapter 6: Multiple Linear Regression",
    "section": "",
    "text": "2019 Galit Shmueli, Peter C. Bruce, Peter Gedeck\n\nCode included in\nData Mining for Business Analytics: Concepts, Techniques, and Applications in Python (First Edition) Galit Shmueli, Peter C. Bruce, Peter Gedeck, and Nitin R. Patel. 2019."
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 06 - Multiple linear regression.html#import-required-packages",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 06 - Multiple linear regression.html#import-required-packages",
    "title": "Chapter 6: Multiple Linear Regression",
    "section": "Import required packages",
    "text": "Import required packages\n\nfrom pathlib import Path\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge, LassoCV, BayesianRidge\nimport statsmodels.formula.api as sm\nimport matplotlib.pylab as plt\n\nimport dmba\nfrom dmba import regressionSummary, exhaustive_search\nfrom dmba import backward_elimination, forward_selection, stepwise_selection\nfrom dmba import adjusted_r2_score, AIC_score, BIC_score\n\n%matplotlib inline\n\nno display found. Using non-interactive Agg backend"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 06 - Multiple linear regression.html#table-6.3",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 06 - Multiple linear regression.html#table-6.3",
    "title": "Chapter 6: Multiple Linear Regression",
    "section": "Table 6.3",
    "text": "Table 6.3\n\n# Reduce data frame to the top 1000 rows and select columns for regression analysis\ncar_df = dmba.load_data('ToyotaCorolla.csv')\ncar_df = car_df.iloc[0:1000]\n\npredictors = ['Age_08_04', 'KM', 'Fuel_Type', 'HP', 'Met_Color', 'Automatic', 'CC', \n              'Doors', 'Quarterly_Tax', 'Weight']\noutcome = 'Price'\n\n# partition data\nX = pd.get_dummies(car_df[predictors], drop_first=True)\ny = car_df[outcome]\ntrain_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size=0.4, random_state=1)\n\ncar_lm = LinearRegression()\ncar_lm.fit(train_X, train_y)\n\n# print coefficients\nprint('intercept ', car_lm.intercept_)\nprint(pd.DataFrame({'Predictor': X.columns, 'coefficient': car_lm.coef_}))\n\n# print performance measures\nregressionSummary(train_y, car_lm.predict(train_X))\n\nintercept  -1319.354380041219\n           Predictor  coefficient\n0          Age_08_04  -140.748761\n1                 KM    -0.017840\n2                 HP    36.103419\n3          Met_Color    84.281830\n4          Automatic   416.781954\n5                 CC     0.017737\n6              Doors   -50.657863\n7      Quarterly_Tax    13.625325\n8             Weight    13.038711\n9   Fuel_Type_Diesel  1066.464681\n10  Fuel_Type_Petrol  2310.249543\n\nRegression statistics\n\n                      Mean Error (ME) : -0.0000\n       Root Mean Squared Error (RMSE) : 1400.5823\n            Mean Absolute Error (MAE) : 1046.9072\n          Mean Percentage Error (MPE) : -1.0223\nMean Absolute Percentage Error (MAPE) : 9.2994\n\n\n\npred_y = car_lm.predict(train_X)\n\nprint('adjusted r2 : ', adjusted_r2_score(train_y, pred_y, car_lm))\nprint('AIC : ', AIC_score(train_y, pred_y, car_lm))\nprint('BIC : ', BIC_score(train_y, pred_y, car_lm))\n\nadjusted r2 :  0.8537958550253093\nAIC :  10422.298278332171\nBIC :  10479.45836384998"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 06 - Multiple linear regression.html#table-6.4",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 06 - Multiple linear regression.html#table-6.4",
    "title": "Chapter 6: Multiple Linear Regression",
    "section": "Table 6.4",
    "text": "Table 6.4\n\n# Use predict() to make predictions on a new set\ncar_lm_pred = car_lm.predict(valid_X)\n\nresult = pd.DataFrame({'Predicted': car_lm_pred, 'Actual': valid_y,\n                       'Residual': valid_y - car_lm_pred})\nprint(result.head(20))\n\n# Compute common accuracy measures\nregressionSummary(valid_y, car_lm_pred)\n\n        Predicted  Actual     Residual\n507  10607.333940   11500   892.666060\n818   9272.705792    8950  -322.705792\n452  10617.947808   11450   832.052192\n368  13600.396275   11450 -2150.396275\n242  12396.694660   11950  -446.694660\n929   9496.498212    9995   498.501788\n262  12480.063217   13500  1019.936783\n810   8834.146068    7950  -884.146068\n318  12183.361282    9900 -2283.361282\n49   19206.965683   21950  2743.034317\n446  10987.498309   11950   962.501691\n142  18501.527375   19950  1448.472625\n968   9914.690947    9950    35.309053\n345  13827.299932   14950  1122.700068\n971   7966.732543   10495  2528.267457\n133  17185.242041   15950 -1235.242041\n104  19952.658062   19450  -502.658062\n6    16570.609280   16900   329.390720\n600  13739.409113   11250 -2489.409113\n496  11267.513740   11750   482.486260\n\nRegression statistics\n\n                      Mean Error (ME) : 103.6803\n       Root Mean Squared Error (RMSE) : 1312.8523\n            Mean Absolute Error (MAE) : 1017.5972\n          Mean Percentage Error (MPE) : -0.2633\nMean Absolute Percentage Error (MAPE) : 9.0111"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 06 - Multiple linear regression.html#figure-6.1",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 06 - Multiple linear regression.html#figure-6.1",
    "title": "Chapter 6: Multiple Linear Regression",
    "section": "Figure 6.1",
    "text": "Figure 6.1\nDetermine the residuals and create a histogram\n\ncar_lm_pred = car_lm.predict(valid_X)\nall_residuals = valid_y - car_lm_pred\n\n# Determine the percentage of datapoints with a residual in [-1406, 1406] = approx. 75\\%\nprint(len(all_residuals[(all_residuals &gt; -1406) & (all_residuals &lt; 1406)]) / len(all_residuals))\n\nax = pd.DataFrame({'Residuals': all_residuals}).hist(bins=25)\n\nplt.tight_layout()\nplt.show()\n\n0.7425"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 06 - Multiple linear regression.html#table-6.5",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 06 - Multiple linear regression.html#table-6.5",
    "title": "Chapter 6: Multiple Linear Regression",
    "section": "Table 6.5",
    "text": "Table 6.5\nRun an exhaustive search. The Fuel type column is categorical and needs to be converted into dummy variables.\n\ndef train_model(variables):\n    model = LinearRegression()\n    model.fit(train_X[variables], train_y)\n    return model\n\ndef score_model(model, variables):\n    pred_y = model.predict(train_X[variables])\n    # we negate as score is optimized to be as low as possible\n    return -adjusted_r2_score(train_y, pred_y, model)\n\nallVariables = train_X.columns\nresults = exhaustive_search(allVariables, train_model, score_model)\n\ndata = []\nfor result in results:\n    model = result['model']\n    variables = result['variables']\n    AIC = AIC_score(train_y, model.predict(train_X[variables]), model)\n    \n    d = {'n': result['n'], 'r2adj': -result['score'], 'AIC': AIC}\n    d.update({var: var in result['variables'] for var in allVariables})\n    data.append(d)\npd.set_option('display.width', 100)\nprint(pd.DataFrame(data, columns=('n', 'r2adj', 'AIC') + tuple(sorted(allVariables))))\npd.reset_option('display.width')\n\n     n     r2adj           AIC  Age_08_04  Automatic     CC  Doors  Fuel_Type_Diesel  \\\n0    1  0.767901  10689.712094       True      False  False  False             False   \n1    2  0.801160  10597.910645       True      False  False  False             False   \n2    3  0.829659  10506.084235       True      False  False  False             False   \n3    4  0.846357  10445.174820       True      False  False  False             False   \n4    5  0.849044  10435.578836       True      False  False  False             False   \n5    6  0.853172  10419.932278       True      False  False  False             False   \n6    7  0.853860  10418.104025       True      False  False  False              True   \n7    8  0.854297  10417.290103       True       True  False  False              True   \n8    9  0.854172  10418.789079       True       True  False   True              True   \n9   10  0.854036  10420.330800       True       True  False   True              True   \n10  11  0.853796  10422.298278       True       True   True   True              True   \n\n    Fuel_Type_Petrol     HP     KM  Met_Color  Quarterly_Tax  Weight  \n0              False  False  False      False          False   False  \n1              False   True  False      False          False   False  \n2              False   True  False      False          False    True  \n3              False   True   True      False          False    True  \n4              False   True   True      False           True    True  \n5               True   True   True      False           True    True  \n6               True   True   True      False           True    True  \n7               True   True   True      False           True    True  \n8               True   True   True      False           True    True  \n9               True   True   True       True           True    True  \n10              True   True   True       True           True    True"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 06 - Multiple linear regression.html#table-6.6-backward-elimination",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 06 - Multiple linear regression.html#table-6.6-backward-elimination",
    "title": "Chapter 6: Multiple Linear Regression",
    "section": "Table 6.6 backward elimination",
    "text": "Table 6.6 backward elimination\n\ndef train_model(variables):\n    model = LinearRegression()\n    model.fit(train_X[variables], train_y)\n    return model\n\ndef score_model(model, variables):\n    return AIC_score(train_y, model.predict(train_X[variables]), model)\n\nbest_model, best_variables = backward_elimination(train_X.columns, train_model, score_model, verbose=True)\n\nprint(best_variables)\n\nVariables: Age_08_04, KM, HP, Met_Color, Automatic, CC, Doors, Quarterly_Tax, Weight, Fuel_Type_Diesel, Fuel_Type_Petrol\nStart: score=10422.30\nStep: score=10420.33, remove CC\nStep: score=10418.79, remove Met_Color\nStep: score=10417.29, remove Doors\nStep: score=10417.29, remove None\n['Age_08_04', 'KM', 'HP', 'Automatic', 'Quarterly_Tax', 'Weight', 'Fuel_Type_Diesel', 'Fuel_Type_Petrol']\n\n\n\nregressionSummary(valid_y, best_model.predict(valid_X[best_variables]))\n\n\nRegression statistics\n\n                      Mean Error (ME) : 103.3045\n       Root Mean Squared Error (RMSE) : 1314.4844\n            Mean Absolute Error (MAE) : 1016.8875\n          Mean Percentage Error (MPE) : -0.2700\nMean Absolute Percentage Error (MAPE) : 8.9984"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 06 - Multiple linear regression.html#table-6.7-forward-selection",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 06 - Multiple linear regression.html#table-6.7-forward-selection",
    "title": "Chapter 6: Multiple Linear Regression",
    "section": "Table 6.7 Forward selection",
    "text": "Table 6.7 Forward selection\n\n# The initial model is the constant model - this requires special handling\n# in train_model and score_model\ndef train_model(variables):\n    if len(variables) == 0:\n        return None\n    model = LinearRegression()\n    model.fit(train_X[variables], train_y)\n    return model\n\ndef score_model(model, variables):\n    if len(variables) == 0:\n        return AIC_score(train_y, [train_y.mean()] * len(train_y), model, df=1)\n    return AIC_score(train_y, model.predict(train_X[variables]), model)\n\nbest_model, best_variables = forward_selection(train_X.columns, train_model, score_model, verbose=True)\n\nprint(best_variables)\n\nVariables: Age_08_04, KM, HP, Met_Color, Automatic, CC, Doors, Quarterly_Tax, Weight, Fuel_Type_Diesel, Fuel_Type_Petrol\nStart: score=11565.07, constant\nStep: score=10689.71, add Age_08_04\nStep: score=10597.91, add HP\nStep: score=10506.08, add Weight\nStep: score=10445.17, add KM\nStep: score=10435.58, add Quarterly_Tax\nStep: score=10419.93, add Fuel_Type_Petrol\nStep: score=10418.10, add Fuel_Type_Diesel\nStep: score=10417.29, add Automatic\nStep: score=10417.29, add None\n['Age_08_04', 'HP', 'Weight', 'KM', 'Quarterly_Tax', 'Fuel_Type_Petrol', 'Fuel_Type_Diesel', 'Automatic']\n\n\n\nbest_model, best_variables = stepwise_selection(train_X.columns, train_model, score_model, verbose=True)\n\nprint(best_variables)\n\nVariables: Age_08_04, KM, HP, Met_Color, Automatic, CC, Doors, Quarterly_Tax, Weight, Fuel_Type_Diesel, Fuel_Type_Petrol\nStart: score=11565.07, constant\nStep: score=10689.71, add Age_08_04\nStep: score=10597.91, add HP\nStep: score=10506.08, add Weight\nStep: score=10445.17, add KM\nStep: score=10435.58, add Quarterly_Tax\nStep: score=10419.93, add Fuel_Type_Petrol\nStep: score=10418.10, add Fuel_Type_Diesel\nStep: score=10417.29, add Automatic\nStep: score=10417.29, unchanged None\n['Age_08_04', 'HP', 'Weight', 'KM', 'Quarterly_Tax', 'Fuel_Type_Petrol', 'Fuel_Type_Diesel', 'Automatic']"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 06 - Multiple linear regression.html#table-xx-regularized-methods",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 06 - Multiple linear regression.html#table-xx-regularized-methods",
    "title": "Chapter 6: Multiple Linear Regression",
    "section": "Table XX regularized methods",
    "text": "Table XX regularized methods\n\nlasso = Lasso(normalize=True, alpha=1)\nlasso.fit(train_X, train_y)\nregressionSummary(valid_y, lasso.predict(valid_X))\n\nlasso_cv = LassoCV(normalize=True, cv=5)\nlasso_cv.fit(train_X, train_y)\nregressionSummary(valid_y, lasso_cv.predict(valid_X))\nprint('Lasso-CV chosen regularization: ', lasso_cv.alpha_)\nprint(lasso_cv.coef_)\n\nridge = Ridge(normalize=True, alpha=1)\nridge.fit(train_X, train_y)\nregressionSummary(valid_y, ridge.predict(valid_X))\n\nbayesianRidge = BayesianRidge(normalize=True)\nbayesianRidge.fit(train_X, train_y)\nregressionSummary(valid_y, bayesianRidge.predict(valid_X))\nprint('Bayesian ridge chosen regularization: ', bayesianRidge.lambda_ / bayesianRidge.alpha_)\n\n\nRegression statistics\n\n                      Mean Error (ME) : 120.6311\n       Root Mean Squared Error (RMSE) : 1332.2752\n            Mean Absolute Error (MAE) : 1021.5286\n          Mean Percentage Error (MPE) : -0.2364\nMean Absolute Percentage Error (MAPE) : 9.0115\n\nRegression statistics\n\n                      Mean Error (ME) : 145.1571\n       Root Mean Squared Error (RMSE) : 1397.9428\n            Mean Absolute Error (MAE) : 1052.4649\n          Mean Percentage Error (MPE) : -0.2966\nMean Absolute Percentage Error (MAPE) : 9.2918\nLasso-CV chosen regularization:  3.5138446691310588\n[-1.40370575e+02 -1.76669006e-02  3.38674037e+01  0.00000000e+00\n  6.94393427e+01  0.00000000e+00  0.00000000e+00  2.70913468e+00\n  1.24342596e+01 -0.00000000e+00  0.00000000e+00]\n\nRegression statistics\n\n                      Mean Error (ME) : 154.3286\n       Root Mean Squared Error (RMSE) : 1879.7426\n            Mean Absolute Error (MAE) : 1353.2735\n          Mean Percentage Error (MPE) : -2.3897\nMean Absolute Percentage Error (MAPE) : 11.1309\n\nRegression statistics\n\n                      Mean Error (ME) : 105.5382\n       Root Mean Squared Error (RMSE) : 1313.0217\n            Mean Absolute Error (MAE) : 1017.2356\n          Mean Percentage Error (MPE) : -0.2703\nMean Absolute Percentage Error (MAPE) : 9.0012\nBayesian ridge chosen regularization:  0.004622833439968832\n\n\n\nlinearRegression = LinearRegression(normalize=True).fit(train_X, train_y)\nregressionSummary(valid_y, linearRegression.predict(valid_X))\n\n\nRegression statistics\n\n                      Mean Error (ME) : 103.6803\n       Root Mean Squared Error (RMSE) : 1312.8523\n            Mean Absolute Error (MAE) : 1017.5972\n          Mean Percentage Error (MPE) : -0.2633\nMean Absolute Percentage Error (MAPE) : 9.0111\n\n\n\npd.DataFrame({'features': train_X.columns, 'linear regression': linearRegression.coef_, \n              'lassoCV': lasso_cv.coef_, 'bayesianRidge': bayesianRidge.coef_})\n\n\n\n\n\n\n\n\nfeatures\nlinear regression\nlassoCV\nbayesianRidge\n\n\n\n\n0\nAge_08_04\n-140.748761\n-140.370575\n-139.754059\n\n\n1\nKM\n-0.017840\n-0.017667\n-0.018131\n\n\n2\nHP\n36.103419\n33.867404\n35.856074\n\n\n3\nMet_Color\n84.281830\n0.000000\n85.088966\n\n\n4\nAutomatic\n416.781954\n69.439343\n408.599781\n\n\n5\nCC\n0.017737\n0.000000\n0.020405\n\n\n6\nDoors\n-50.657863\n0.000000\n-47.917629\n\n\n7\nQuarterly_Tax\n13.625325\n2.709135\n13.269979\n\n\n8\nWeight\n13.038711\n12.434260\n13.114412\n\n\n9\nFuel_Type_Diesel\n1066.464681\n-0.000000\n955.581484\n\n\n10\nFuel_Type_Petrol\n2310.249543\n0.000000\n2162.115763"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 06 - Multiple linear regression.html#table-6.10",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 06 - Multiple linear regression.html#table-6.10",
    "title": "Chapter 6: Multiple Linear Regression",
    "section": "Table 6.10",
    "text": "Table 6.10\n\n# run a linear regression of Price on the remaining 11 predictors in the training set\ntrain_df = train_X.join(train_y)\n\npredictors = train_X.columns\nformula = 'Price ~ ' + ' + '.join(predictors)\n\ncar_lm = sm.ols(formula=formula, data=train_df).fit()\nprint(car_lm.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  Price   R-squared:                       0.856\nModel:                            OLS   Adj. R-squared:                  0.854\nMethod:                 Least Squares   F-statistic:                     319.0\nDate:                Thu, 27 May 2021   Prob (F-statistic):          1.73e-239\nTime:                        20:03:03   Log-Likelihood:                -5198.1\nNo. Observations:                 600   AIC:                         1.042e+04\nDf Residuals:                     588   BIC:                         1.047e+04\nDf Model:                          11                                         \nCovariance Type:            nonrobust                                         \n====================================================================================\n                       coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------------\nIntercept        -1319.3544   1728.427     -0.763      0.446   -4713.997    2075.288\nAge_08_04         -140.7488      5.142    -27.374      0.000    -150.847    -130.650\nKM                  -0.0178      0.002     -7.286      0.000      -0.023      -0.013\nHP                  36.1034      5.321      6.785      0.000      25.653      46.554\nMet_Color           84.2818    127.005      0.664      0.507    -165.158     333.721\nAutomatic          416.7820    259.794      1.604      0.109     -93.454     927.018\nCC                   0.0177      0.099      0.179      0.858      -0.177       0.213\nDoors              -50.6579     65.187     -0.777      0.437    -178.686      77.371\nQuarterly_Tax       13.6253      2.518      5.411      0.000       8.680      18.571\nWeight              13.0387      1.602      8.140      0.000       9.893      16.185\nFuel_Type_Diesel  1066.4647    527.285      2.023      0.044      30.872    2102.057\nFuel_Type_Petrol  2310.2495    521.045      4.434      0.000    1286.914    3333.585\n==============================================================================\nOmnibus:                       62.422   Durbin-Watson:                   1.899\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              366.046\nSkew:                           0.186   Prob(JB):                     3.27e-80\nKurtosis:                       6.808   Cond. No.                     2.20e+06\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 2.2e+06. This might indicate that there are\nstrong multicollinearity or other numerical problems."
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 02 - Overview.html",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 02 - Overview.html",
    "title": "Chapter 2: Overview of the Data Mining Process",
    "section": "",
    "text": "2019 Galit Shmueli, Peter C. Bruce, Peter Gedeck\n\nCode included in\nData Mining for Business Analytics: Concepts, Techniques, and Applications in Python (First Edition) Galit Shmueli, Peter C. Bruce, Peter Gedeck, and Nitin R. Patel. 2019."
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 02 - Overview.html#import-required-packages",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 02 - Overview.html#import-required-packages",
    "title": "Chapter 2: Overview of the Data Mining Process",
    "section": "Import required packages",
    "text": "Import required packages\nWe use the pandas, the Python data anlysis library, for handling data. The API of this library is very similar to R data frames. See https://pandas.pydata.org/ for details.\n\n%matplotlib inline\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nfrom sklearn.linear_model import LinearRegression\nimport dmba\n\nimport matplotlib.pylab as plt\n\nno display found. Using non-interactive Agg backend\n\n\nNote that we import some of the packages using an alias notation, to make the code more readable.\nimport &lt;package&gt; as &lt;alias&gt;\nThe aliases np, pd, and plt are commonly used in the data science community.\nThe following statement defines the location of the data files in your file system. Here we assume that the data folder is located in the parent folder of this ipython notebook."
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 02 - Overview.html#table-2.3",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 02 - Overview.html#table-2.3",
    "title": "Chapter 2: Overview of the Data Mining Process",
    "section": "Table 2.3",
    "text": "Table 2.3\nLoad the West Roxbury data set\n\nhousing_df = dmba.load_data('WestRoxbury.csv')\n\nDetermine the shape of the data frame. It has 5802 rows and 14 columns\n\nhousing_df.shape\n\n(5802, 14)\n\n\nShow the top rows of the dataframe\n\nhousing_df.head()\n\n\n\n\n\n\n\n\nTOTAL VALUE\nTAX\nLOT SQFT\nYR BUILT\nGROSS AREA\nLIVING AREA\nFLOORS\nROOMS\nBEDROOMS\nFULL BATH\nHALF BATH\nKITCHEN\nFIREPLACE\nREMODEL\n\n\n\n\n0\n344.2\n4330\n9965\n1880\n2436\n1352\n2.0\n6\n3\n1\n1\n1\n0\nNone\n\n\n1\n412.6\n5190\n6590\n1945\n3108\n1976\n2.0\n10\n4\n2\n1\n1\n0\nRecent\n\n\n2\n330.1\n4152\n7500\n1890\n2294\n1371\n2.0\n8\n4\n1\n1\n1\n0\nNone\n\n\n3\n498.6\n6272\n13773\n1957\n5032\n2608\n1.0\n9\n5\n1\n1\n1\n1\nNone\n\n\n4\n331.5\n4170\n5000\n1910\n2370\n1438\n2.0\n7\n3\n2\n0\n1\n0\nNone"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 02 - Overview.html#cleanup",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 02 - Overview.html#cleanup",
    "title": "Chapter 2: Overview of the Data Mining Process",
    "section": "Cleanup",
    "text": "Cleanup\nPreprocessing and cleaning up data is an important aspect of data analysis.\nShow the column names.\n\nhousing_df.columns\n\nIndex(['TOTAL VALUE ', 'TAX', 'LOT SQFT ', 'YR BUILT', 'GROSS AREA ',\n       'LIVING AREA', 'FLOORS ', 'ROOMS', 'BEDROOMS ', 'FULL BATH',\n       'HALF BATH', 'KITCHEN', 'FIREPLACE', 'REMODEL'],\n      dtype='object')\n\n\nNote that some column titles end with spaces and some consist of two space separated words. For further analysis it’s more convenient to have column names which are single words.\nIn the rename command you can specify individual columns by name and provide their new name using a dictionary. Note that we use the inplace argument here. This means that the data frame is modified directly. By default, the modification is done on a copy and the copy returned by the method.\n\nhousing_df = housing_df.rename(columns={'TOTAL VALUE ': 'TOTAL_VALUE'})\nhousing_df.columns\n\nIndex(['TOTAL_VALUE', 'TAX', 'LOT SQFT ', 'YR BUILT', 'GROSS AREA ',\n       'LIVING AREA', 'FLOORS ', 'ROOMS', 'BEDROOMS ', 'FULL BATH',\n       'HALF BATH', 'KITCHEN', 'FIREPLACE', 'REMODEL'],\n      dtype='object')\n\n\nWe therefore strip trailing spaces and replace the remaining spaces with an underscore _. Instead of using the rename method, we create a modified copy of columns and assign to the columns field of the dataframe.\n\nhousing_df.columns = [s.strip().replace(' ', '_') for s in housing_df.columns]\nhousing_df.columns\n\nIndex(['TOTAL_VALUE', 'TAX', 'LOT_SQFT', 'YR_BUILT', 'GROSS_AREA',\n       'LIVING_AREA', 'FLOORS', 'ROOMS', 'BEDROOMS', 'FULL_BATH', 'HALF_BATH',\n       'KITCHEN', 'FIREPLACE', 'REMODEL'],\n      dtype='object')"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 02 - Overview.html#accessing-subsets-of-the-data",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 02 - Overview.html#accessing-subsets-of-the-data",
    "title": "Chapter 2: Overview of the Data Mining Process",
    "section": "Accessing subsets of the data",
    "text": "Accessing subsets of the data\nPandas uses two methods to access rows in a data frame; loc and iloc. The loc method is more general and allows accessing rows using labels. The iloc method on the other hand only allows using integer numbers. To specify a range of rows use the slice notation, e.g. 0:9.\n\nNote that in contrast to R, Python uses 0-indexing, which means that indices start at 0 and not at 1.\n\nTo show the first four rows of the data frame, you can use the following commands.\n\nhousing_df.loc[0:3]  # for loc, the second index in the slice is inclusive\n\n\n\n\n\n\n\n\nTOTAL_VALUE\nTAX\nLOT_SQFT\nYR_BUILT\nGROSS_AREA\nLIVING_AREA\nFLOORS\nROOMS\nBEDROOMS\nFULL_BATH\nHALF_BATH\nKITCHEN\nFIREPLACE\nREMODEL\n\n\n\n\n0\n344.2\n4330\n9965\n1880\n2436\n1352\n2.0\n6\n3\n1\n1\n1\n0\nNone\n\n\n1\n412.6\n5190\n6590\n1945\n3108\n1976\n2.0\n10\n4\n2\n1\n1\n0\nRecent\n\n\n2\n330.1\n4152\n7500\n1890\n2294\n1371\n2.0\n8\n4\n1\n1\n1\n0\nNone\n\n\n3\n498.6\n6272\n13773\n1957\n5032\n2608\n1.0\n9\n5\n1\n1\n1\n1\nNone\n\n\n\n\n\n\n\n\nhousing_df.iloc[0:4]  # for loc, the second index in the slice is exclusive\n\n\n\n\n\n\n\n\nTOTAL_VALUE\nTAX\nLOT_SQFT\nYR_BUILT\nGROSS_AREA\nLIVING_AREA\nFLOORS\nROOMS\nBEDROOMS\nFULL_BATH\nHALF_BATH\nKITCHEN\nFIREPLACE\nREMODEL\n\n\n\n\n0\n344.2\n4330\n9965\n1880\n2436\n1352\n2.0\n6\n3\n1\n1\n1\n0\nNone\n\n\n1\n412.6\n5190\n6590\n1945\n3108\n1976\n2.0\n10\n4\n2\n1\n1\n0\nRecent\n\n\n2\n330.1\n4152\n7500\n1890\n2294\n1371\n2.0\n8\n4\n1\n1\n1\n0\nNone\n\n\n3\n498.6\n6272\n13773\n1957\n5032\n2608\n1.0\n9\n5\n1\n1\n1\n1\nNone\n\n\n\n\n\n\n\nNote the difference in the two methods with respect to the slice notation! For consistency with how slices are defined in Python, we will use the iloc method mostly from here on.\nNext, show the first ten rows of the first column\n\nhousing_df['TOTAL_VALUE'].iloc[0:10]\nhousing_df.iloc[0:10]['TOTAL_VALUE']  # the order is not important\nhousing_df.iloc[0:10].TOTAL_VALUE\n\n0    344.2\n1    412.6\n2    330.1\n3    498.6\n4    331.5\n5    337.4\n6    359.4\n7    320.4\n8    333.5\n9    409.4\nName: TOTAL_VALUE, dtype: float64\n\n\nShow the fifth row of the first 10 columns. The iloc methods allows specifying the rows and columns within one set of brackets. dataframe.iloc[rows, columns]\n\nhousing_df.iloc[4][0:10]\nhousing_df.iloc[4, 0:10]  # this is equivalent\n\nTOTAL_VALUE    331.5\nTAX             4170\nLOT_SQFT        5000\nYR_BUILT        1910\nGROSS_AREA      2370\nLIVING_AREA     1438\nFLOORS           2.0\nROOMS              7\nBEDROOMS           3\nFULL_BATH          2\nName: 4, dtype: object\n\n\nIf you prefer to preserve the data frame format, use a slice for the rows as well.\n\nhousing_df.iloc[4:5, 0:10]\n\n\n\n\n\n\n\n\nTOTAL_VALUE\nTAX\nLOT_SQFT\nYR_BUILT\nGROSS_AREA\nLIVING_AREA\nFLOORS\nROOMS\nBEDROOMS\nFULL_BATH\n\n\n\n\n4\n331.5\n4170\n5000\n1910\n2370\n1438\n2.0\n7\n3\n2\n\n\n\n\n\n\n\nUse the pd.concat method if you want to combine non-consecutive columns into a new data frame. The axis argument specifies the dimension along which the concatenation happens, 0=rows, 1=columns.\n\npd.concat([housing_df.iloc[4:6,0:2], housing_df.iloc[4:6,4:6]], axis=1)\n\n\n\n\n\n\n\n\nTOTAL_VALUE\nTAX\nGROSS_AREA\nLIVING_AREA\n\n\n\n\n4\n331.5\n4170\n2370\n1438\n\n\n5\n337.4\n4244\n2124\n1060\n\n\n\n\n\n\n\nTo specify a full column, use the : on its own.\nhousing_df.iloc[:,0:1]\nA often more practical way is to use the column name as follows\n\nhousing_df['TOTAL_VALUE']\n\n0       344.2\n1       412.6\n2       330.1\n3       498.6\n4       331.5\n        ...  \n5797    404.8\n5798    407.9\n5799    406.5\n5800    308.7\n5801    447.6\nName: TOTAL_VALUE, Length: 5802, dtype: float64\n\n\nWe can subset the column using a slice\n\nhousing_df['TOTAL_VALUE'][0:10]\n\n0    344.2\n1    412.6\n2    330.1\n3    498.6\n4    331.5\n5    337.4\n6    359.4\n7    320.4\n8    333.5\n9    409.4\nName: TOTAL_VALUE, dtype: float64\n\n\nPandas provides a number of ways to access statistics of the columns.\n\nprint('Number of rows ', len(housing_df['TOTAL_VALUE']))\nprint('Mean of TOTAL_VALUE ', housing_df['TOTAL_VALUE'].mean())\n\nNumber of rows  5802\nMean of TOTAL_VALUE  392.6857149258877\n\n\nA data frame also has the method describe that prints a number of common statistics\n\nhousing_df['TOTAL_VALUE'].describe()\n\ncount    5802.000000\nmean      392.685715\nstd        99.177414\nmin       105.000000\n25%       325.125000\n50%       375.900000\n75%       438.775000\nmax      1217.800000\nName: TOTAL_VALUE, dtype: float64\n\n\n\nhousing_df.describe()\n\n\n\n\n\n\n\n\nTOTAL_VALUE\nTAX\nLOT_SQFT\nYR_BUILT\nGROSS_AREA\nLIVING_AREA\nFLOORS\nROOMS\nBEDROOMS\nFULL_BATH\nHALF_BATH\nKITCHEN\nFIREPLACE\n\n\n\n\ncount\n5802.000000\n5802.000000\n5802.000000\n5802.000000\n5802.000000\n5802.000000\n5802.000000\n5802.000000\n5802.000000\n5802.000000\n5802.000000\n5802.00000\n5802.000000\n\n\nmean\n392.685715\n4939.485867\n6278.083764\n1936.744916\n2924.842123\n1657.065322\n1.683730\n6.994829\n3.230093\n1.296794\n0.613926\n1.01534\n0.739917\n\n\nstd\n99.177414\n1247.649118\n2669.707974\n35.989910\n883.984726\n540.456726\n0.444884\n1.437657\n0.846607\n0.522040\n0.533839\n0.12291\n0.565108\n\n\nmin\n105.000000\n1320.000000\n997.000000\n0.000000\n821.000000\n504.000000\n1.000000\n3.000000\n1.000000\n1.000000\n0.000000\n1.00000\n0.000000\n\n\n25%\n325.125000\n4089.500000\n4772.000000\n1920.000000\n2347.000000\n1308.000000\n1.000000\n6.000000\n3.000000\n1.000000\n0.000000\n1.00000\n0.000000\n\n\n50%\n375.900000\n4728.000000\n5683.000000\n1935.000000\n2700.000000\n1548.500000\n2.000000\n7.000000\n3.000000\n1.000000\n1.000000\n1.00000\n1.000000\n\n\n75%\n438.775000\n5519.500000\n7022.250000\n1955.000000\n3239.000000\n1873.750000\n2.000000\n8.000000\n4.000000\n2.000000\n1.000000\n1.00000\n1.000000\n\n\nmax\n1217.800000\n15319.000000\n46411.000000\n2011.000000\n8154.000000\n5289.000000\n3.000000\n14.000000\n9.000000\n5.000000\n3.000000\n2.00000\n4.000000"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 02 - Overview.html#table-2.4",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 02 - Overview.html#table-2.4",
    "title": "Chapter 2: Overview of the Data Mining Process",
    "section": "Table 2.4",
    "text": "Table 2.4\nUse the sample method to retrieve a random sample of observations. Here we sample 5 observations without replacement.\n\nhousing_df.sample(5)\n\n\n\n\n\n\n\n\nTOTAL_VALUE\nTAX\nLOT_SQFT\nYR_BUILT\nGROSS_AREA\nLIVING_AREA\nFLOORS\nROOMS\nBEDROOMS\nFULL_BATH\nHALF_BATH\nKITCHEN\nFIREPLACE\nREMODEL\n\n\n\n\n1144\n299.0\n3761\n6343\n1950\n2694\n1201\n1.0\n4\n2\n1\n1\n1\n0\nNone\n\n\n320\n440.2\n5537\n8247\n1940\n3210\n1980\n2.0\n7\n3\n1\n0\n1\n1\nRecent\n\n\n2122\n498.0\n6264\n7375\n1937\n3243\n1896\n2.0\n6\n3\n1\n1\n1\n1\nRecent\n\n\n3062\n419.1\n5272\n5202\n1925\n3983\n1529\n1.0\n8\n4\n1\n1\n1\n1\nRecent\n\n\n4235\n602.7\n7581\n12905\n1915\n3812\n2087\n2.5\n7\n4\n1\n1\n1\n0\nNone\n\n\n\n\n\n\n\nThe sample method allows to specify weights for the individual rows. We use this here to oversample houses with over 10 rooms.\n\nweights = [0.9 if rooms &gt; 10 else 0.01 for rooms in housing_df.ROOMS]\nhousing_df.sample(5, weights=weights)\n\n\n\n\n\n\n\n\nTOTAL_VALUE\nTAX\nLOT_SQFT\nYR_BUILT\nGROSS_AREA\nLIVING_AREA\nFLOORS\nROOMS\nBEDROOMS\nFULL_BATH\nHALF_BATH\nKITCHEN\nFIREPLACE\nREMODEL\n\n\n\n\n5273\n294.7\n3707\n10551\n1945\n1092\n999\n1.0\n4\n1\n1\n0\n1\n1\nNone\n\n\n2866\n337.1\n4240\n3687\n1939\n2247\n1349\n2.0\n8\n3\n1\n1\n1\n1\nNone\n\n\n2283\n444.6\n5593\n8709\n1956\n3837\n1718\n1.0\n7\n3\n1\n1\n1\n1\nNone\n\n\n2687\n397.1\n4995\n5000\n1910\n3014\n1878\n2.0\n11\n6\n2\n0\n1\n1\nNone\n\n\n4680\n571.3\n7186\n7981\n1886\n4580\n2480\n2.0\n11\n6\n2\n1\n1\n1\nNone"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 02 - Overview.html#table-2.5",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 02 - Overview.html#table-2.5",
    "title": "Chapter 2: Overview of the Data Mining Process",
    "section": "Table 2.5",
    "text": "Table 2.5\n\nhousing_df.columns\n\nIndex(['TOTAL_VALUE', 'TAX', 'LOT_SQFT', 'YR_BUILT', 'GROSS_AREA',\n       'LIVING_AREA', 'FLOORS', 'ROOMS', 'BEDROOMS', 'FULL_BATH', 'HALF_BATH',\n       'KITCHEN', 'FIREPLACE', 'REMODEL'],\n      dtype='object')\n\n\nThe REMODEL column is a factor, so we need to change it’s type.\n\nprint(housing_df.REMODEL.dtype)\nhousing_df.REMODEL = housing_df.REMODEL.astype('category')\nprint(housing_df.REMODEL.cat.categories)  # It can take one of three levels\nprint(housing_df.REMODEL.dtype)  # Type is now 'category'\n\nobject\nIndex(['None', 'Old', 'Recent'], dtype='object')\ncategory\n\n\nOther columns also have types.\n\nprint(housing_df.BEDROOMS.dtype)  # BEDROOMS is an integer variable\nprint(housing_df.TOTAL_VALUE.dtype)  # Total_Value is a numeric variable\n\nint64\nfloat64\n\n\nIt’s also possible to the all columns data types\n\nhousing_df.dtypes\n\nTOTAL_VALUE     float64\nTAX               int64\nLOT_SQFT          int64\nYR_BUILT          int64\nGROSS_AREA        int64\nLIVING_AREA       int64\nFLOORS          float64\nROOMS             int64\nBEDROOMS          int64\nFULL_BATH         int64\nHALF_BATH         int64\nKITCHEN           int64\nFIREPLACE         int64\nREMODEL        category\ndtype: object"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 02 - Overview.html#table-2.6",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 02 - Overview.html#table-2.6",
    "title": "Chapter 2: Overview of the Data Mining Process",
    "section": "Table 2.6",
    "text": "Table 2.6\nPandas provides a method to convert factors into dummy variables.\n\nhousing_df = pd.get_dummies(housing_df, prefix_sep='_', drop_first=True)\nhousing_df.columns\n\nIndex(['TOTAL_VALUE', 'TAX', 'LOT_SQFT', 'YR_BUILT', 'GROSS_AREA',\n       'LIVING_AREA', 'FLOORS', 'ROOMS', 'BEDROOMS', 'FULL_BATH', 'HALF_BATH',\n       'KITCHEN', 'FIREPLACE', 'REMODEL_Old', 'REMODEL_Recent'],\n      dtype='object')\n\n\n\nprint(housing_df.loc[:, 'REMODEL_Old':'REMODEL_Recent'].head(5))\n\n   REMODEL_Old  REMODEL_Recent\n0            0               0\n1            0               1\n2            0               0\n3            0               0\n4            0               0"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 02 - Overview.html#table-2.7",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 02 - Overview.html#table-2.7",
    "title": "Chapter 2: Overview of the Data Mining Process",
    "section": "Table 2.7",
    "text": "Table 2.7\nTo illustrate missing data procedures, we first convert a few entries for bedrooms to NA’s. Then we impute these missing values using the median of the remaining values.\n\nprint('Number of rows with valid BEDROOMS values before: ', \n      housing_df['BEDROOMS'].count()) \nmissingRows = housing_df.sample(10).index\nhousing_df.loc[missingRows, 'BEDROOMS'] = np.nan\nprint('Number of rows with valid BEDROOMS values after setting to NAN: ', \n      housing_df['BEDROOMS'].count()) \nhousing_df['BEDROOMS'].count()\n\nNumber of rows with valid BEDROOMS values before:  5802\nNumber of rows with valid BEDROOMS values after setting to NAN:  5792\n\n\n5792\n\n\n\n# remove rows with missing values \nreduced_df = housing_df.dropna()\nprint('Number of rows after removing rows with missing values: ', len(reduced_df))\n\nNumber of rows after removing rows with missing values:  5792\n\n\nReplace the missing values using the median of the remaining values.\nBy default, the median method of a pandas dataframe ignores NA values. This is in contrast to R where this must be specified explicitly.\n\nmedianBedrooms = housing_df['BEDROOMS'].median()\nhousing_df.BEDROOMS = housing_df.BEDROOMS.fillna(value=medianBedrooms)\nprint('Number of rows with valid BEDROOMS values after filling NA values: ',\n      housing_df['BEDROOMS'].count())\n\nNumber of rows with valid BEDROOMS values after filling NA values:  5802"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 02 - Overview.html#table---scaling-data",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 02 - Overview.html#table---scaling-data",
    "title": "Chapter 2: Overview of the Data Mining Process",
    "section": "Table - scaling data",
    "text": "Table - scaling data\n\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\ndf = housing_df.copy()\n\n# Normalizing a data frame\n\n# pandas:\nnorm_df = (housing_df - housing_df.mean()) / housing_df.std()\n\n# scikit-learn: \nscaler = StandardScaler()\nnorm_df = pd.DataFrame(scaler.fit_transform(housing_df), \n                       index=housing_df.index, columns=housing_df.columns)\n# the result of the transformation is a numpy array, we convert it into a dataframe\n\n# Rescaling a data frame\n# pandas:\nrescaled_df = (housing_df - housing_df.min()) / (housing_df.max() - housing_df.min())\n\n# scikit-learn:\nscaler = MinMaxScaler()\nrescaled_df = pd.DataFrame(scaler.fit_transform(housing_df), \n                       index=housing_df.index, columns=housing_df.columns)\n\nThe standardization of the dataset may give a DataConversionWarning. This informs you that the integer columns in the dataframe are automatically converted to real numbers (float64). This is expected and you can therefore ignore this warning. If you want to suppress the warning, you can explicitly convert the integer columns to real numbers\n\n# Option 1: Identify all integer columns, remove personal loan, \n# and change their type\nintColumns = [c for c in housing_df.columns if housing_df[c].dtype == 'int']\nhousing_df[intColumns] = housing_df[intColumns].astype('float64')\n\nAlternatively, you can suppress the warning as follows:\n# Option 2: use the warnings package to suppress the display of the warning\nimport warnings\nwith warnings.catch_warnings():\n    warnings.simplefilter('ignore')\n    norm_df = pd.DataFrame(scaler.fit_transform(housing_df), \n                       index=housing_df.index, columns=housing_df.columns)"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 02 - Overview.html#table-2.9",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 02 - Overview.html#table-2.9",
    "title": "Chapter 2: Overview of the Data Mining Process",
    "section": "Table 2.9",
    "text": "Table 2.9\nSplit the dataset into training (60%) and validation (40%) sets. Randomly sample 60% of the dataset into a new data frame trainData. The remaining 40% serve as validation.\n\n# random_state is set to a defined value to get the same partitions when re-running the code\ntrainData= housing_df.sample(frac=0.6, random_state=1)\n# assign rows that are not already in the training set, into validation \nvalidData = housing_df.drop(trainData.index)\n\nprint('Training   : ', trainData.shape)\nprint('Validation : ', validData.shape)\nprint()\n\n# alternative way using scikit-learn\ntrainData, validData = train_test_split(housing_df, test_size=0.40, random_state=1)\nprint('Training   : ', trainData.shape)\nprint('Validation : ', validData.shape)\n\nTraining   :  (3481, 15)\nValidation :  (2321, 15)\n\nTraining   :  (3481, 15)\nValidation :  (2321, 15)\n\n\nPartition the dataset into training (50%), validation (30%), and test sets (20%).\n\n# randomly sample 50% of the row IDs for training\ntrainData = housing_df.sample(frac=0.5, random_state=1)\n# sample 30% of the row IDs into the validation set, drawing only from records\n# not already in the training set; 60% of 50% is 30%\nvalidData = housing_df.drop(trainData.index).sample(frac=0.6, random_state=1)  \n# the remaining 20% rows serve as test\ntestData = housing_df.drop(trainData.index).drop(validData.index)\n\nprint('Training   : ', trainData.shape)\nprint('Validation : ', validData.shape)\nprint('Test       : ', testData.shape)\nprint()\n\n# alternative way using scikit-learn\ntrainData, temp = train_test_split(housing_df, test_size=0.5, random_state=1)\nvalidData, testData = train_test_split(temp, test_size=0.4, random_state=1)\nprint('Training   : ', trainData.shape)\nprint('Validation : ', validData.shape)\nprint('Test       : ', testData.shape)\n\nTraining   :  (2901, 15)\nValidation :  (1741, 15)\nTest       :  (1160, 15)\n\nTraining   :  (2901, 15)\nValidation :  (1740, 15)\nTest       :  (1161, 15)"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 02 - Overview.html#table-2.11",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 02 - Overview.html#table-2.11",
    "title": "Chapter 2: Overview of the Data Mining Process",
    "section": "Table 2.11",
    "text": "Table 2.11\nThe statsmodels package allows to define linear regression models using a formula definition similar to R. In contrast to R, all variables need to be specified explicitly. We construct a formula excluding the dependent variable and the TAX column\n\n# Data loading and preprocessing\nhousing_df = dmba.load_data('WestRoxbury.csv')\nhousing_df.columns = [s.strip().replace(' ', '_') for s in housing_df.columns]\nhousing_df = pd.get_dummies(housing_df, prefix_sep='_', drop_first=True)\n\nexcludeColumns = ('TOTAL_VALUE', 'TAX')\npredictors = [s for s in housing_df.columns if s not in excludeColumns]\noutcome = 'TOTAL_VALUE'\n\n# partition data\nX = housing_df[predictors]\ny = housing_df[outcome]\ntrain_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size=0.4, random_state=1)\n\nmodel = LinearRegression()\nmodel.fit(train_X, train_y)\n\ntrain_pred = model.predict(train_X)\ntrain_results = pd.DataFrame({\n    'TOTAL_VALUE': train_y, \n    'predicted': train_pred, \n    'residual': train_y - train_pred\n})\nprint(train_results.head())\n\n      TOTAL_VALUE   predicted   residual\n2024        392.0  387.726258   4.273742\n5140        476.3  430.785540  45.514460\n5259        367.4  384.042952 -16.642952\n421         350.3  369.005551 -18.705551\n1401        348.1  314.725722  33.374278\n\n\n\nplt.plot(train_results.TOTAL_VALUE, train_results.predicted, '.')\nplt.xlabel('actual')  # set x-axis label\nplt.ylabel('predicted')  # set y-axis label\naxes = plt.gca()\nplt.plot(axes.get_xlim(), axes.get_xlim(), '--')\nplt.show()\n\nPredict the validation data\n\nvalid_pred = model.predict(valid_X)\nvalid_results = pd.DataFrame({\n    'TOTAL_VALUE': valid_y, \n    'predicted': valid_pred, \n    'residual': valid_y - valid_pred\n})\nprint(valid_results.head())\n\n      TOTAL_VALUE   predicted   residual\n1822        462.0  406.946377  55.053623\n1998        370.4  362.888928   7.511072\n5126        407.4  390.287208  17.112792\n808         316.1  382.470203 -66.370203\n4034        393.2  434.334998 -41.134998\n\n\n\nplt.plot(valid_results.TOTAL_VALUE, valid_results.predicted, '.')\nplt.xlabel('actual')  # set x-axis label\nplt.ylabel('predicted')  # set y-axis label\naxes = plt.gca()\nplt.plot(axes.get_xlim(), axes.get_xlim(), '--')\nplt.show()"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 02 - Overview.html#table-2.13",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 02 - Overview.html#table-2.13",
    "title": "Chapter 2: Overview of the Data Mining Process",
    "section": "Table 2.13",
    "text": "Table 2.13\nWe can use the metrics that scikit-learn provides.\n\nprint('Training set r2: ', r2_score(train_results.TOTAL_VALUE, train_results.predicted))\nprint('Validation set r2: ', r2_score(valid_results.TOTAL_VALUE, valid_results.predicted))\n\nTraining set r2:  0.8097361461091853\nValidation set r2:  0.8171327286147876\n\n\n\n# import the utility function regressionSummary\nfrom dmba import regressionSummary\n\n# training set\nregressionSummary(train_results.TOTAL_VALUE, train_results.predicted)\n\n# validation set\nregressionSummary(valid_results.TOTAL_VALUE, valid_results.predicted)\n\n\nRegression statistics\n\n                      Mean Error (ME) : 0.0000\n       Root Mean Squared Error (RMSE) : 43.0306\n            Mean Absolute Error (MAE) : 32.6042\n          Mean Percentage Error (MPE) : -1.1116\nMean Absolute Percentage Error (MAPE) : 8.4886\n\nRegression statistics\n\n                      Mean Error (ME) : -0.1463\n       Root Mean Squared Error (RMSE) : 42.7292\n            Mean Absolute Error (MAE) : 31.9663\n          Mean Percentage Error (MPE) : -1.0884\nMean Absolute Percentage Error (MAPE) : 8.3283"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 02 - Overview.html#table-2.14",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 02 - Overview.html#table-2.14",
    "title": "Chapter 2: Overview of the Data Mining Process",
    "section": "Table 2.14",
    "text": "Table 2.14\n\nnew_data = pd.DataFrame({\n    'LOT_SQFT': [4200, 6444, 5035],\n    'YR_BUILT': [1960, 1940, 1925],\n    'GROSS_AREA': [2670, 2886, 3264],\n    'LIVING_AREA': [1710, 1474, 1523],\n    'FLOORS': [2.0, 1.5, 1.9],\n    'ROOMS': [10, 6, 6],\n    'BEDROOMS': [4, 3, 2],\n    'FULL_BATH': [1, 1, 1],\n    'HALF_BATH': [1, 1, 0],\n    'KITCHEN': [1, 1, 1],\n    'FIREPLACE': [1, 1, 0],\n    'REMODEL_Old': [0, 0, 0],\n    'REMODEL_Recent': [0, 0, 1],\n})\nprint(new_data)\n\nprint('Predictions: ', model.predict(new_data))\n\n   LOT_SQFT  YR_BUILT  GROSS_AREA  LIVING_AREA  FLOORS  ROOMS  BEDROOMS  \\\n0      4200      1960        2670         1710     2.0     10         4   \n1      6444      1940        2886         1474     1.5      6         3   \n2      5035      1925        3264         1523     1.9      6         2   \n\n   FULL_BATH  HALF_BATH  KITCHEN  FIREPLACE  REMODEL_Old  REMODEL_Recent  \n0          1          1        1          1            0               0  \n1          1          1        1          1            0               0  \n2          1          0        1          0            0               1  \nPredictions:  [384.47210285 378.06696706 386.01773842]"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 02 - Overview.html#figure-2.2-and-2.3",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 02 - Overview.html#figure-2.2-and-2.3",
    "title": "Chapter 2: Overview of the Data Mining Process",
    "section": "Figure 2.2 and 2.3",
    "text": "Figure 2.2 and 2.3\n\ndf = pd.DataFrame({'Expenditure': [239, 364,602, 644, 770, 789, 911], \n                   'Revenue': [514, 789, 550, 1386, 1394, 1440, 1354]})\ndf\n\n\n\n\n\n\n\n\nExpenditure\nRevenue\n\n\n\n\n0\n239\n514\n\n\n1\n364\n789\n\n\n2\n602\n550\n\n\n3\n644\n1386\n\n\n4\n770\n1394\n\n\n5\n789\n1440\n\n\n6\n911\n1354\n\n\n\n\n\n\n\n\ndf.plot.scatter(x='Expenditure', y='Revenue', xlim=(0, 1000), ylim=(0, 1600), figsize=(5, 3.5))\nplt.tight_layout()  # Increase the separation between the plots\nplt.show()\n\n\nx = list(df.Expenditure)\ny = list(df.Revenue)\n\nfrom scipy import interpolate\nf = interpolate.PchipInterpolator(x, y)\n\nx_new = np.linspace(x[0], x[-1], 100)\ny_new = [f(xi) for xi in x_new]\n\n\ndf.plot.scatter(x='Expenditure', y='Revenue', xlim=(0, 1000), ylim=(0, 1600), figsize=(5, 3.5))\nplt.plot(x_new, y_new)\nplt.tight_layout()  # Increase the separation between the plots\nplt.show()"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 11 - Neural nets (add-on).html",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 11 - Neural nets (add-on).html",
    "title": "Chapter 11: Neural nets (code to produce additional figures)",
    "section": "",
    "text": "2019 Galit Shmueli, Peter C. Bruce, Peter Gedeck\n\nCode included in\nData Mining for Business Analytics: Concepts, Techniques, and Applications in Python (First Edition) Galit Shmueli, Peter C. Bruce, Peter Gedeck, and Nitin R. Patel. 2019."
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 11 - Neural nets (add-on).html#import-required-packages",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 11 - Neural nets (add-on).html#import-required-packages",
    "title": "Chapter 11: Neural nets (code to produce additional figures)",
    "section": "Import required packages",
    "text": "Import required packages\n\nfrom pathlib import Path\n\nimport pydotplus\nfrom IPython.display import Image\nimport numbers\n\nimport dmba\n\n%matplotlib inline\n\nno display found. Using non-interactive Agg backend\n\n\n\ndef layoutGraph(dot_data, pdfFile=None):\n    graph = pydotplus.graph_from_dot_data(dot_data)\n    return Image(graph.create_png())\n\n\ndef unbiasedNode(id, label=None, output=None, style=None):\n    label = label or f'N{id}'\n    common = style or 'shape=record, style=rounded, color=gray26'\n    if output is None:\n        return f'{id} [ label=\"{label}\", fontsize=11, {common} ];'\n    return f'{id} [ label=\"{{ {label} | {output:.2f} }}\", fontsize=11, {common} ];'\n\ndef biasedNode(id, bias, label=None, output=None):\n    label = label or f'N{id}'\n    if isinstance(bias, numbers.Number):\n        bias = f'{bias:.3f}'\n    if isinstance(output, numbers.Number):\n        output = f'{output:.3f}'\n    common = 'shape=record, style=rounded, fontsize=11, color=gray26'\n    if output is None:\n        return f'{id} [ label=\"{label}&#92;n{bias}\", {common} ];'\n    return f'{id} [ label=\"{{ {label}&#92;n{bias} | {output} }}\", {common} ];'\n\ndef edge(n1, n2, weight=None):\n    if weight is None:\n        return f'{n1} -&gt; {n2} [ label=\"w{n1}{n2}\" ];'\n    return f'{n1} -&gt; {n2} [ label=\"{weight:.3f}\" ];'\n\ncommonSettings = \"\"\"\nedge [ fontsize=11, color=gray55 ];\nrankdir=LR;\n# size=\"10,10\"\ngraph [ranksep=\"0.8\"];\n\"\"\"\n# node [shape=record, style=rounded, fontsize=11, color=gray26]; \n\ns = f\"\"\"\ndigraph ethane {{\n{ commonSettings }\n{ unbiasedNode('Salt', label='Salt', style='color=white') }\n{ unbiasedNode('Fat', label='Fat', style='color=white') }\nsubgraph cluster_input {{ label=\"Input layer\"; color=\"grey75\"; \n    { unbiasedNode(2) }\n    { unbiasedNode(1) }\n}}\nsubgraph cluster_hidden {{ label=\"Hidden layer\"; color=\"grey75\"; \n    { biasedNode(4, '&theta;4')}\n    { biasedNode(3, '&theta;3')}\n    { biasedNode(5, '&theta;5')}\n}}\nsubgraph cluster_output {{ label=\"Output layer\"; color=\"grey75\"; \n    { biasedNode(6, '&theta;6')}\n}}\n{ unbiasedNode('like', label='like', style='color=white') }\nSalt -&gt; 2;\nFat -&gt; 1;\n{ edge(1, 3) }\n{ edge(2, 3) }\n{ edge(1, 4) }\n{ edge(2, 4) }\n{ edge(1, 5) }\n{ edge(2, 5) }\n{ edge(3, 6) }\n{ edge(4, 6) }\n{ edge(5, 6) }\n6 -&gt; like\n }}\n \"\"\"\nlayoutGraph(s, pdfFile='network-layout.pdf')\n\n\n\n\n\n\n\n\n\n# Random network\ns = f\"\"\"\ndigraph ethane {{\n{ commonSettings }\n{ unbiasedNode('Salt', label='Salt', style='color=white') }\n{ unbiasedNode('Fat', label='Fat', style='color=white') }\n{ unbiasedNode(2, output=0.9) }\n{ unbiasedNode(1, output=0.2) }\n{ biasedNode(4, 0.02, output=0.51224755)}\n{ biasedNode(3, -0.3, output=0.42580196)}\n{ biasedNode(5, 0.05, output=0.520238936)}\n{ biasedNode(6, -0.015, output=0.505668252)}\n{ unbiasedNode('like', label='like', style='color=white') }\nSalt -&gt; 2;\nFat -&gt; 1;\n{ edge(1, 3, 0.05) }\n{ edge(2, 3, 0.01) }\n{ edge(1, 4, -0.01) }\n{ edge(2, 4, 0.03) }\n{ edge(1, 5, 0.2) }\n{ edge(2, 5, -0.01) }\n{ edge(3, 6, 0.01) }\n{ edge(4, 6, 0.05) }\n{ edge(5, 6, 0.015) }\n6 -&gt; like\n }}\n \"\"\"\nlayoutGraph(s, pdfFile='network-random.pdf')\n\n\n\n\n\n\n\n\n\n# Optimized network\ns = f\"\"\"\ndigraph ethane {{\n{ commonSettings }\n{ unbiasedNode('Salt', label='Salt', style='color=white') }\n{ unbiasedNode('Fat', label='Fat', style='color=white') }\n{ unbiasedNode(2, output=0.9) }\n{ unbiasedNode(1, output=0.2) }\n{ biasedNode(4, 4.07247552, output=0.232750971)}\n{ biasedNode(3, 0.13368045, output=0.458287289)}\n{ biasedNode(5, 7.00768104, output=0.253183219)}\n{ biasedNode(6, 14.30748676, output=0.99950954)}\n{ unbiasedNode('like', label='like', style='color=white') }\nSalt -&gt; 2;\nFat -&gt; 1;\n{ edge(1, 3, -1.30656481) }\n{ edge(2, 3, -0.04399727) }\n{ edge(1, 4, -4.20427792) }\n{ edge(2, 4, -4.91606924) }\n{ edge(1, 5, -13.29587332) }\n{ edge(2, 5, -6.03356987) }\n{ edge(3, 6, -0.27348313) }\n{ edge(4, 6, -9.01211573) }\n{ edge(5, 6, -17.63504694) }\n6 -&gt; like\n}}\n \"\"\"\nlayoutGraph(s) \n\n\n\n\n\n\n\n\n\n# Optimized network\ns = f\"\"\"\ndigraph ethane {{\n{ commonSettings }\n{ unbiasedNode('Salt', label='Salt', style='color=white') }\n{ unbiasedNode('Fat', label='Fat', style='color=white') }\n{ unbiasedNode(2) }\n{ unbiasedNode(1) }\n{ biasedNode(4, 4.07247552)}\n{ biasedNode(3, 0.13368045)}\n{ biasedNode(5, 7.00768104)}\n{ biasedNode(6, 14.30748676)}\n{ unbiasedNode('like', label='like', style='color=white') }\nSalt -&gt; 2;\nFat -&gt; 1;\n{ edge(1, 3, -1.30656481) }\n{ edge(2, 3, -0.04399727) }\n{ edge(1, 4, -4.20427792) }\n{ edge(2, 4, -4.91606924) }\n{ edge(1, 5, -13.29587332) }\n{ edge(2, 5, -6.03356987) }\n{ edge(3, 6, -0.27348313) }\n{ edge(4, 6, -9.01211573) }\n{ edge(5, 6, -17.63504694) }\n6 -&gt; like\n}}\n \"\"\"\nlayoutGraph(s, pdfFile='network-optimized.pdf') \n\n\n\n\n\n\n\n\n\ndef node(color):\n    return f'[label=\"\", shape=\"circle\", color=\"{color}\", fillcolor=\"{color}\", style=\"filled\"]'\n\ninput = '\\n'.join(f'i{i} {node(\"#A55B60\")}; ' for i in range(4))\nhidden1 = '\\n'.join(f'h1{i} {node(\"#74BEEB\")}; ' for i in range(3))\nhidden2 = '\\n'.join(f'h2{i} {node(\"#74BEEB\")}; ' for i in range(3))\noutput = '\\n'.join(f'o{i} {node(\"#3D92A2\")}; ' for i in range(1))\n\ndef connections(layer1, count1, layer2, count2):\n    s = []\n    for i in range(count1):\n        for j in range(count2):\n            s.append(f'{layer1}{i} -- {layer2}{j} [color=\"gray55\"];')\n    return '\\n'.join(s)\n\ns = f\"\"\"\ngraph {{\n    size=\"10,10\"\n    graph [ranksep=\"1\"];\n    rankdir=LR;\n    splines=line;\n    subgraph cluster_input {{ label=\"Input layer\"; color=\"grey75\"; { input } }}\n    subgraph cluster_hidden1 {{ label=\"Hidden layer 1\"; color=\"grey75\"; { hidden1 } }}\n    subgraph cluster_hidden2 {{ label=\"Hidden layer 2\"; color=\"grey75\"; { hidden2 } }}\n    subgraph cluster_output {{ label=\"Output layer\"; color=\"grey75\"; { output } }}\n    { connections('i', 4, 'h1', 3)}\n    { connections('h1', 3, 'h2', 3)}\n    { connections('h2', 3, 'o', 1)}\n}}\n\"\"\"\nlayoutGraph(s, pdfFile='network-4331.pdf')"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 16 - Handling time series.html",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 16 - Handling time series.html",
    "title": "Chapter 16: Handling Time Series",
    "section": "",
    "text": "2019 Galit Shmueli, Peter C. Bruce, Peter Gedeck\n\nCode included in\nData Mining for Business Analytics: Concepts, Techniques, and Applications in Python (First Edition) Galit Shmueli, Peter C. Bruce, Peter Gedeck, and Nitin R. Patel. 2019."
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 16 - Handling time series.html#import-required-packages",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 16 - Handling time series.html#import-required-packages",
    "title": "Chapter 16: Handling Time Series",
    "section": "Import required packages",
    "text": "Import required packages\n\nfrom pathlib import Path\nimport datetime\n\nimport pandas as pd\nimport matplotlib.pylab as plt\nimport statsmodels.formula.api as sm\nfrom statsmodels.tsa import tsatools\n\nimport dmba\nfrom dmba import regressionSummary\n\n%matplotlib inline\n\nno display found. Using non-interactive Agg backend"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 16 - Handling time series.html#figure-16.1",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 16 - Handling time series.html#figure-16.1",
    "title": "Chapter 16: Handling Time Series",
    "section": "Figure 16.1",
    "text": "Figure 16.1\n\nbaregg_df = dmba.load_data('BareggTunnel.csv')\n\n# convert the date information to a datetime object\nbaregg_df['Date'] = pd.to_datetime(baregg_df.Day, format='%d %b %Y')\nvehicles_ts = pd.Series(baregg_df['Number of vehicles'].values, index=baregg_df.Date, name='Vehicles')\n\nfig, axes = plt.subplots(nrows=2, ncols=1, figsize=(10,6))\nvehicles_ts.plot(ax=axes[0])\naxes[0].set_xlabel('Day')\naxes[0].set_ylabel('Number of vehicles')\nvehicles_ts['2004-02-01':'2004-05-31'].plot(ax=axes[1])\naxes[1].set_xlabel('Day')\naxes[1].set_ylabel('Number of vehicles')\nplt.suptitle('')  # Suppress the overall title\nplt.tight_layout()  # Increase the separation between the plots\n\nplt.show()\n\n\n\n\n\n\n\n\n\nAmtrak_df = dmba.load_data('Amtrak.csv')\n\n# convert the date information to a datetime object\nAmtrak_df['Date'] = pd.to_datetime(Amtrak_df.Month, format='%d/%m/%Y')\n\n# convert dataframe column to series (name is used to label the data)\nridership_ts = pd.Series(Amtrak_df.Ridership.values, index=Amtrak_df.Date, name='Ridership')\n\n# define the time series frequency\nridership_ts.index = pd.DatetimeIndex(ridership_ts.index, freq=ridership_ts.index.inferred_freq)\n\n\nFigure 16.2\n\n# plot the series\nax = ridership_ts.plot()\nax.set_xlabel('Time')\nax.set_ylabel('Ridership (in 000s)')\n_ = ax.set_ylim(1300, 2300)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nFigure 16.3\n\n# create short time series from 1997 to 1999 using a slice \nridership_ts_3yrs = ridership_ts['1997':'1999']\n\n# create a data frame with additional independent variables from time series\n# the following command adds a constant term, a trend term and a quadratic trend term\nridership_df = tsatools.add_trend(ridership_ts, trend='ctt')\n# fit a linear regression model to the time series\nridership_lm = sm.ols(formula='Ridership ~ trend + trend_squared', data=ridership_df).fit()\n\n# shorter and longer time series\nfig, axes = plt.subplots(nrows=2, ncols=1, figsize=(10,6))\nridership_ts_3yrs.plot(ax=axes[0])\nridership_ts.plot(ax=axes[1])\nfor ax in axes:\n    ax.set_xlabel('Time')\n    ax.set_ylabel('Ridership (in 000s)')\n    ax.set_ylim(1300, 2300)\nridership_lm.predict(ridership_df).plot(ax=axes[1])\nplt.show()"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 16 - Handling time series.html#figure-16.4",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 16 - Handling time series.html#figure-16.4",
    "title": "Chapter 16: Handling Time Series",
    "section": "Figure 16.4",
    "text": "Figure 16.4\n\nnValid = 36\nnTrain = len(ridership_ts) - nValid\n\n# partition the data\ntrain_ts = ridership_ts[:nTrain]\nvalid_ts = ridership_ts[nTrain:]\n\n# Generate the naive and seasonal naive forecast\nnaive_pred = pd.Series(train_ts[-1], index=valid_ts.index)\nlast_season = train_ts[-12:]\nseasonal_pred = pd.Series(pd.concat([last_season]*5)[:len(valid_ts)].values, index=valid_ts.index)\n\n\n\n# plot forecasts and actual in the training and validation sets\nax = train_ts.plot(color='C0', linewidth=0.75, figsize=(9,7))\nvalid_ts.plot(ax=ax, color='C0', linestyle='dashed', linewidth=0.75)\n# ax.set_xlim('1990', '2006-06-01')\nax.set_ylim(1300, 2600)\nax.set_xlabel('Time')\nax.set_ylabel('Ridership (in 000s)')\n\nnaive_pred.plot(ax=ax, color='green')\nseasonal_pred.plot(ax=ax, color='orange')\n\n\n# Determine coordinates for drawing the arrows and lines\none_month = pd.Timedelta('31 days')\nxtrain = (min(train_ts.index), max(train_ts.index) - one_month)\nxvalid = (min(valid_ts.index) + one_month, max(valid_ts.index) - one_month)\nxfuture = (max(valid_ts.index) + one_month, max(valid_ts.index) + 20 * one_month)\nxtv = xtrain[1] + 0.5 * (xvalid[0] - xtrain[1])\nxvf = xvalid[1] + 0.5 * (xfuture[0] - xvalid[1])\nax.set_xlim(min(train_ts.index) - 6 * one_month, max(valid_ts.index) + 24 * one_month)\n\nax.add_line(plt.Line2D(xtrain, (2450, 2450), color='black', linewidth=0.5))\nax.add_line(plt.Line2D(xvalid, (2450, 2450), color='black', linewidth=0.5))\nax.add_line(plt.Line2D(xfuture, (2450, 2450), color='black', linewidth=0.5))\nax.text('1995', 2500, 'Training')\nax.text('2001-9', 2500, 'Validation')\nax.text('2004-7', 2500, 'Future')\nax.axvline(x=xtv, ymin=0, ymax=1, color='black', linewidth=0.5)\nax.axvline(x=xvf, ymin=0, ymax=1, color='black', linewidth=0.5)\nplt.show()\n\n\n\n\n\n\n\n\n\nTable 16.1\n\nregressionSummary(valid_ts, naive_pred)\n\n\nRegression statistics\n\n                      Mean Error (ME) : -14.7177\n       Root Mean Squared Error (RMSE) : 142.7551\n            Mean Absolute Error (MAE) : 115.9234\n          Mean Percentage Error (MPE) : -1.2750\nMean Absolute Percentage Error (MAPE) : 6.0214\n\n\n\nregressionSummary(valid_ts, seasonal_pred)\n\n\nRegression statistics\n\n                      Mean Error (ME) : 54.7296\n       Root Mean Squared Error (RMSE) : 95.6243\n            Mean Absolute Error (MAE) : 84.0941\n          Mean Percentage Error (MPE) : 2.6528\nMean Absolute Percentage Error (MAPE) : 4.2477\n\n\n\nprint('naive model in training set')\nregressionSummary(train_ts[1:], train_ts[:-1])\n\nprint('seasonal model in training set')\nregressionSummary(train_ts[12:], train_ts[:-12])\n\nnaive model in training set\n\nRegression statistics\n\n                      Mean Error (ME) : 2.4509\n       Root Mean Squared Error (RMSE) : 168.1470\n            Mean Absolute Error (MAE) : 125.2975\n          Mean Percentage Error (MPE) : -0.3460\nMean Absolute Percentage Error (MAPE) : 7.2714\nseasonal model in training set\n\nRegression statistics\n\n                      Mean Error (ME) : 13.9399\n       Root Mean Squared Error (RMSE) : 99.2656\n            Mean Absolute Error (MAE) : 82.4920\n          Mean Percentage Error (MPE) : 0.5851\nMean Absolute Percentage Error (MAPE) : 4.7153\n\n\n\nnaive_train_ts = pd.Series(train_ts[:-1].values, index=train_ts[1:].index)\nax = train_ts[1:].plot()\nnaive_train_ts.plot(ax=ax)\n\n\n\n\n\n\n\n\n\nseasonal_train_ts = pd.Series(train_ts[:-12].values, index=train_ts[12:].index)\nax = train_ts[12:].plot()\nseasonal_train_ts.plot(ax=ax)\n\n\n\n\n\n\n\n\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nresult = seasonal_decompose(ridership_ts, model='additive')\nresult.plot()\nplt.show()"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 10 - Logistic regression.html",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 10 - Logistic regression.html",
    "title": "Chapter 10: Logistic Regression",
    "section": "",
    "text": "2019 Galit Shmueli, Peter C. Bruce, Peter Gedeck\n\nCode included in\nData Mining for Business Analytics: Concepts, Techniques, and Applications in Python (First Edition) Galit Shmueli, Peter C. Bruce, Peter Gedeck, and Nitin R. Patel. 2019."
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 10 - Logistic regression.html#import-required-packages",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 10 - Logistic regression.html#import-required-packages",
    "title": "Chapter 10: Logistic Regression",
    "section": "Import required packages",
    "text": "Import required packages\n\n%matplotlib inline\n\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nfrom sklearn.model_selection import train_test_split\nimport statsmodels.api as sm\nfrom mord import LogisticIT\nimport matplotlib.pylab as plt\nimport seaborn as sns\nfrom dmba import classificationSummary, gainsChart, liftChart\nfrom dmba.metric import AIC_score"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 10 - Logistic regression.html#table-10.2",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 10 - Logistic regression.html#table-10.2",
    "title": "Chapter 10: Logistic Regression",
    "section": "Table 10.2",
    "text": "Table 10.2\nLoad the UniversalBank.csv dataset. The columns ID and ZIP Code are not relevant for model building and therefore removed. Treat Education as categorical\n\nbank_df = pd.read_csv('UniversalBank.csv')\nbank_df.drop(columns=['ID', 'ZIP Code'], inplace=True)\nbank_df.columns = [c.replace(' ', '_') for c in bank_df.columns]\n\n# Treat education as categorical, convert to dummy variables\nbank_df['Education'] = bank_df['Education'].astype('category')\nnew_categories = {1: 'Undergrad', 2: 'Graduate', 3: 'Advanced/Professional'}\nbank_df.Education.cat.rename_categories(new_categories, inplace=True)\nbank_df = pd.get_dummies(bank_df, prefix_sep='_', drop_first=True)\n\ny = bank_df['Personal_Loan']\nX = bank_df.drop(columns=['Personal_Loan'])\n\n# partition data\ntrain_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size=0.4, random_state=1)\n\n# fit a logistic regression (set penalty=l2 and C=1e42 to avoid regularization)\nlogit_reg = LogisticRegression(penalty=\"l2\", C=1e42, solver='liblinear')\nlogit_reg.fit(train_X, train_y)\n\nprint('intercept ', logit_reg.intercept_[0])\nprint(pd.DataFrame({'coeff': logit_reg.coef_[0]}, index=X.columns).transpose())\nprint()\nprint('AIC', AIC_score(valid_y, logit_reg.predict(valid_X), df = len(train_X.columns) + 1))\n\nintercept  -12.493436061176814\n            Age  Experience    Income    Family     CCAvg  Mortgage  \\\ncoeff -0.037685    0.039202  0.058844  0.612251  0.240489  0.001012   \n\n       Securities_Account  CD_Account    Online  CreditCard  \\\ncoeff            -1.01428    3.649097 -0.678306   -0.958283   \n\n       Education_Graduate  Education_Advanced/Professional  \ncoeff            4.202148                         4.355761  \n\nAIC -709.1524769205962\n\n\n/opt/conda/lib/python3.9/site-packages/pandas/core/arrays/categorical.py:2630: FutureWarning: The `inplace` parameter in pandas.Categorical.rename_categories is deprecated and will be removed in a future version. Removing unused categories will always return a new Categorical object.\n  res = method(*args, **kwargs)\n\n\n\n# Set to True to calculate logistic regression using Income only\nif False:\n    predictors = ['Income']\n    outcome = 'Personal_Loan'\n\n    y = bank_df[outcome]\n    X = bank_df[predictors]\n\n    # partition data\n    train_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size=0.4, random_state=1)\n\n    # fit a logistic regression (set penalty=l2 and C=1e42 to avoid regularization)\n    logit_reg_income = LogisticRegression(penalty=\"l2\", C=1e42, solver='liblinear')\n    logit_reg_income.fit(train_X, train_y)\n\n    print('intercept ', logit_reg_income.intercept_[0])\n    print(pd.DataFrame({'coefficient': logit_reg_income.coef_[0]}, index=X.columns).transpose())\n    print()\n    print('AIC', AIC_score(valid_y, logit_reg_income.predict(valid_X), df = len(train_X.columns) + 1))"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 10 - Logistic regression.html#table-10.3",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 10 - Logistic regression.html#table-10.3",
    "title": "Chapter 10: Logistic Regression",
    "section": "Table 10.3",
    "text": "Table 10.3\nPredict to get the probabilities\n\nlogit_reg_pred = logit_reg.predict(valid_X)\nlogit_reg_proba = logit_reg.predict_proba(valid_X)\nlogit_result = pd.DataFrame({'actual': valid_y, \n                             'p(0)': [p[0] for p in logit_reg_proba],\n                             'p(1)': [p[1] for p in logit_reg_proba],\n                             'predicted': logit_reg_pred })\n\n# display four different cases\ninterestingCases = [2764, 932, 2721, 702]\nprint(logit_result.loc[interestingCases])\n\n      actual      p(0)      p(1)  predicted\n2764       0  0.976092  0.023908          0\n932        0  0.331000  0.669000          1\n2721       1  0.031430  0.968570          1\n702        1  0.985893  0.014107          0"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 10 - Logistic regression.html#table-10.4",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 10 - Logistic regression.html#table-10.4",
    "title": "Chapter 10: Logistic Regression",
    "section": "Table 10.4",
    "text": "Table 10.4\n\nclassificationSummary(train_y, logit_reg.predict(train_X))\nclassificationSummary(valid_y, logit_reg.predict(valid_X))\n\nConfusion Matrix (Accuracy 0.9600)\n\n       Prediction\nActual    0    1\n     0 2683   30\n     1   90  197\nConfusion Matrix (Accuracy 0.9595)\n\n       Prediction\nActual    0    1\n     0 1791   16\n     1   65  128"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 10 - Logistic regression.html#figure-10.3",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 10 - Logistic regression.html#figure-10.3",
    "title": "Chapter 10: Logistic Regression",
    "section": "Figure 10.3",
    "text": "Figure 10.3\n\ndf = logit_result.sort_values(by=['p(1)'], ascending=False)\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 4))\n\ngainsChart(df.actual, ax=axes[0])\nliftChart(df.actual, title=False, ax=axes[1])\n    \nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 10 - Logistic regression.html#figure-10.4",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 10 - Logistic regression.html#figure-10.4",
    "title": "Chapter 10: Logistic Regression",
    "section": "Figure 10.4",
    "text": "Figure 10.4\n\ndelays_df = pd.read_csv('FlightDelays.csv')\n# Create an indicator variable\ndelays_df['isDelayed'] = [1 if status == 'delayed' else 0 for status in delays_df['Flight Status']]\n\n# group information by day of week and determine the average delay\naverageDelay = delays_df.groupby(['DAY_WEEK'])['isDelayed'].mean()\n\n# create a bar chart\nax = averageDelay.plot.bar(color='C0')\nax.set_xlabel('Day of week')\nax.set_ylabel('Average Delay')\n_ = ax.set_xticklabels(['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\n\n\ndelays_df = pd.read_csv('FlightDelays.csv')\n# Create an indicator variable\ndelays_df['isDelayed'] = [1 if status == 'delayed' else 0 \n                          for status in delays_df['Flight Status']]\n\ndef createGraph(group, xlabel, axis):\n    groupAverage = delays_df.groupby([group])['isDelayed'].mean()\n    if group == 'DAY_WEEK': # rotate so that display starts on Sunday\n        groupAverage = groupAverage.reindex(index=np.roll(groupAverage.index,1))\n        groupAverage.index = ['Sun', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat']\n    ax = groupAverage.plot.bar(color='C0', ax=axis)\n    ax.set_ylabel('Average Delay')\n    ax.set_xlabel(xlabel)\n    return ax\n\ndef graphDepartureTime(xlabel, axis):\n    temp_df = pd.DataFrame({'CRS_DEP_TIME': delays_df['CRS_DEP_TIME'] // 100, \n                            'isDelayed': delays_df['isDelayed']})\n    groupAverage = temp_df.groupby(['CRS_DEP_TIME'])['isDelayed'].mean()\n    ax = groupAverage.plot.bar(color='C0', ax=axis)\n    ax.set_xlabel(xlabel); ax.set_ylabel('Average Delay')\n    \nfig, axes = plt.subplots(nrows=3, ncols=2, figsize=(10, 9))\n\ncreateGraph('DAY_WEEK', 'Day of week', axis=axes[0][0])\ncreateGraph('DEST', 'Destination', axis=axes[0][1])\ngraphDepartureTime('Departure time', axis=axes[1][0])\ncreateGraph('CARRIER', 'Carrier', axis=axes[1][1])\ncreateGraph('ORIGIN', 'Origin', axis=axes[2][0])\ncreateGraph('Weather', 'Weather', axis=axes[2][1])\nplt.tight_layout()\n\nplt.show()"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 10 - Logistic regression.html#figure-10.5",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 10 - Logistic regression.html#figure-10.5",
    "title": "Chapter 10: Logistic Regression",
    "section": "Figure 10.5",
    "text": "Figure 10.5\n\nagg = delays_df.groupby(['ORIGIN', 'DAY_WEEK', 'CARRIER']).isDelayed.mean()\nagg = agg.reset_index()\n\n# Define the layout of the graph\nheight_ratios = []\nfor i, origin in enumerate(sorted(delays_df.ORIGIN.unique())):\n    height_ratios.append(len(agg[agg.ORIGIN == origin].CARRIER.unique()))\ngridspec_kw = {'height_ratios': height_ratios, 'width_ratios': [15, 1]}\nfig, axes = plt.subplots(nrows=3, ncols=2, figsize=(10, 6), \n                         gridspec_kw = gridspec_kw)\naxes[0, 1].axis('off')\naxes[2, 1].axis('off')\n\nmaxIsDelay = agg.isDelayed.max()\nfor i, origin in enumerate(sorted(delays_df.ORIGIN.unique())):\n    data = pd.pivot_table(agg[agg.ORIGIN == origin], values='isDelayed', aggfunc=np.sum, \n                          index=['CARRIER'], columns=['DAY_WEEK'])\n    data = data[[7, 1, 2, 3, 4, 5, 6]]  # Shift last columns to first\n    ax = sns.heatmap(data, ax=axes[i][0], vmin=0, vmax=maxIsDelay, \n                     cbar_ax=axes[1][1], cmap=sns.light_palette(\"navy\"))\n    ax.set_xticklabels(['Sun', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat'])\n    if i != 2: \n        ax.get_xaxis().set_visible(False)\n    ax.set_ylabel('Airport ' + origin)\n    \n\nplt.show()"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 10 - Logistic regression.html#table-10.7",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 10 - Logistic regression.html#table-10.7",
    "title": "Chapter 10: Logistic Regression",
    "section": "Table 10.7",
    "text": "Table 10.7\n\ndelays_df = pd.read_csv('FlightDelays.csv')\n# Create an indicator variable\ndelays_df['isDelayed'] = [1 if status == 'delayed' else 0 \n                          for status in delays_df['Flight Status']]\n\n# convert to categorical\ndelays_df.DAY_WEEK = delays_df.DAY_WEEK.astype('category')\n\n# create hourly bins departure time \ndelays_df.CRS_DEP_TIME = [round(t / 100) for t in delays_df.CRS_DEP_TIME]\ndelays_df.CRS_DEP_TIME = delays_df.CRS_DEP_TIME.astype('category')\n\npredictors = ['DAY_WEEK', 'CRS_DEP_TIME', 'ORIGIN', 'DEST', 'CARRIER', 'Weather']\noutcome = 'isDelayed'\n\nX = pd.get_dummies(delays_df[predictors], drop_first=True)\ny = delays_df[outcome]\nclasses = ['ontime', 'delayed']\n\n# split into training and validation\ntrain_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size=0.4, \n                                                      random_state=1)\n\nlogit_full = LogisticRegression(penalty=\"l2\", C=1e42, solver='liblinear')\nlogit_full.fit(train_X, train_y)\n\npd.set_option('display.width', 95)\npd.set_option('precision',3)\npd.set_option('max_columns', 33)\nprint('intercept ', logit_full.intercept_[0])\n\nprint(pd.DataFrame({'coeff': logit_full.coef_[0]}, index=X.columns).transpose())\nprint()\nprint('AIC', AIC_score(valid_y, logit_full.predict(valid_X), df=len(train_X.columns) + 1))\npd.reset_option('display.width')\npd.reset_option('precision')\npd.reset_option('max_columns')\n\nintercept  -1.2191013737953593\n       Weather  DAY_WEEK_2  DAY_WEEK_3  DAY_WEEK_4  DAY_WEEK_5  DAY_WEEK_6  DAY_WEEK_7  \\\ncoeff    9.325      -0.598      -0.705      -0.799      -0.296      -1.129      -0.135   \n\n       CRS_DEP_TIME_7  CRS_DEP_TIME_8  CRS_DEP_TIME_9  CRS_DEP_TIME_10  CRS_DEP_TIME_11  \\\ncoeff           0.631           0.382          -0.365            0.337            0.078   \n\n       CRS_DEP_TIME_12  CRS_DEP_TIME_13  CRS_DEP_TIME_14  CRS_DEP_TIME_15  CRS_DEP_TIME_16  \\\ncoeff            0.399            0.175            0.202            1.265            0.628   \n\n       CRS_DEP_TIME_17  CRS_DEP_TIME_18  CRS_DEP_TIME_19  CRS_DEP_TIME_20  CRS_DEP_TIME_21  \\\ncoeff            1.093            0.285            1.655            1.023            1.077   \n\n       ORIGIN_DCA  ORIGIN_IAD  DEST_JFK  DEST_LGA  CARRIER_DH  CARRIER_DL  CARRIER_MQ  \\\ncoeff       -0.01      -0.134    -0.524    -0.546       0.352      -0.685       0.743   \n\n       CARRIER_OH  CARRIER_RU  CARRIER_UA  CARRIER_US  \ncoeff      -0.711      -0.194       0.315      -0.971  \n\nAIC 1004.5346225948085"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 10 - Logistic regression.html#figure-10.6",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 10 - Logistic regression.html#figure-10.6",
    "title": "Chapter 10: Logistic Regression",
    "section": "Figure 10.6",
    "text": "Figure 10.6\n\nlogit_reg_pred = logit_full.predict_proba(valid_X)\nfull_result = pd.DataFrame({'actual': valid_y, \n                            'p(0)': [p[0] for p in logit_reg_pred],\n                            'p(1)': [p[1] for p in logit_reg_pred],\n                            'predicted': logit_full.predict(valid_X)})\nfull_result = full_result.sort_values(by=['p(1)'], ascending=False)\n\n# confusion matrix\nclassificationSummary(full_result.actual, full_result.predicted, class_names=classes)\n\ngainsChart(full_result.actual, figsize=[5, 5])\n\nplt.show()\n\nConfusion Matrix (Accuracy 0.8309)\n\n        Prediction\n Actual  ontime delayed\n ontime     705       9\ndelayed     140      27"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 10 - Logistic regression.html#table-10.9",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 10 - Logistic regression.html#table-10.9",
    "title": "Chapter 10: Logistic Regression",
    "section": "Table 10.9",
    "text": "Table 10.9\n\ndelays_df = pd.read_csv('FlightDelays.csv')\ndelays_df['isDelayed'] = [1 if status == 'delayed' else 0 \n                          for status in delays_df['Flight Status']]\ndelays_df['CRS_DEP_TIME'] = [round(t / 100) for t in delays_df['CRS_DEP_TIME']]\ndelays_red_df = pd.DataFrame({\n    'Sun_Mon' : [1 if d in (1, 7) else 0 for d in delays_df.DAY_WEEK],\n    'Weather' : delays_df.Weather,\n    'CARRIER_CO_MQ_DH_RU' : [1 if d in (\"CO\", \"MQ\", \"DH\", \"RU\") else 0 \n                             for d in delays_df.CARRIER],\n    'MORNING' : [1 if d in (6, 7, 8, 9) else 0 for d in delays_df.CRS_DEP_TIME],\n    'NOON' : [1 if d in (10, 11, 12, 13) else 0 for d in delays_df.CRS_DEP_TIME],\n    'AFTER2P' : [1 if d in (14, 15, 16, 17, 18) else 0 for d in delays_df.CRS_DEP_TIME],\n    'EVENING' : [1 if d in (19, 20) else 0 for d in delays_df.CRS_DEP_TIME],\n    'isDelayed' : [1 if status == 'delayed' else 0 for status in delays_df['Flight Status']],\n})\n\nX = delays_red_df.drop(columns=['isDelayed'])\ny = delays_red_df['isDelayed']\nclasses = ['ontime', 'delayed']\n\n# split into training and validation\ntrain_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size=0.4, \n                                                      random_state=1)\n\nlogit_red = LogisticRegressionCV(penalty=\"l1\", solver='liblinear', cv=5)\nlogit_red.fit(train_X, train_y)\n\npd.set_option('display.width', 100)\nprint('regularization', logit_red.C_)\nprint('intercept ', logit_red.intercept_[0])\nprint(pd.DataFrame({'coeff': logit_red.coef_[0]}, index=X.columns).transpose())\npd.reset_option('display.width')\nprint('AIC', AIC_score(valid_y, logit_red.predict(valid_X), df=len(train_X.columns) + 1))\n\n\n# confusion matrix\nclassificationSummary(valid_y, logit_red.predict(valid_X), class_names=classes)\n\nregularization [2.7825594]\nintercept  -2.2872921237506123\n        Sun_Mon  Weather  CARRIER_CO_MQ_DH_RU   MORNING      NOON   AFTER2P   EVENING\ncoeff  0.577924  4.97792             1.298771 -0.583344 -0.665919 -0.055221  0.560798\nAIC 934.6153607819033\nConfusion Matrix (Accuracy 0.8343)\n\n        Prediction\n Actual  ontime delayed\n ontime     711       3\ndelayed     143      24\n\n\n\nlogit_reg_proba = logit_red.predict_proba(valid_X)\nred_result = pd.DataFrame({'actual': valid_y, \n                            'p(0)': [p[0] for p in logit_reg_proba],\n                            'p(1)': [p[1] for p in logit_reg_proba],\n                            'predicted': logit_red.predict(valid_X),\n                          })\nred_result = red_result.sort_values(by=['p(1)'], ascending=False)\n\nax = gainsChart(full_result.actual, label='Full model', color='C1', figsize=[5, 5])\nax = gainsChart(red_result.actual, label='Reduced model', color='C0', ax=ax)\nax.legend()\n\nplt.show()"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 10 - Logistic regression.html#table-10.9-1",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 10 - Logistic regression.html#table-10.9-1",
    "title": "Chapter 10: Logistic Regression",
    "section": "Table 10.9",
    "text": "Table 10.9\n\n# same initial preprocessing and creating dummies\n\n# add constant column\nbank_df = sm.add_constant(bank_df, prepend=True)\n\ny = bank_df['Personal_Loan']\nX = bank_df.drop(columns=['Personal_Loan'])\n\n# partition data\ntrain_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size=0.4, random_state=1)\n\n# use GLM (general linear model) with the binomial family to fit a logistic regression\nlogit_reg = sm.GLM(train_y, train_X, family=sm.families.Binomial())\nlogit_result = logit_reg.fit()\nprint(logit_result.summary())\n\n                 Generalized Linear Model Regression Results                  \n==============================================================================\nDep. Variable:          Personal_Loan   No. Observations:                 3000\nModel:                            GLM   Df Residuals:                     2987\nModel Family:                Binomial   Df Model:                           12\nLink Function:                  logit   Scale:                          1.0000\nMethod:                          IRLS   Log-Likelihood:                -340.15\nDate:                Wed, 22 Sep 2021   Deviance:                       680.30\nTime:                        14:17:20   Pearson chi2:                 8.10e+03\nNo. Iterations:                     8                                         \nCovariance Type:            nonrobust                                         \n===================================================================================================\n                                      coef    std err          z      P&gt;|z|      [0.025      0.975]\n---------------------------------------------------------------------------------------------------\nconst                             -12.5634      2.336     -5.377      0.000     -17.143      -7.984\nAge                                -0.0354      0.086     -0.412      0.680      -0.204       0.133\nExperience                          0.0369      0.086      0.431      0.666      -0.131       0.205\nIncome                              0.0589      0.004     15.044      0.000       0.051       0.067\nFamily                              0.6128      0.103      5.931      0.000       0.410       0.815\nCCAvg                               0.2408      0.060      4.032      0.000       0.124       0.358\nMortgage                            0.0010      0.001      1.301      0.193      -0.001       0.003\nSecurities_Account                 -1.0305      0.422     -2.443      0.015      -1.857      -0.204\nCD_Account                          3.6628      0.460      7.961      0.000       2.761       4.565\nOnline                             -0.6794      0.216     -3.140      0.002      -1.103      -0.255\nCreditCard                         -0.9609      0.274     -3.507      0.000      -1.498      -0.424\nEducation_Graduate                  4.2075      0.364     11.573      0.000       3.495       4.920\nEducation_Advanced/Professional     4.3580      0.365     11.937      0.000       3.642       5.074\n===================================================================================================\n\n\n/opt/conda/lib/python3.9/site-packages/statsmodels/tsa/tsatools.py:142: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n  x = pd.concat(x[::order], 1)\n\n\n\ndata = pd.read_csv('accidentsFull.csv')\noutcome = 'MAX_SEV_IR'\npredictors = ['ALCHL_I', 'WEATHER_R']\n\ny = data[outcome]\nX = data[predictors]\ntrain_X, train_y = X, y\nclasses = sorted(y.unique())\n\nprint('Nominal logistic regression')\nlogit = LogisticRegression(penalty=\"l2\", solver='lbfgs', C=1e24, multi_class='multinomial')\nlogit.fit(X, y)\nprint('  intercept', logit.intercept_)\nprint('  coefficients', logit.coef_)\nprint()\nprobs = logit.predict_proba(X)\nresults = pd.DataFrame({\n    'actual': y, 'predicted': logit.predict(X),\n    'P(0)': [p[0] for p in probs],\n    'P(1)': [p[1] for p in probs],\n    'P(2)': [p[2] for p in probs],\n})\nprint(results.head())\nprint()\n\n\nprint('Ordinal logistic regression')\nlogit = LogisticIT(alpha=0)\nlogit.fit(X, y)\nprint('  theta', logit.theta_)\nprint('  coefficients', logit.coef_)\nprint()\nprobs = logit.predict_proba(X)\nresults = pd.DataFrame({\n    'actual': y, 'predicted': logit.predict(X),\n    'P(0)': [p[0] for p in probs],\n    'P(1)': [p[1] for p in probs],\n    'P(2)': [p[2] for p in probs],\n})\nprint(results.head())\n\n\nNominal logistic regression\n  intercept [-0.09100315  0.9036454  -0.81264225]\n  coefficients [[ 0.51606685  0.3391015 ]\n [ 0.14900396  0.09543369]\n [-0.66507082 -0.43453518]]\n\n   actual  predicted      P(0)      P(1)      P(2)\n0       1          1  0.490649  0.498989  0.010362\n1       0          0  0.553461  0.441147  0.005392\n2       0          0  0.553461  0.441147  0.005392\n3       0          1  0.490649  0.498989  0.010362\n4       0          1  0.394192  0.578684  0.027124\n\nOrdinal logistic regression\n  theta [-1.06916285  2.77444326]\n  coefficients [-0.40112008 -0.25174207]\n\n   actual  predicted      P(0)      P(1)      P(2)\n0       1          1  0.496205  0.482514  0.021281\n1       0          0  0.558866  0.424510  0.016624\n2       0          0  0.558866  0.424510  0.016624\n3       0          1  0.496205  0.482514  0.021281\n4       0          1  0.397402  0.571145  0.031453"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 10 - Logistic regression.html#nominal-logistic-regression",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 10 - Logistic regression.html#nominal-logistic-regression",
    "title": "Chapter 10: Logistic Regression",
    "section": "Nominal logistic regression",
    "text": "Nominal logistic regression\n\ndata = pd.read_csv('accidentsFull.csv')\noutcome = 'MAX_SEV_IR'\npredictors = ['ALCHL_I', 'WEATHER_R']\nprint('predictors', predictors)\nprint()\ny = data[outcome]\nX = data[predictors]\ntrain_X, train_y = X, y\nclasses = sorted(y.unique())\n\nlogit = LogisticRegression(penalty=\"l2\", solver='lbfgs', C=1e24, multi_class='multinomial')\nlogit.fit(X, y)\n\nprint('intercept', logit.intercept_)\nprint('coef', logit.coef_)\nprint('classes', logit.classes_)\n\nprobs = logit.predict_proba(X)\nresults = pd.DataFrame({\n    'actual': y,\n    'predicted': logit.predict(X),\n    'P(a)': [p[0] for p in probs],\n    'P(b)': [p[1] for p in probs],\n    'P(c)': [p[2] for p in probs],\n})\nprint(results.head())\n# classificationSummary(y, results.predicted, class_names=classes)\n\npredictors ['ALCHL_I', 'WEATHER_R']\n\nintercept [-0.09100315  0.9036454  -0.81264225]\ncoef [[ 0.51606685  0.3391015 ]\n [ 0.14900396  0.09543369]\n [-0.66507082 -0.43453518]]\nclasses [0 1 2]\n   actual  predicted      P(a)      P(b)      P(c)\n0       1          1  0.490649  0.498989  0.010362\n1       0          0  0.553461  0.441147  0.005392\n2       0          0  0.553461  0.441147  0.005392\n3       0          1  0.490649  0.498989  0.010362\n4       0          1  0.394192  0.578684  0.027124\n\n\n\ndef plotProbabilities(model):\n    n = 100\n    Xtest = pd.DataFrame({\n        'ALCHL_I': [(i % n) * 0.1 + (j // n) * 0.1 - 5 for i in range(n) for j in range(n)],\n        'WEATHER_R': [(i // n) * 0.1 + (j % n) * 0.1 - 5 for i in range(n) for j in range(n)],\n    })\n    probs = logit.predict_proba(Xtest[predictors])\n\n    results = pd.DataFrame({\n        'predicted': logit.predict(Xtest[predictors]),\n        'P(a)': [round(10 * p[0]) / 10 for p in probs],\n        'P(b)': [round(10 * p[1]) / 10 for p in probs],\n        'P(c)': [round(10 * p[2]) / 10 for p in probs],\n    })\n    mapped = pd.concat([results, Xtest], axis=1, sort=False)\n\n    fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(10, 3))\n    for i, c in enumerate(['P(a)', 'P(b)', 'P(c)']):\n        ax = mapped.plot.scatter(x='ALCHL_I', y='WEATHER_R', c=c, title=c, colormap='cividis', ax=axes[i])\n        if i &gt; 0: ax.set_ylabel('')\nplotProbabilities(logit)"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 10 - Logistic regression.html#ordinal-logistic-regression",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 10 - Logistic regression.html#ordinal-logistic-regression",
    "title": "Chapter 10: Logistic Regression",
    "section": "Ordinal logistic regression",
    "text": "Ordinal logistic regression\n\nimport random\nfrom mord import LogisticIT\n\ndata = pd.read_csv('accidentsFull.csv')\noutcome = 'MAX_SEV_IR'\npredictors = ['ALCHL_I', 'WEATHER_R']\ny = data[outcome]\nX = data[predictors]\nX['ALCHL_I']\ntrain_X, train_y = X, y\nclasses = sorted(y.unique())\n\nlogit = LogisticIT(alpha=0)\nlogit.fit(X, y)\n\nprint('theta', logit.theta_)\nprint('coef', logit.coef_)\nprint('classes', logit.classes_)\nprint()\n\n\nprobs = logit.predict_proba(X)\n\nresults = pd.DataFrame({\n    'actual': [yi + random.gauss(0, 0.001) for yi in y],\n    'predicted': logit.predict(X),\n    'P(a)': [p[0] + random.gauss(0, 0.001) for p in probs],\n    'P(b)': [p[1] + random.gauss(0, 0.001) for p in probs],\n    'P(c)': [p[2] + random.gauss(0, 0.001) for p in probs],\n})\nprint(results.head())\n\n# classificationSummary(y, results.predicted, class_names=classes)\n\ntheta [-1.06916285  2.77444326]\ncoef [-0.40112008 -0.25174207]\nclasses [0 1 2]\n\n     actual  predicted      P(a)      P(b)      P(c)\n0  1.001292          1  0.496079  0.483221  0.022266\n1  0.000238          0  0.559020  0.425609  0.016593\n2  0.001339          0  0.559629  0.424719  0.015781\n3  0.001341          1  0.495112  0.480833  0.020045\n4 -0.000662          1  0.397295  0.572080  0.032554\n\n\n\nplotProbabilities(logit)"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 15 - Cluster analysis.html",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 15 - Cluster analysis.html",
    "title": "Chapter 15: Cluster Analysis",
    "section": "",
    "text": "2019 Galit Shmueli, Peter C. Bruce, Peter Gedeck\n\nCode included in\nData Mining for Business Analytics: Concepts, Techniques, and Applications in Python (First Edition) Galit Shmueli, Peter C. Bruce, Peter Gedeck, and Nitin R. Patel. 2019."
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 15 - Cluster analysis.html#import-required-packages",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 15 - Cluster analysis.html#import-required-packages",
    "title": "Chapter 15: Cluster Analysis",
    "section": "Import required packages",
    "text": "Import required packages\n\nfrom pathlib import Path\n\nimport pandas as pd\nfrom sklearn import preprocessing\nfrom sklearn.metrics import pairwise\nfrom scipy.cluster.hierarchy import dendrogram, linkage, fcluster\nfrom sklearn.cluster import KMeans\nimport matplotlib.pylab as plt\nimport seaborn as sns\nfrom pandas.plotting import parallel_coordinates\n\nimport dmba\n\n%matplotlib inline\n\nno display found. Using non-interactive Agg backend"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 15 - Cluster analysis.html#table-15.2",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 15 - Cluster analysis.html#table-15.2",
    "title": "Chapter 15: Cluster Analysis",
    "section": "Table 15.2",
    "text": "Table 15.2\nLoad the data, set row names (index) to the utilities column (company) and remove it. Convert all columns to float\n\nutilities_df = dmba.load_data('Utilities.csv')\nutilities_df.set_index('Company', inplace=True)\n\n# while not required, the conversion of integer data to float will avoid a warning when \n# applying the scale function\nutilities_df = utilities_df.apply(lambda x: x.astype('float64'))\nutilities_df.head()\n\n\n\n\n\n\n\n\nFixed_charge\nRoR\nCost\nLoad_factor\nDemand_growth\nSales\nNuclear\nFuel_Cost\n\n\nCompany\n\n\n\n\n\n\n\n\n\n\n\n\nArizona\n1.06\n9.2\n151.0\n54.4\n1.6\n9077.0\n0.0\n0.628\n\n\nBoston\n0.89\n10.3\n202.0\n57.9\n2.2\n5088.0\n25.3\n1.555\n\n\nCentral\n1.43\n15.4\n113.0\n53.0\n3.4\n9212.0\n0.0\n1.058\n\n\nCommonwealth\n1.02\n11.2\n168.0\n56.0\n0.3\n6423.0\n34.3\n0.700\n\n\nNY\n1.49\n8.8\n192.0\n51.2\n1.0\n3300.0\n15.6\n2.044\n\n\n\n\n\n\n\nCompute Euclidean distance matrix (to compute other metrics, change the name of metric argument)\n\nd = pairwise.pairwise_distances(utilities_df, metric='euclidean')\npd.DataFrame(d, columns=utilities_df.index, index=utilities_df.index).head(5)\n\n\n\n\n\n\n\nCompany\nArizona\nBoston\nCentral\nCommonwealth\nNY\nFlorida\nHawaiian\nIdaho\nKentucky\nMadison\n...\nNorthern\nOklahoma\nPacific\nPuget\nSan Diego\nSouthern\nTexas\nWisconsin\nUnited\nVirginia\n\n\nCompany\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArizona\n0.000000\n3989.408076\n140.402855\n2654.277632\n5777.167672\n2050.529440\n1435.265019\n4006.104187\n671.276346\n2622.699002\n...\n1899.279821\n598.556633\n2609.045363\n6914.742065\n3363.061626\n1063.009074\n4430.251585\n1790.485648\n2427.588875\n1016.617691\n\n\nBoston\n3989.408076\n0.000000\n4125.044132\n1335.466502\n1788.068027\n6039.689076\n2554.287162\n7994.155985\n3318.276558\n1367.090634\n...\n2091.160485\n4586.302564\n1380.749962\n10903.146464\n629.760748\n5052.331669\n8419.610541\n2199.721665\n1562.210811\n5005.081262\n\n\nCentral\n140.402855\n4125.044132\n0.000000\n2789.759674\n5912.552908\n1915.155154\n1571.295401\n3872.257626\n807.920792\n2758.559663\n...\n2035.441520\n461.341670\n2744.502847\n6780.430307\n3498.113013\n928.749249\n4295.014690\n1925.772564\n2563.637362\n883.535455\n\n\nCommonwealth\n2654.277632\n1335.466502\n2789.759674\n0.000000\n3123.153215\n4704.363099\n1219.560005\n6659.534567\n1983.314354\n43.648894\n...\n756.831954\n3250.984589\n56.644626\n9568.434429\n710.292965\n3717.202963\n7084.372839\n864.273153\n232.476871\n3670.018191\n\n\nNY\n5777.167672\n1788.068027\n5912.552908\n3123.153215\n0.000000\n7827.429211\n4342.093798\n9782.158178\n5106.094153\n3155.095594\n...\n3879.167462\n6373.743249\n3168.177463\n12691.155108\n2414.698757\n6840.150291\n10207.392630\n3987.335962\n3350.073118\n6793.035300\n\n\n\n\n5 rows × 22 columns"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 15 - Cluster analysis.html#table-15.4",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 15 - Cluster analysis.html#table-15.4",
    "title": "Chapter 15: Cluster Analysis",
    "section": "Table 15.4",
    "text": "Table 15.4\nHere are two ways to normalize the input variables. Pandas calculates by default the sample standard deviation, whereas scikit-learn uses the population standard deviation. The normalized data from the two methods will therefore differ slightly. We will use the Pandas approach as it is equivalent to the R implementation of scale.\n\n# scikit-learn uses population standard deviation\nutilities_df_norm = utilities_df.apply(preprocessing.scale, axis=0)\n\n# pandas uses sample standard deviation\nutilities_df_norm = (utilities_df - utilities_df.mean())/utilities_df.std()\n\n# compute normalized distance based on Sales and Fuel Cost\nd_norm = pairwise.pairwise_distances(utilities_df_norm[['Sales', 'Fuel_Cost']], \n                                     metric='euclidean')\npd.DataFrame(d_norm, columns=utilities_df.index, index=utilities_df.index).head(5)\n\n\n\n\n\n\n\nCompany\nArizona\nBoston\nCentral\nCommonwealth\nNY\nFlorida\nHawaiian\nIdaho\nKentucky\nMadison\n...\nNorthern\nOklahoma\nPacific\nPuget\nSan Diego\nSouthern\nTexas\nWisconsin\nUnited\nVirginia\n\n\nCompany\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nArizona\n1.490116e-08\n2.010329\n0.774179\n0.758738\n3.021907\n1.244422\n1.885248\n1.265638\n0.461292\n0.738650\n...\n0.564657\n0.182648\n1.570780\n1.947668\n2.509043\n0.913621\n1.247976\n0.521491\n2.761745\n1.252350\n\n\nBoston\n2.010329e+00\n0.000000\n1.465703\n1.582821\n1.013370\n1.792397\n0.740283\n3.176654\n1.557738\n1.719632\n...\n1.940166\n2.166078\n0.478334\n3.501390\n0.679634\n1.634425\n2.890560\n1.654255\n1.100595\n1.479261\n\n\nCentral\n7.741795e-01\n1.465703\n0.000000\n1.015710\n2.432528\n0.631892\n1.156092\n1.732777\n0.419254\n1.102287\n...\n1.113433\n0.855093\n0.987772\n2.065643\n1.836762\n0.276440\n1.428159\n0.838967\n2.034824\n0.510365\n\n\nCommonwealth\n7.587375e-01\n1.582821\n1.015710\n0.000000\n2.571969\n1.643857\n1.746027\n2.003230\n0.629994\n0.138758\n...\n0.377004\n0.937389\n1.258835\n2.699060\n2.202930\n1.278514\n1.998818\n0.243408\n2.547116\n1.502093\n\n\nNY\n3.021907e+00\n1.013370\n2.432528\n2.571969\n0.000000\n2.635573\n1.411695\n4.162561\n2.566439\n2.705445\n...\n2.938637\n3.174588\n1.462019\n4.397433\n0.715629\n2.558409\n3.831132\n2.661786\n0.952507\n2.328691\n\n\n\n\n5 rows × 22 columns"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 15 - Cluster analysis.html#figure-15.3",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 15 - Cluster analysis.html#figure-15.3",
    "title": "Chapter 15: Cluster Analysis",
    "section": "Figure 15.3",
    "text": "Figure 15.3\n\nZ = linkage(utilities_df_norm, method='single')\n\nfig = plt.figure(figsize=(10, 6))\nfig.subplots_adjust(bottom=0.23)\nplt.title('Hierarchical Clustering Dendrogram (Single linkage)')\nplt.xlabel('Company')\ndendrogram(Z, labels=utilities_df_norm.index, color_threshold=2.75)\nplt.axhline(y=2.75, color='black', linewidth=0.5, linestyle='dashed')\nplt.show()\n\n\n\n\n\n\n\n\n\nZ = linkage(utilities_df_norm, method='average')\n\nfig = plt.figure(figsize=(10, 6))\nfig.subplots_adjust(bottom=0.23)\nplt.title('Hierarchical Clustering Dendrogram (Average linkage)')\nplt.xlabel('Company')\ndendrogram(Z, labels=utilities_df_norm.index, color_threshold=3.6)\nplt.axhline(y=3.6, color='black', linewidth=0.5, linestyle='dashed')\nplt.show()"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 15 - Cluster analysis.html#table-15.6",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 15 - Cluster analysis.html#table-15.6",
    "title": "Chapter 15: Cluster Analysis",
    "section": "Table 15.6",
    "text": "Table 15.6\n\nmemb = fcluster(linkage(utilities_df_norm, 'single'), 6, criterion='maxclust')\nmemb = pd.Series(memb, index=utilities_df_norm.index)\nfor key, item in memb.groupby(memb):\n    print(key, ': ', ', '.join(item.index))\n\n1 :  Idaho, Puget\n2 :  Arizona , Boston , Commonwealth, Florida , Hawaiian , Kentucky, Madison , New England, Northern, Oklahoma, Pacific , Southern, Texas, Wisconsin, United, Virginia\n3 :  Central \n4 :  San Diego\n5 :  Nevada\n6 :  NY\n\n\n\nmemb = fcluster(linkage(utilities_df_norm, 'average'), 6, criterion='maxclust')\nmemb = pd.Series(memb, index=utilities_df_norm.index)\nfor key, item in memb.groupby(memb):\n    print(key, ': ', ', '.join(item.index))\n\n1 :  Idaho, Nevada, Puget\n2 :  Hawaiian , New England, Pacific , United\n3 :  San Diego\n4 :  Boston , Commonwealth, Madison , Northern, Wisconsin, Virginia\n5 :  Arizona , Central , Florida , Kentucky, Oklahoma, Southern, Texas\n6 :  NY"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 15 - Cluster analysis.html#figure-15.4",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 15 - Cluster analysis.html#figure-15.4",
    "title": "Chapter 15: Cluster Analysis",
    "section": "Figure 15.4",
    "text": "Figure 15.4\n\nutilities_df_norm.index = ['{}: {}'.format(cluster, state) for cluster, state in zip(memb, utilities_df_norm.index)]\nsns.clustermap(utilities_df_norm, method='average', col_cluster=False,  cmap=\"mako_r\")\nplt.show()"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 15 - Cluster analysis.html#figure-15.9",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 15 - Cluster analysis.html#figure-15.9",
    "title": "Chapter 15: Cluster Analysis",
    "section": "Figure 15.9",
    "text": "Figure 15.9\n\n# Load and preprocess data\nutilities_df = dmba.load_data('Utilities.csv')\nutilities_df.set_index('Company', inplace=True)\nutilities_df = utilities_df.apply(lambda x: x.astype('float64'))\n\n# Normalized distance\nutilities_df_norm = utilities_df.apply(preprocessing.scale, axis=0)\n\nkmeans = KMeans(n_clusters=6, random_state=0).fit(utilities_df_norm)\n\n# Cluster membership\nmemb = pd.Series(kmeans.labels_, index=utilities_df_norm.index)\nfor key, item in memb.groupby(memb):\n    print(key, ': ', ', '.join(item.index))\n\n0 :  Commonwealth, Madison , Northern, Wisconsin, Virginia\n1 :  Boston , Hawaiian , New England, Pacific , San Diego, United\n2 :  Arizona , Central , Florida , Kentucky, Oklahoma, Southern, Texas\n3 :  NY\n4 :  Nevada\n5 :  Idaho, Puget"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 15 - Cluster analysis.html#table-15.10",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 15 - Cluster analysis.html#table-15.10",
    "title": "Chapter 15: Cluster Analysis",
    "section": "Table 15.10",
    "text": "Table 15.10\n\ncentroids = pd.DataFrame(kmeans.cluster_centers_, columns=utilities_df_norm.columns)\npd.set_option('precision', 3)\nprint(centroids)\npd.set_option('precision', 6)\n\n   Fixed_charge    RoR   Cost  Load_factor  Demand_growth  Sales  Nuclear  \\\n0        -0.012  0.339  0.224       -0.366          0.170 -0.411    1.602   \n1        -0.633 -0.640  0.207        1.175          0.058 -0.758   -0.381   \n2         0.516  0.798 -1.009       -0.345         -0.501  0.360   -0.536   \n3         2.085 -0.883  0.592       -1.325         -0.736 -1.619    0.219   \n4        -2.020 -1.476  0.120       -1.257          1.070  2.458   -0.731   \n5         0.088 -0.541  1.996       -0.110          0.988  1.621   -0.731   \n\n   Fuel_Cost  \n0     -0.609  \n1      1.204  \n2     -0.420  \n3      1.732  \n4     -0.616  \n5     -1.175  \n\n\n\nwithinClusterSS = [0] * 6\nclusterCount = [0] * 6\nfor cluster, distance in zip(kmeans.labels_, kmeans.transform(utilities_df_norm)):\n    withinClusterSS[cluster] += distance[cluster]**2\n    clusterCount[cluster] += 1\nfor cluster, withClustSS in enumerate(withinClusterSS):\n    print('Cluster {} ({} members): {:5.2f} within cluster'.format(cluster, \n        clusterCount[cluster], withinClusterSS[cluster]))\n\nCluster 0 (5 members): 10.66 within cluster\nCluster 1 (6 members): 22.20 within cluster\nCluster 2 (7 members): 27.77 within cluster\nCluster 3 (1 members):  0.00 within cluster\nCluster 4 (1 members):  0.00 within cluster\nCluster 5 (2 members):  2.54 within cluster\n\n\n\n# calculate the distances of each data point to the cluster centers\ndistances = kmeans.transform(utilities_df_norm)\n\n# reduce to the minimum squared distance of each data point to the cluster centers\nminSquaredDistances = distances.min(axis=1) ** 2\n\n# combine with cluster labels into a data frame\ndf = pd.DataFrame({'squaredDistance': minSquaredDistances, 'cluster': kmeans.labels_}, \n    index=utilities_df_norm.index)\n\n# Group by cluster and print information\nfor cluster, data in df.groupby('cluster'):\n    count = len(data)\n    withinClustSS = data.squaredDistance.sum()\n    print(f'Cluster {cluster} ({count} members): {withinClustSS:.2f} within cluster ')\n\nCluster 0 (5 members): 10.66 within cluster \nCluster 1 (6 members): 22.20 within cluster \nCluster 2 (7 members): 27.77 within cluster \nCluster 3 (1 members): 0.00 within cluster \nCluster 4 (1 members): 0.00 within cluster \nCluster 5 (2 members): 2.54 within cluster"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 15 - Cluster analysis.html#figure-15.5",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 15 - Cluster analysis.html#figure-15.5",
    "title": "Chapter 15: Cluster Analysis",
    "section": "Figure 15.5",
    "text": "Figure 15.5\n\ncentroids['cluster'] = ['Cluster {}'.format(i) for i in centroids.index]\n\nplt.figure(figsize=(10,6))\nfig.subplots_adjust(right=3)\nax = parallel_coordinates(centroids, class_column='cluster', colormap='Dark2', linewidth=5)\nplt.legend(loc='center left', bbox_to_anchor=(0.95, 0.5))\nplt.xlim(-0.5,7.5)\ncentroids\n\n\n\n\n\n\n\n\nFixed_charge\nRoR\nCost\nLoad_factor\nDemand_growth\nSales\nNuclear\nFuel_Cost\ncluster\n\n\n\n\n0\n-0.011599\n0.339180\n0.224086\n-0.366466\n0.170386\n-0.411331\n1.601868\n-0.609460\nCluster 0\n\n\n1\n-0.632893\n-0.639936\n0.206692\n1.175321\n0.057691\n-0.757719\n-0.380962\n1.203616\nCluster 1\n\n\n2\n0.516184\n0.797896\n-1.009097\n-0.345490\n-0.501098\n0.360140\n-0.535523\n-0.420198\nCluster 2\n\n\n3\n2.085268\n-0.883194\n0.591840\n-1.325495\n-0.735555\n-1.618644\n0.219434\n1.732470\nCluster 3\n\n\n4\n-2.019709\n-1.476137\n0.119723\n-1.256665\n1.069762\n2.458495\n-0.731447\n-0.616086\nCluster 4\n\n\n5\n0.088252\n-0.541112\n1.995766\n-0.109502\n0.987702\n1.621068\n-0.731447\n-1.174696\nCluster 5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nutilities_df_norm.groupby(kmeans.labels_).mean()\n\n\n\n\n\n\n\n\nFixed_charge\nRoR\nCost\nLoad_factor\nDemand_growth\nSales\nNuclear\nFuel_Cost\n\n\n\n\n0\n-0.011599\n0.339180\n0.224086\n-0.366466\n0.170386\n-0.411331\n1.601868\n-0.609460\n\n\n1\n-0.632893\n-0.639936\n0.206692\n1.175321\n0.057691\n-0.757719\n-0.380962\n1.203616\n\n\n2\n0.516184\n0.797896\n-1.009097\n-0.345490\n-0.501098\n0.360140\n-0.535523\n-0.420198\n\n\n3\n2.085268\n-0.883194\n0.591840\n-1.325495\n-0.735555\n-1.618644\n0.219434\n1.732470\n\n\n4\n-2.019709\n-1.476137\n0.119723\n-1.256665\n1.069762\n2.458495\n-0.731447\n-0.616086\n\n\n5\n0.088252\n-0.541112\n1.995766\n-0.109502\n0.987702\n1.621068\n-0.731447\n-1.174696"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 15 - Cluster analysis.html#table-15.11",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 15 - Cluster analysis.html#table-15.11",
    "title": "Chapter 15: Cluster Analysis",
    "section": "Table 15.11",
    "text": "Table 15.11\n\nprint(pd.DataFrame(pairwise.pairwise_distances(kmeans.cluster_centers_, metric='euclidean')))\n\n          0         1         2         3         4         5\n0  0.000000  3.327706  2.767756  4.076778  4.756091  3.814743\n1  3.327706  0.000000  3.376575  3.982534  4.831882  4.205250\n2  2.767756  3.376575  0.000000  4.240989  4.544657  3.933837\n3  4.076778  3.982534  4.240989  0.000000  6.613030  5.511431\n4  4.756091  4.831882  4.544657  6.613030  0.000000  3.342656\n5  3.814743  4.205250  3.933837  5.511431  3.342656  0.000000\n\n\n\npd.DataFrame(pairwise.pairwise_distances(kmeans.cluster_centers_, metric='euclidean')).sum(axis=0)\n\n0    18.743074\n1    19.723946\n2    18.863814\n3    24.424763\n4    24.088316\n5    20.807918\ndtype: float64"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 15 - Cluster analysis.html#figure-15.6",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 15 - Cluster analysis.html#figure-15.6",
    "title": "Chapter 15: Cluster Analysis",
    "section": "Figure 15.6",
    "text": "Figure 15.6\n\ninertia = []\nfor n_clusters in range(1, 7):\n    kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(utilities_df_norm)\n    inertia.append(kmeans.inertia_ / n_clusters)\ninertias = pd.DataFrame({'n_clusters': range(1, 7), 'inertia': inertia})\nax = inertias.plot(x='n_clusters', y='inertia')\nplt.xlabel('Number of clusters(k)')\nplt.ylabel('Average Within-Cluster Squared Distances')\nplt.ylim((0, 1.1 * inertias.inertia.max()))\nax.legend().set_visible(False)\nplt.show()"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 11 - Neural nets.html",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 11 - Neural nets.html",
    "title": "Chapter 11: Neural nets",
    "section": "",
    "text": "2019 Galit Shmueli, Peter C. Bruce, Peter Gedeck\n\nCode included in\nData Mining for Business Analytics: Concepts, Techniques, and Applications in Python (First Edition) Galit Shmueli, Peter C. Bruce, Peter Gedeck, and Nitin R. Patel. 2019."
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 11 - Neural nets.html#import-required-packages",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 11 - Neural nets.html#import-required-packages",
    "title": "Chapter 11: Neural nets",
    "section": "Import required packages",
    "text": "Import required packages\n\nfrom pathlib import Path\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPClassifier\nimport matplotlib.pyplot as plt\n\nimport dmba\nfrom dmba import classificationSummary\n\n%matplotlib inline\n\nno display found. Using non-interactive Agg backend"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 11 - Neural nets.html#table-11.2",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 11 - Neural nets.html#table-11.2",
    "title": "Chapter 11: Neural nets",
    "section": "Table 11.2",
    "text": "Table 11.2\n\nexample_df = dmba.load_data('TinyData.csv')\n\npredictors = ['Fat', 'Salt']\noutcome = 'Acceptance'\n\nX = example_df[predictors]\ny = example_df[outcome]\nclasses = sorted(y.unique())\n\nclf = MLPClassifier(hidden_layer_sizes=(3), activation='logistic', solver='lbfgs', random_state=1)\nclf.fit(X, y)\nclf.predict(X)\n\n# Network structure\nprint('Intercepts')\nprint(clf.intercepts_)\n\nprint('Weights')\nprint(clf.coefs_)\n\n# Prediction\nprint(pd.concat([\n    example_df,\n    pd.DataFrame(clf.predict_proba(X), columns=classes)\n], axis=1))\n\nIntercepts\n[array([0.13368045, 4.07247552, 7.00768104]), array([14.30748676])]\nWeights\n[array([[ -1.30656481,  -4.20427792, -13.29587332],\n       [ -0.04399727,  -4.91606924,  -6.03356987]]), array([[ -0.27348313],\n       [ -9.01211573],\n       [-17.63504694]])]\n   Obs.  Fat  Salt Acceptance   dislike      like\n0     1  0.2   0.9       like  0.000490  0.999510\n1     2  0.1   0.1    dislike  0.999994  0.000006\n2     3  0.2   0.4    dislike  0.999741  0.000259\n3     4  0.2   0.5    dislike  0.997368  0.002632\n4     5  0.4   0.5       like  0.002133  0.997867\n5     6  0.3   0.8       like  0.000075  0.999925\n\n\n\n# Pretty print layers\nfor i, (weights, intercepts) in enumerate(zip(clf.coefs_, clf.intercepts_)):\n    print('Hidden layer' if i == 0 else 'Output layer', '{0[0]} =&gt; {0[1]}'.format(weights.shape))\n    print(' Intercepts:\\n ', intercepts)\n    print(' Weights:')\n    for weight in weights:\n        print(' ', weight)\n    print()\n\nHidden layer 2 =&gt; 3\n Intercepts:\n  [0.13368045 4.07247552 7.00768104]\n Weights:\n  [ -1.30656481  -4.20427792 -13.29587332]\n  [-0.04399727 -4.91606924 -6.03356987]\n\nOutput layer 3 =&gt; 1\n Intercepts:\n  [14.30748676]\n Weights:\n  [-0.27348313]\n  [-9.01211573]\n  [-17.63504694]"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 11 - Neural nets.html#table-11.3",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 11 - Neural nets.html#table-11.3",
    "title": "Chapter 11: Neural nets",
    "section": "Table 11.3",
    "text": "Table 11.3\n\nclassificationSummary(y, clf.predict(X), class_names=classes)\n\nConfusion Matrix (Accuracy 1.0000)\n\n        Prediction\n Actual dislike    like\ndislike       3       0\n   like       0       3"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 11 - Neural nets.html#table-11.6",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 11 - Neural nets.html#table-11.6",
    "title": "Chapter 11: Neural nets",
    "section": "Table 11.6",
    "text": "Table 11.6\n\naccidents_df = dmba.load_data('accidentsnn.csv')\ninput_vars = ['ALCHL_I', 'PROFIL_I_R', 'VEH_INVL']\n\naccidents_df.SUR_COND = accidents_df.SUR_COND.astype('category')\naccidents_df.MAX_SEV_IR = accidents_df.MAX_SEV_IR.astype('category')\n\n# convert the categorical data into dummy variables\n# exclude the column for SUR_COND 9 = unknown\nprocessed = pd.get_dummies(accidents_df, columns=['SUR_COND']).drop(columns=['SUR_COND_9'])\n\noutcome = 'MAX_SEV_IR'\npredictors = [c for c in processed.columns if c != outcome]\n\n# partition data\nX = processed[predictors]\ny = processed[outcome]\ntrain_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size=0.4, random_state=1)\n\n# train neural network with 2 hidden nodes\nclf = MLPClassifier(hidden_layer_sizes=(2), activation='logistic', solver='lbfgs',\n                    random_state=1)\nclf.fit(train_X, train_y.values)\n\n# training performance (use idxmax to revert the one-hot-encoding)\nclassificationSummary(train_y, clf.predict(train_X))\n\n# validation performance\nclassificationSummary(valid_y, clf.predict(valid_X))\n\nConfusion Matrix (Accuracy 0.8664)\n\n       Prediction\nActual   0   1   2\n     0 331   0   1\n     1   0 180   0\n     2  30  49   8\nConfusion Matrix (Accuracy 0.8550)\n\n       Prediction\nActual   0   1   2\n     0 218   0   1\n     1   0 119   0\n     2  24  33   5"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 11 - Neural nets.html#fitting-class-probabilities-separately",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 11 - Neural nets.html#fitting-class-probabilities-separately",
    "title": "Chapter 11: Neural nets",
    "section": "Fitting class probabilities separately",
    "text": "Fitting class probabilities separately\n\naccidents_df = dmba.load_data('accidentsnn.csv')\ninput_vars = ['ALCHL_I', 'PROFIL_I_R', 'VEH_INVL']\n\naccidents_df.SUR_COND = accidents_df.SUR_COND.astype('category')\naccidents_df.MAX_SEV_IR = accidents_df.MAX_SEV_IR.astype('category')\n\n# convert the categorical data into dummy variables\nprocessed = pd.get_dummies(accidents_df)\n# drop the column for SUR_COND 9 = unknown\nprocessed = processed.drop(columns=['SUR_COND_9'])\n\noutcome = ['MAX_SEV_IR_0', 'MAX_SEV_IR_1', 'MAX_SEV_IR_2']\npredictors = [c for c in processed.columns if c not in outcome]\nclasses = sorted(outcome)\n\n# partition data\nX = processed[predictors]\ny = processed[outcome]\ntrain_X, valid_X, train_y, valid_y = train_test_split(X, y, test_size=0.4, random_state=1)\n\n# train neural network with 2 hidden nodes\nclf = MLPClassifier(hidden_layer_sizes=(2), activation='logistic', solver='lbfgs',\n                    random_state=1, max_iter=500)\nclf.fit(train_X, train_y)\n\n# training performance (use idxmax to revert the one-hot-encoding)\ntrainPrediction = pd.DataFrame(clf.predict(train_X), columns=outcome).idxmax(axis=1)\nclassificationSummary(train_y.idxmax(axis=1), trainPrediction, class_names=classes)\n\n# validation performance\nvalidPrediction = pd.DataFrame(clf.predict(valid_X), columns=outcome).idxmax(axis=1)\nclassificationSummary(valid_y.idxmax(axis=1), validPrediction, class_names=classes)\n\nConfusion Matrix (Accuracy 0.8731)\n\n             Prediction\n      Actual MAX_SEV_IR_0 MAX_SEV_IR_1 MAX_SEV_IR_2\nMAX_SEV_IR_0          332            0            0\nMAX_SEV_IR_1            0          170           10\nMAX_SEV_IR_2           31           35           21\nConfusion Matrix (Accuracy 0.8675)\n\n             Prediction\n      Actual MAX_SEV_IR_0 MAX_SEV_IR_1 MAX_SEV_IR_2\nMAX_SEV_IR_0          218            0            1\nMAX_SEV_IR_1            0          113            6\nMAX_SEV_IR_2           24           22           16"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 11 - Neural nets.html#grid-search",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 11 - Neural nets.html#grid-search",
    "title": "Chapter 11: Neural nets",
    "section": "Grid search",
    "text": "Grid search\n\nfrom sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n\n# train neural network with 2 hidden nodes\nclf = MLPClassifier(hidden_layer_sizes=(2), activation='logistic', solver='lbfgs',\n                    random_state=1, max_iter=500)\nclf.fit(train_X, train_y.values)\n\nparam_grid = {\n    'hidden_layer_sizes': [(1), (2), (3), (4), (5)], \n}\ngridSearch = GridSearchCV(MLPClassifier(activation='logistic', solver='lbfgs', random_state=1, max_iter=1000), \n                          param_grid, cv=5, n_jobs=-1, return_train_score=True)\ngridSearch.fit(train_X, train_y)\nprint('Best score: ', gridSearch.best_score_)\nprint('Best parameters: ', gridSearch.best_params_)\n\nBest score:  0.8614005602240896\nBest parameters:  {'hidden_layer_sizes': 3}\n\n\n\ndisplay=['param_hidden_layer_sizes', 'mean_test_score', 'std_test_score']\nprint(pd.DataFrame(gridSearch.cv_results_)[display])\n\n  param_hidden_layer_sizes  mean_test_score  std_test_score\n0                        1         0.844762        0.011030\n1                        2         0.848067        0.009913\n2                        3         0.861401        0.023566\n3                        4         0.861387        0.025918\n4                        5         0.854748        0.023393\n\n\n\npd.DataFrame(gridSearch.cv_results_)[display].plot(x='param_hidden_layer_sizes', \n                                                   y='mean_test_score', yerr='std_test_score', ylim=(0.8, 0.9))\nplt.show()"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 20 - Text mining.html",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 20 - Text mining.html",
    "title": "Chapter 20: Text mining",
    "section": "",
    "text": "2019 Galit Shmueli, Peter C. Bruce, Peter Gedeck\n\nCode included in\nData Mining for Business Analytics: Concepts, Techniques, and Applications in Python (First Edition) Galit Shmueli, Peter C. Bruce, Peter Gedeck, and Nitin R. Patel. 2019."
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 20 - Text mining.html#import-required-packages",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 20 - Text mining.html#import-required-packages",
    "title": "Chapter 20: Text mining",
    "section": "Import required packages",
    "text": "Import required packages\n\nfrom pathlib import Path\n\nfrom zipfile import ZipFile\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import LogisticRegression\nimport nltk\nfrom nltk import word_tokenize          \nfrom nltk.stem.snowball import EnglishStemmer \nimport matplotlib.pylab as plt\n\nimport dmba\nfrom dmba import printTermDocumentMatrix, classificationSummary, liftChart\n\nnltk.download('punkt')\n\n%matplotlib inline\n\nno display found. Using non-interactive Agg backend\n\n\n[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt.zip."
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 20 - Text mining.html#table-20.1-term-document-representation-of-words-in-sentences-s1-s3",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 20 - Text mining.html#table-20.1-term-document-representation-of-words-in-sentences-s1-s3",
    "title": "Chapter 20: Text mining",
    "section": "Table 20.1 Term-document representation of words in sentences S1-S3",
    "text": "Table 20.1 Term-document representation of words in sentences S1-S3\n\ntext = ['this is the first sentence.',\n        'this is a second sentence.',\n        'the third sentence is here.']\n\n# Learn features based on text\ncount_vect = CountVectorizer()\ncounts = count_vect.fit_transform(text)\n\nprintTermDocumentMatrix(count_vect, counts)\n\n          S1  S2  S3\nfirst      1   0   0\nhere       0   0   1\nis         1   1   1\nsecond     0   1   0\nsentence   1   1   1\nthe        1   0   1\nthird      0   0   1\nthis       1   1   0"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 20 - Text mining.html#table-20.2-term-document-representation-of-words-in-sentences-s1-s4-example-2",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 20 - Text mining.html#table-20.2-term-document-representation-of-words-in-sentences-s1-s4-example-2",
    "title": "Chapter 20: Text mining",
    "section": "Table 20.2 Term-document representation of words in sentences S1-S4 (Example 2)",
    "text": "Table 20.2 Term-document representation of words in sentences S1-S4 (Example 2)\n\ntext = ['this is the first     sentence!!',\n        'this is a second Sentence :)',\n        'the third sentence, is here ',\n        'forth of all sentences']\n\n# Learn features based on text. Include special characters that are part of a word in the analysis\ncount_vect = CountVectorizer()\ncounts = count_vect.fit_transform(text)\n\nprintTermDocumentMatrix(count_vect, counts)\n\n           S1  S2  S3  S4\nall         0   0   0   1\nfirst       1   0   0   0\nforth       0   0   0   1\nhere        0   0   1   0\nis          1   1   1   0\nof          0   0   0   1\nsecond      0   1   0   0\nsentence    1   1   1   0\nsentences   0   0   0   1\nthe         1   0   1   0\nthird       0   0   1   0\nthis        1   1   0   0"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 20 - Text mining.html#table-20.3-tokenization-of-s1-s4",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 20 - Text mining.html#table-20.3-tokenization-of-s1-s4",
    "title": "Chapter 20: Text mining",
    "section": "Table 20.3 Tokenization of S1-S4",
    "text": "Table 20.3 Tokenization of S1-S4\n\ntext = ['this is the first     sentence!!',\n        'this is a second Sentence :)',\n        'the third sentence, is here ',\n        'forth of all sentences']\n\n# Learn features based on text. Include special characters that are part of a word in the analysis\ncount_vect = CountVectorizer(token_pattern='[a-zA-Z!:)]+')\ncounts = count_vect.fit_transform(text)\n\nprintTermDocumentMatrix(count_vect, counts)\n\n            S1  S2  S3  S4\n:)           0   1   0   0\na            0   1   0   0\nall          0   0   0   1\nfirst        1   0   0   0\nforth        0   0   0   1\nhere         0   0   1   0\nis           1   1   1   0\nof           0   0   0   1\nsecond       0   1   0   0\nsentence     0   1   1   0\nsentence!!   1   0   0   0\nsentences    0   0   0   1\nthe          1   0   1   0\nthird        0   0   1   0\nthis         1   1   0   0"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 20 - Text mining.html#table-20.4-stopwords-in-scitkit-learn",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 20 - Text mining.html#table-20.4-stopwords-in-scitkit-learn",
    "title": "Chapter 20: Text mining",
    "section": "Table 20.4 Stopwords in scitkit-learn",
    "text": "Table 20.4 Stopwords in scitkit-learn\n\nfrom sklearn.feature_extraction._stop_words import ENGLISH_STOP_WORDS\nstopWords = list(sorted(ENGLISH_STOP_WORDS))\nncolumns = 6; nrows= 30\n\nprint('First {} of {} stopwords'.format(ncolumns * nrows, len(stopWords)))\nfor i in range(0, len(stopWords[:(ncolumns * nrows)]), ncolumns):\n    print(''.join(word.ljust(13) for word in stopWords[i:(i+ncolumns)]))\n\nFirst 180 of 318 stopwords\na            about        above        across       after        afterwards   \nagain        against      all          almost       alone        along        \nalready      also         although     always       am           among        \namongst      amoungst     amount       an           and          another      \nany          anyhow       anyone       anything     anyway       anywhere     \nare          around       as           at           back         be           \nbecame       because      become       becomes      becoming     been         \nbefore       beforehand   behind       being        below        beside       \nbesides      between      beyond       bill         both         bottom       \nbut          by           call         can          cannot       cant         \nco           con          could        couldnt      cry          de           \ndescribe     detail       do           done         down         due          \nduring       each         eg           eight        either       eleven       \nelse         elsewhere    empty        enough       etc          even         \never         every        everyone     everything   everywhere   except       \nfew          fifteen      fifty        fill         find         fire         \nfirst        five         for          former       formerly     forty        \nfound        four         from         front        full         further      \nget          give         go           had          has          hasnt        \nhave         he           hence        her          here         hereafter    \nhereby       herein       hereupon     hers         herself      him          \nhimself      his          how          however      hundred      i            \nie           if           in           inc          indeed       interest     \ninto         is           it           its          itself       keep         \nlast         latter       latterly     least        less         ltd          \nmade         many         may          me           meanwhile    might        \nmill         mine         more         moreover     most         mostly       \nmove         much         must         my           myself       name         \nnamely       neither      never        nevertheless next         nine         \nno           nobody       none         noone        nor          not"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 20 - Text mining.html#table-20.5-text-reduction-of-s1-s4-using-stemming",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 20 - Text mining.html#table-20.5-text-reduction-of-s1-s4-using-stemming",
    "title": "Chapter 20: Text mining",
    "section": "Table 20.5 Text reduction of S1-S4 using stemming",
    "text": "Table 20.5 Text reduction of S1-S4 using stemming\n\nfrom sklearn.feature_extraction._stop_words import ENGLISH_STOP_WORDS\ntext = ['this is the first     sentence!! ',\n        'this is a second Sentence :)',\n        'the third sentence, is here ',\n        'forth of all sentences']\n\n# Create a custom tokenizer that will use NLTK for tokenizing and lemmatizing \n# (removes interpunctuation and stop words)\nclass LemmaTokenizer:\n    def __init__(self):\n        self.stemmer = EnglishStemmer()\n        self.stopWords = set(ENGLISH_STOP_WORDS)\n\n    def __call__(self, doc):\n        return [self.stemmer.stem(t) for t in word_tokenize(doc) \n                if t.isalpha() and t not in self.stopWords]\n\n# Learn features based on text\ncount_vect = CountVectorizer(tokenizer=LemmaTokenizer())\ncounts = count_vect.fit_transform(text)\n\nprintTermDocumentMatrix(count_vect, counts)\n\n         S1  S2  S3  S4\nforth     0   0   0   1\nsecond    0   1   0   0\nsentenc   1   1   1   1"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 20 - Text mining.html#table-20.6-tf-idf-matrix-for-s1-s4-example-after-tokenization",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 20 - Text mining.html#table-20.6-tf-idf-matrix-for-s1-s4-example-after-tokenization",
    "title": "Chapter 20: Text mining",
    "section": "Table 20.6 tf-idf matrix for S1-S4 example (after tokenization)",
    "text": "Table 20.6 tf-idf matrix for S1-S4 example (after tokenization)\n\ntext = ['this is the first     sentence!!',\n        'this is a second Sentence :)',\n        'the third sentence, is here ',\n        'forth of all sentences']\n\n# Apply CountVectorizer and TfidfTransformer sequentially\ncount_vect = CountVectorizer()\ntfidfTransformer = TfidfTransformer(smooth_idf=False, norm=None)\ncounts = count_vect.fit_transform(text)\ntfidf = tfidfTransformer.fit_transform(counts)\n\nprintTermDocumentMatrix(count_vect, tfidf)\n\n                 S1        S2        S3        S4\nall        0.000000  0.000000  0.000000  2.386294\nfirst      2.386294  0.000000  0.000000  0.000000\nforth      0.000000  0.000000  0.000000  2.386294\nhere       0.000000  0.000000  2.386294  0.000000\nis         1.287682  1.287682  1.287682  0.000000\nof         0.000000  0.000000  0.000000  2.386294\nsecond     0.000000  2.386294  0.000000  0.000000\nsentence   1.287682  1.287682  1.287682  0.000000\nsentences  0.000000  0.000000  0.000000  2.386294\nthe        1.693147  0.000000  1.693147  0.000000\nthird      0.000000  0.000000  2.386294  0.000000\nthis       1.693147  1.693147  0.000000  0.000000"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 20 - Text mining.html#table-20.7-importing-and-labeling-the-records-preprocessing-text-and-producing-concept-matrix",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 20 - Text mining.html#table-20.7-importing-and-labeling-the-records-preprocessing-text-and-producing-concept-matrix",
    "title": "Chapter 20: Text mining",
    "section": "Table 20.7 Importing and labeling the records, preprocessing text, and producing concept matrix",
    "text": "Table 20.7 Importing and labeling the records, preprocessing text, and producing concept matrix\n\n# Step 1: import and label records\ncorpus = []\nlabel = []\nwith ZipFile(dmba.get_data_file('AutoAndElectronics.zip')) as rawData:\n    for info in rawData.infolist():\n        if info.is_dir(): \n            continue\n        label.append(1 if 'rec.autos' in info.filename else 0)\n        corpus.append(rawData.read(info))\n\n# Step 2: preprocessing (tokenization, stemming, and stopwords)\nclass LemmaTokenizer(object):\n    def __init__(self):\n        self.stemmer = EnglishStemmer()\n        self.stopWords = set(ENGLISH_STOP_WORDS)\n    def __call__(self, doc):\n        return [self.stemmer.stem(t) for t in word_tokenize(doc) \n                if t.isalpha() and t not in self.stopWords]\n\npreprocessor = CountVectorizer(tokenizer=LemmaTokenizer(), encoding='latin1')\npreprocessedText = preprocessor.fit_transform(corpus)\n\n\n\n# Step 3: TF-IDF and latent semantic analysis\ntfidfTransformer = TfidfTransformer()\ntfidf = tfidfTransformer.fit_transform(preprocessedText)\n\n# Extract 20 concepts using LSA ()\nsvd = TruncatedSVD(20)\nnormalizer = Normalizer(copy=False)\nlsa = make_pipeline(svd, normalizer)\n\nlsa_tfidf = lsa.fit_transform(tfidf)"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 20 - Text mining.html#table-20.8-fitting-a-predictive-model-to-the-autos-and-electronics-discussion-data",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 20 - Text mining.html#table-20.8-fitting-a-predictive-model-to-the-autos-and-electronics-discussion-data",
    "title": "Chapter 20: Text mining",
    "section": "Table 20.8 Fitting a predictive model to the autos and electronics discussion data",
    "text": "Table 20.8 Fitting a predictive model to the autos and electronics discussion data\n\n# split dataset into 60% training and 40% test set\nXtrain, Xtest, ytrain, ytest = train_test_split(lsa_tfidf, label, test_size=0.4, random_state=42)\n\n# run logistic regression model on training\nlogit_reg = LogisticRegression(solver='lbfgs')\nlogit_reg.fit(Xtrain, ytrain)\n\n# print confusion matrix and accuracty\nclassificationSummary(ytest, logit_reg.predict(Xtest))\n\nConfusion Matrix (Accuracy 0.9563)\n\n       Prediction\nActual   0   1\n     0 389   8\n     1  27 376\n\n\n\ndf = pd.DataFrame(data={'prob': [p[1] for p in logit_reg.predict_proba(Xtest)], 'actual': ytest})\ndf = df.sort_values(by=['prob'], ascending=False).reset_index(drop=True)\n\nax = liftChart(df.actual, labelBars=False)\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 13 - Ensembles and uplift.html",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 13 - Ensembles and uplift.html",
    "title": "Chapter 13: Combining Models: Ensembles and Uplift Modeling",
    "section": "",
    "text": "2019 Galit Shmueli, Peter C. Bruce, Peter Gedeck\n\nCode included in\nData Mining for Business Analytics: Concepts, Techniques, and Applications in Python (First Edition) Galit Shmueli, Peter C. Bruce, Peter Gedeck, and Nitin R. Patel. 2019."
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 13 - Ensembles and uplift.html#import-required-packages",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 13 - Ensembles and uplift.html#import-required-packages",
    "title": "Chapter 13: Combining Models: Ensembles and Uplift Modeling",
    "section": "Import required packages",
    "text": "Import required packages\n\nfrom pathlib import Path\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport dmba\nfrom dmba import classificationSummary\n\n%matplotlib inline\n\nno display found. Using non-interactive Agg backend"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 13 - Ensembles and uplift.html#table-13.1-13.2",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 13 - Ensembles and uplift.html#table-13.1-13.2",
    "title": "Chapter 13: Combining Models: Ensembles and Uplift Modeling",
    "section": "Table 13.1, 13.2",
    "text": "Table 13.1, 13.2\n\nbank_df = dmba.load_data('UniversalBank.csv')\nbank_df.drop(columns=['ID', 'ZIP Code'], inplace=True)\n\n# split into training and validation\nX = bank_df.drop(columns=['Personal Loan'])\ny = bank_df['Personal Loan']\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.40, random_state=3)\n\n\nsingle tree\n\ndefaultTree = DecisionTreeClassifier(random_state=1)\ndefaultTree.fit(X_train, y_train)\n\nclasses = defaultTree.classes_\nclassificationSummary(y_valid, defaultTree.predict(X_valid), class_names=defaultTree.classes_)\n\nConfusion Matrix (Accuracy 0.9825)\n\n       Prediction\nActual    0    1\n     0 1778   15\n     1   20  187\n\n\n\n\nbagging\n\nbagging = BaggingClassifier(DecisionTreeClassifier(random_state=1), \n                            n_estimators=100, random_state=1)\nbagging.fit(X_train, y_train)\n\nclassificationSummary(y_valid, bagging.predict(X_valid), class_names=classes)\n\nConfusion Matrix (Accuracy 0.9855)\n\n       Prediction\nActual    0    1\n     0 1781   12\n     1   17  190\n\n\n\n\nboosting\n\nboost = AdaBoostClassifier(DecisionTreeClassifier(random_state=1), n_estimators=100, random_state=1)\nboost.fit(X_train, y_train)\n\nclassificationSummary(y_valid, boost.predict(X_valid), class_names=classes)\n\nConfusion Matrix (Accuracy 0.9840)\n\n       Prediction\nActual    0    1\n     0 1776   17\n     1   15  192"
  },
  {
    "objectID": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 13 - Ensembles and uplift.html#table-13.9",
    "href": "Course/MGS3701/07_Script/Chapter Examples Python/Chapter 13 - Ensembles and uplift.html#table-13.9",
    "title": "Chapter 13: Combining Models: Ensembles and Uplift Modeling",
    "section": "Table 13.9",
    "text": "Table 13.9\n\nvoter_df = dmba.load_data('Voter-Persuasion.csv')\n\n# Preprocess data frame\npredictors = ['AGE', 'NH_WHITE', 'COMM_PT', 'H_F1', 'REG_DAYS', \n              'PR_PELIG', 'E_PELIG', 'POLITICALC', 'MESSAGE_A']\noutcome = 'MOVED_AD'\n\nclasses = list(voter_df.MOVED_AD.unique())\n\n# Partition the data\nX = voter_df[predictors]\ny = voter_df[outcome]\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.40, random_state=1)\n\n# Train a random forest classifier using the training set\nrfModel = RandomForestClassifier(n_estimators=100, random_state=1)\nrfModel.fit(X_train, y_train)\n\nclassificationSummary(y_valid, rfModel.predict(X_valid), class_names=classes)\n\nConfusion Matrix (Accuracy 0.6415)\n\n       Prediction\nActual    N    Y\n     N 2018  450\n     Y  984  548\n\n\n\nuplift_df = X_valid.copy()  # Need to create a copy to allow modifying data\n\nuplift_df.MESSAGE_A = 1\npredTreatment = rfModel.predict_proba(uplift_df)\nuplift_df.MESSAGE_A = 0\npredControl = rfModel.predict_proba(uplift_df)\n\nupliftResult_df = pd.DataFrame({\n    'probMessage': predTreatment[:,1],\n    'probNoMessage': predControl[:,1],\n    'uplift': predTreatment[:,1] - predControl[:,1],\n    }, index=uplift_df.index)\nprint(upliftResult_df.head())\n\n      probMessage  probNoMessage  uplift\n9953         0.77           0.62    0.15\n3850         0.39           0.39    0.00\n4962         0.20           0.14    0.06\n3886         0.86           0.62    0.24\n5437         0.10           0.28   -0.18\n\n\n\nprint(predTreatment)\n\n[[0.23 0.77]\n [0.61 0.39]\n [0.8  0.2 ]\n ...\n [0.56 0.44]\n [0.23 0.77]\n [0.3  0.7 ]]"
  },
  {
    "objectID": "Course/WKU2025 Class Information/lecture note template/chapter0.html#short-video-introduction",
    "href": "Course/WKU2025 Class Information/lecture note template/chapter0.html#short-video-introduction",
    "title": "TITLE",
    "section": "SHORT VIDEO INTRODUCTION",
    "text": "SHORT VIDEO INTRODUCTION"
  },
  {
    "objectID": "Course/WKU2025 Class Information/lecture note template/chapter0.html#lv1_1",
    "href": "Course/WKU2025 Class Information/lecture note template/chapter0.html#lv1_1",
    "title": "TITLE",
    "section": "Lv1_1",
    "text": "Lv1_1\n\nThis is level1 contents\nThis is level1 contents\nThis is level1 contents\nThis is level1 contents\nThis is level1 contents"
  },
  {
    "objectID": "Course/WKU2025 Class Information/lecture note template/chapter0.html#lv2_2",
    "href": "Course/WKU2025 Class Information/lecture note template/chapter0.html#lv2_2",
    "title": "TITLE",
    "section": "Lv2_2",
    "text": "Lv2_2"
  },
  {
    "objectID": "Course/WKU2025 Class Information/lecture note template/chapter0.html#discussion",
    "href": "Course/WKU2025 Class Information/lecture note template/chapter0.html#discussion",
    "title": "TITLE",
    "section": "Discussion",
    "text": "Discussion"
  },
  {
    "objectID": "Course/WKU2025 Class Information/lecture note template/chapter0.html#summary",
    "href": "Course/WKU2025 Class Information/lecture note template/chapter0.html#summary",
    "title": "TITLE",
    "section": "Summary",
    "text": "Summary"
  },
  {
    "objectID": "Course/WKU2025 Class Information/lecture note template/chapter0.html#terms",
    "href": "Course/WKU2025 Class Information/lecture note template/chapter0.html#terms",
    "title": "TITLE",
    "section": "Terms",
    "text": "Terms\n:::"
  },
  {
    "objectID": "Course/WKU2025 Class Information/lecture note template/chapter0.html#more-to-read",
    "href": "Course/WKU2025 Class Information/lecture note template/chapter0.html#more-to-read",
    "title": "TITLE",
    "section": "More to Read",
    "text": "More to Read"
  },
  {
    "objectID": "Course/WKU2025 Class Information/lecture note template/chapter0.html#assignment",
    "href": "Course/WKU2025 Class Information/lecture note template/chapter0.html#assignment",
    "title": "TITLE",
    "section": "Assignment",
    "text": "Assignment\n\n\n\nWhat to do\n\n\n\nRequirement\n\nPDF format\nfile name should be include your student id and name\n\nstuID_name_title.pdf (e.g. 1111111_ChungilChae_SelfIntroduction.pdf)\n\n\nDue date\n\nby Feb23(Sun) 11:59PM\nNO LATE SUBMISSION ALLOWED!!!!"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#what-is-reserach",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#what-is-reserach",
    "title": "Ch1: The Purpose of Research",
    "section": "What is reserach?",
    "text": "What is reserach?"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#not-a-scientific-knowledge",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#not-a-scientific-knowledge",
    "title": "Ch1: The Purpose of Research",
    "section": "Not a Scientific Knowledge",
    "text": "Not a Scientific Knowledge\n\nTraditional Knowledge: a form of knowledge inherited from the culture one grew up in\nAuthority: a form of knowledge believed to be true because its source is authoritative, such as parents or teachers.\nExperiential knowledge: a form of knowledge gained through experiences"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#scientific-knowledge",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#scientific-knowledge",
    "title": "Ch1: The Purpose of Research",
    "section": "Scientific Knowledge",
    "text": "Scientific Knowledge\n\nScientific knowledge: a form of knowledge based on studies conducted by researchers; knowledge that can be trusted.\nNew scientific knowledge is produced by systematic research\nReality and knowledge are two different things\nResearch is generated by following sets of rules, embodying skills, and following a framework to analyze results."
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#karl-poppers-falsifiability",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#karl-poppers-falsifiability",
    "title": "Ch1: The Purpose of Research",
    "section": "Karl Popper’s Falsifiability",
    "text": "Karl Popper’s Falsifiability\n\nSystematic attempts to prove things wrong advance scientific knowledge\nEmpirical evidence: acquiring data or information by systematically observing people or events, i.e. practical experience.\nFalsified: proven an established theory wrong."
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#thomas-kuhns-structure-of-scientific-revolution",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#thomas-kuhns-structure-of-scientific-revolution",
    "title": "Ch1: The Purpose of Research",
    "section": "Thomas Kuhn’s Structure of Scientific Revolution",
    "text": "Thomas Kuhn’s Structure of Scientific Revolution\n\nNormal science: general rules, laws and paradigms accepted as truths; does not aim to explore new ideas or build on scientific knowledge, experiment, or risk.\nParadigm: an unchangeable pattern used over and over again; widely accepted by the scientific community.\nAnomalies: things that do not fit into the paradigms of normal science; happens once or twice and does not fit into commonly accepted patterns.\nCrisis: the accumulation of many anomalies against an accepted truth of normal science; encountered when normal science does not fit with reality anymore.\nRevolution: after a crisis, the revolution replaces the old paradigm with a new paradigm.\nParadigm shift: when the widely accepted paradigm encounters many anomalies, leading to crisis, then revolution, and settles into a new paradigm, which becomes accepted and the new normal science."
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#qualitative-research",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#qualitative-research",
    "title": "Ch1: The Purpose of Research",
    "section": "Qualitative Research",
    "text": "Qualitative Research\n\nQualitative research aims at gaining insight and depth; not just simple facts, but insights, emotions, perspectives, and so on about the topic of research\n\n\n\n\n\n\n\nInductive reasoning\n\n\nInductive reasoning: begins with specific observations and moves to a broader understanding of a topic or problem; often leads to creating new theories of science; allows researchers to become immersed in their study without preconceived notions or assumptions."
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#quantitative-research",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#quantitative-research",
    "title": "Ch1: The Purpose of Research",
    "section": "Quantitative Research",
    "text": "Quantitative Research\n\nQuantitative Research requires a good grasp of the topic of study and research conducted in that topic before data collection; the design of study is essential and their work is based on the systematic calculation of data\n\n\n\n\n\n\n\nDeductive reasoning\n\n\nDeductive reasoning begins with a broad theory that can lead to a specific idea or concept that is ready to be tested; data collection is straightforward with no digressions."
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#mixed-methods",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#mixed-methods",
    "title": "Ch1: The Purpose of Research",
    "section": "Mixed Methods",
    "text": "Mixed Methods\n\nMixed methods is a research design combine the best features of qualitative and quantitative methodologies.\nMixed-method design is useful in studies when traditional qualitative or quantitative methods will leave the study with significant limitations.\nThere are four most widely used types of mixed-methods:\n\nConvergent design is used when the qualitative and quantitative data are collected simultaneously; the data is analyzed separately and only brought together when compiling results.\nExplanatory sequential design\n\nQuantitative lead design is a two-phase process that starts with quantitative data collection, followed by qualitative collection of cases important to the study.\nQualitative lead design is a two-phase data collection that starts with qualitative and then is followed by quantitative methods.\n\nEmbedded design is when one qualitative or quantitative study is ongoing and collects different types of data before, during, or after the study."
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#ethics-and-research-ethics",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#ethics-and-research-ethics",
    "title": "Ch1: The Purpose of Research",
    "section": "Ethics and Research Ethics",
    "text": "Ethics and Research Ethics\n\nEthics: the group of morals and values that govern behaviors and decisions\nResearch ethics: research ethics use rules and regulations concerned with protecting the rights of people who participate in research studies."
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#ethical-rules",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#ethical-rules",
    "title": "Ch1: The Purpose of Research",
    "section": "Ethical Rules",
    "text": "Ethical Rules\n\nInformed consent provides participants with information about the study, particularly about any risks involved.\nConfidentiality is the promise not to disclose identifiable information about the participant to any third party.\nIt is unethical to coerce people into taking part in a study\nConflict of interest is also a concern; it refers to the possibility that the study may be part of an outside agenda."
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#a-violation-of-ethics",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#a-violation-of-ethics",
    "title": "Ch1: The Purpose of Research",
    "section": "A Violation of Ethics",
    "text": "A Violation of Ethics\n\nThe Tuskegee syphilis study is an infamous case of ethics violation in research.\n\n\nIt started with…\n\nBetween 1932 and 1972, the United States Public Health Service and the Tuskegee Institute conducted a study on the effects of syphilis on the human body."
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#researchers-biases",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#researchers-biases",
    "title": "Ch1: The Purpose of Research",
    "section": "Researchers’ Biases",
    "text": "Researchers’ Biases\n\nObjectivity: perceiving something from different angles without personal preferences or judgements.\nSubjective thinking: based on personal emotions, experiences, and prejudices.\nSelective observation: a common type of bias, when one is focused on a specific occurrence or group of people instead of including an entire sample of the observation; that is, focuses on what interests the researcher and failing to notice things that may contradict their theory.\nOverconfidence bias: when a researcher feels overconfident in their own abilities and does not consider additional details or aspect that could need more attention.\nOvergeneralization: using a small number of cases to draw conclusions about the entire population."
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#terms",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#terms",
    "title": "Ch1: The Purpose of Research",
    "section": "Terms",
    "text": "Terms\n\nAnomaly: something that does not fit into the paradigms of normal science.\nAuthority: a form of knowledge we believe to be true because it comes from authoritative sources, such as parents, teachers, and professional figures.\nCrisis: an accumulation of many anomalies against an accepted truth.\nDeductive reasoning: reasoning that begins with a broad theory that leads to a specific idea or concept to be tested.\nEmpirical evidence: acquiring information by systematically observing people or events.\nEthics: a set of guidelines that are primarily concerned with protecting the rights of study participants and are mandatory for the researcher.\nExperiential knowledge: a form of knowledge that we learn through pleasant or unpleasant experiences.\nFalsify: prove that a theory is incorrect.\nInductive reasoning: reasoning that begins with specific observations and moves to a broader understanding of a topic or problem."
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#discussion",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#discussion",
    "title": "Ch1: The Purpose of Research",
    "section": "Discussion",
    "text": "Discussion\n\nWhat is the difference between reality and knowledge?\nWhat is research methodology and what do we need it for?\nCan you think of examples that can illustrate Popper’s falsifiability?\nHow does inductive reasoning differ from deductive reasoning?\nWhat are some examples that can illustrate Kuhn’s paradigm shift?\nHow does traditional knowledge differ from subjective thinking?"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#more-to-read",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#more-to-read",
    "title": "Ch1: The Purpose of Research",
    "section": "More to Read",
    "text": "More to Read\n\nChalmers, A. F. (1999). What Is This Thing Called Science? (3rd Edition)\n\nThis book provides a foundational understanding of science and scientific inquiry. It delves into the nature of scientific knowledge, exploring ideas like falsifiability, paradigms, and how science progresses.\n\nPopper, K. (2002). The Logic of Scientific Discovery\n\nKarl Popper’s seminal work focuses on the philosophy of science, particularly his principle of falsifiability. It offers insights into how scientific theories are tested and improved through empirical evidence.\n\nKuhn, T. S. (2012). The Structure of Scientific Revolutions\n\nOne of the most influential texts on how science evolves, Kuhn discusses the concepts of paradigms, crises, and revolutionary shifts in scientific thought.\n\nCreswell, J. W., & Creswell, J. D. (2017). Research Design: Qualitative, Quantitative, and Mixed Methods Approaches\n\nThis book is an excellent guide for understanding the different research methodologies and how to design and conduct studies following scientific principles.\n\nZiman, J. (2000). Real Science: What It Is, and What It Means\n\nThis book examines how modern science operates, including its methodologies, ethics, and societal implications. It provides a philosophical and practical analysis of scientific research."
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#assignment",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter1/chapter1.html#assignment",
    "title": "Ch1: The Purpose of Research",
    "section": "Assignment",
    "text": "Assignment\n\n\n\nWrite an one page self-introduction\n\na brief self-introduction &lt; 300 words\nChinese name in English\nEnglish name\nStudent ID\nPicture that shows your face\nMajor\nClass name and section (e.g. GE2021, W09; MGS3001, W01)\n\n\n\n\nRequirement\n\nPDF format\nfile name should be include your student id and name\n\nstuID_name_title.pdf (e.g. 1111111_ChungilChae_SelfIntroduction.pdf)\n\nsend it to cchae@kean.edu with proper subject and contents of the mail\n\ndue date\n\nby Feb23(Sun) 11:59PM\nNO LATE SUBMISSION ALLOWED!!!!"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter7/chapter7.html#short-video-introduction",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter7/chapter7.html#short-video-introduction",
    "title": "TITLE",
    "section": "SHORT VIDEO INTRODUCTION",
    "text": "SHORT VIDEO INTRODUCTION"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter7/chapter7.html#lv1_1",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter7/chapter7.html#lv1_1",
    "title": "TITLE",
    "section": "Lv1_1",
    "text": "Lv1_1\n\nThis is level1 contents\nThis is level1 contents\nThis is level1 contents\nThis is level1 contents\nThis is level1 contents"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter7/chapter7.html#lv2_2",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter7/chapter7.html#lv2_2",
    "title": "TITLE",
    "section": "Lv2_2",
    "text": "Lv2_2"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter7/chapter7.html#discussion",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter7/chapter7.html#discussion",
    "title": "TITLE",
    "section": "Discussion",
    "text": "Discussion"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter7/chapter7.html#summary",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter7/chapter7.html#summary",
    "title": "TITLE",
    "section": "Summary",
    "text": "Summary"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter7/chapter7.html#terms",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter7/chapter7.html#terms",
    "title": "TITLE",
    "section": "Terms",
    "text": "Terms\n:::"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter7/chapter7.html#more-to-read",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter7/chapter7.html#more-to-read",
    "title": "TITLE",
    "section": "More to Read",
    "text": "More to Read"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter7/chapter7.html#assignment",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter7/chapter7.html#assignment",
    "title": "TITLE",
    "section": "Assignment",
    "text": "Assignment\n\n\n\nWhat to do\n\n\n\nRequirement\n\nPDF format\nfile name should be include your student id and name\n\nstuID_name_title.pdf (e.g. 1111111_ChungilChae_SelfIntroduction.pdf)\n\n\nDue date\n\nby Feb23(Sun) 11:59PM\nNO LATE SUBMISSION ALLOWED!!!!"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter9/chapter9.html#short-video-introduction",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter9/chapter9.html#short-video-introduction",
    "title": "TITLE",
    "section": "SHORT VIDEO INTRODUCTION",
    "text": "SHORT VIDEO INTRODUCTION"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter9/chapter9.html#lv1_1",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter9/chapter9.html#lv1_1",
    "title": "TITLE",
    "section": "Lv1_1",
    "text": "Lv1_1\n\nThis is level1 contents\nThis is level1 contents\nThis is level1 contents\nThis is level1 contents\nThis is level1 contents"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter9/chapter9.html#lv2_2",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter9/chapter9.html#lv2_2",
    "title": "TITLE",
    "section": "Lv2_2",
    "text": "Lv2_2"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter9/chapter9.html#discussion",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter9/chapter9.html#discussion",
    "title": "TITLE",
    "section": "Discussion",
    "text": "Discussion"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter9/chapter9.html#summary",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter9/chapter9.html#summary",
    "title": "TITLE",
    "section": "Summary",
    "text": "Summary"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter9/chapter9.html#terms",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter9/chapter9.html#terms",
    "title": "TITLE",
    "section": "Terms",
    "text": "Terms\n:::"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter9/chapter9.html#more-to-read",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter9/chapter9.html#more-to-read",
    "title": "TITLE",
    "section": "More to Read",
    "text": "More to Read"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter9/chapter9.html#assignment",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter9/chapter9.html#assignment",
    "title": "TITLE",
    "section": "Assignment",
    "text": "Assignment\n\n\n\nWhat to do\n\n\n\nRequirement\n\nPDF format\nfile name should be include your student id and name\n\nstuID_name_title.pdf (e.g. 1111111_ChungilChae_SelfIntroduction.pdf)\n\n\nDue date\n\nby Feb23(Sun) 11:59PM\nNO LATE SUBMISSION ALLOWED!!!!"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter11/chapter11.html#short-video-introduction",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter11/chapter11.html#short-video-introduction",
    "title": "TITLE",
    "section": "SHORT VIDEO INTRODUCTION",
    "text": "SHORT VIDEO INTRODUCTION"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter11/chapter11.html#lv1_1",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter11/chapter11.html#lv1_1",
    "title": "TITLE",
    "section": "Lv1_1",
    "text": "Lv1_1\n\nThis is level1 contents\nThis is level1 contents\nThis is level1 contents\nThis is level1 contents\nThis is level1 contents"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter11/chapter11.html#lv2_2",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter11/chapter11.html#lv2_2",
    "title": "TITLE",
    "section": "Lv2_2",
    "text": "Lv2_2"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter11/chapter11.html#discussion",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter11/chapter11.html#discussion",
    "title": "TITLE",
    "section": "Discussion",
    "text": "Discussion"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter11/chapter11.html#summary",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter11/chapter11.html#summary",
    "title": "TITLE",
    "section": "Summary",
    "text": "Summary"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter11/chapter11.html#terms",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter11/chapter11.html#terms",
    "title": "TITLE",
    "section": "Terms",
    "text": "Terms\n:::"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter11/chapter11.html#more-to-read",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter11/chapter11.html#more-to-read",
    "title": "TITLE",
    "section": "More to Read",
    "text": "More to Read"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter11/chapter11.html#assignment",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter11/chapter11.html#assignment",
    "title": "TITLE",
    "section": "Assignment",
    "text": "Assignment\n\n\n\nWhat to do\n\n\n\nRequirement\n\nPDF format\nfile name should be include your student id and name\n\nstuID_name_title.pdf (e.g. 1111111_ChungilChae_SelfIntroduction.pdf)\n\n\nDue date\n\nby Feb23(Sun) 11:59PM\nNO LATE SUBMISSION ALLOWED!!!!"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#short-video-introduction",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#short-video-introduction",
    "title": "Ch2: Formulating a Research Question",
    "section": "SHORT VIDEO INTRODUCTION",
    "text": "SHORT VIDEO INTRODUCTION"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#topic-choice",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#topic-choice",
    "title": "Ch2: Formulating a Research Question",
    "section": "Topic Choice",
    "text": "Topic Choice\nWhat is the anatomy of a topic? (Colquitt & George, 2011)\n\nSignificance: Taking on “Grand Challenges”\nNovelty: Changing the Conversation\nCuriosity: Catching and Holding Attention\nScope: Casting a Wider Net\nActionability: Insights for Practice"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#fundamental-vs-applied-research",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#fundamental-vs-applied-research",
    "title": "Ch2: Formulating a Research Question",
    "section": "Fundamental vs Applied Research",
    "text": "Fundamental vs Applied Research\n\nFundamental research: looks at the world at large and tries to generate new ideas or explanation about how the world works and why; may not have application in everyday life in the immediate sense.\nApplied research: seeks to solve a specific societal problem or uncover more information about a particular issue; has direct implications in practice."
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#narrowing-the-research-topic",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#narrowing-the-research-topic",
    "title": "Ch2: Formulating a Research Question",
    "section": "Narrowing the Research Topic",
    "text": "Narrowing the Research Topic\n\nTwo features of a broader topic can lead to the research question: the constructs and the population of interest.\n\nConstruct: a concept that is possible to measure in a form.\nThe population of interest: group of subject that share simialr charateristics or tied in a similar context that researcher have interested in"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#independent-and-dependent-variables",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#independent-and-dependent-variables",
    "title": "Ch2: Formulating a Research Question",
    "section": "Independent and Dependent Variables",
    "text": "Independent and Dependent Variables\n\nIndependent variable: the explanatory or predicting variable that explains the variation in the dependent variable; often constant/doesn’t change for the participant\nDependent variable: the outcome; what researchers want to find out from a study and what they hope is influenced by the independent variable.\nVariables are unique to each study; a variable can be dependent in one study and independent in another."
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#control-variables",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#control-variables",
    "title": "Ch2: Formulating a Research Question",
    "section": "Control Variables",
    "text": "Control Variables\n\nControl variables: any variables that are used to control the results; not directly related to the focus of the study but crucial for understanding the relationship between the variables of focus.\nThey help minimize biases and provide more accurate findings."
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#confounding-and-disturbance-variables",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#confounding-and-disturbance-variables",
    "title": "Ch2: Formulating a Research Question",
    "section": "Confounding and Disturbance Variables",
    "text": "Confounding and Disturbance Variables\n\nConfounding variables: influence the independent variable in such a way that the results from the dependent variable become untrustworthy; could not have been controlled or predicted during the study’s design; only become apparent during data collection or analysis.\nDisturbance variable: lurks in the background and disturbs the findings of the dependent variable; common characteristics of participants mislead the findings of the study without the researchers’ awareness."
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#moderators-and-mediators",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#moderators-and-mediators",
    "title": "Ch2: Formulating a Research Question",
    "section": "Moderators and Mediators",
    "text": "Moderators and Mediators\n\nModerators: variables that can strengthen or weaken an already established relationship between the independent and dependent variables.\nMediators: intervening variables that interfere with the relationship between the main variables; when a mediator is present, the relationship between the independent and dependent variables may not even exist anymore."
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#alternative-hypothesis-and-null-hypothesis",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#alternative-hypothesis-and-null-hypothesis",
    "title": "Ch2: Formulating a Research Question",
    "section": "Alternative Hypothesis and Null Hypothesis",
    "text": "Alternative Hypothesis and Null Hypothesis\n\nAlternative hypothesis: the prediction based on literature and theory about what the testing results will be; that is, what the variables are expected to look like once the data is collected.\nNull hypothesis: claims there is no relationship between the variables of interest in the study; this is the hypothesis researchers are actually testing with their findings.\nResearch outcomes have two options: reject the null hypothesis in favor of the alternative hypothesis, or fail to reject the null hypothesis."
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#directional-hypothesis-and-nondirectional-hypothesis",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#directional-hypothesis-and-nondirectional-hypothesis",
    "title": "Ch2: Formulating a Research Question",
    "section": "Directional Hypothesis and Nondirectional Hypothesis",
    "text": "Directional Hypothesis and Nondirectional Hypothesis\n\nDirectional hypothesis: predicts a specific course for the variables\nNondirectional hypothesis: has no direction and simply predicts a relationship between two or more variables; rather than making assumptions on how the variables will behaving, researchers are investigating the possible relationship between variables."
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#open-ended-question",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#open-ended-question",
    "title": "Ch2: Formulating a Research Question",
    "section": "Open-Ended Question",
    "text": "Open-Ended Question\n\nFor qualitative studies rather than quantitative; that is, when the researcher has many constructs but is not attempting to measure them\nA qualitative study asks different types of research questions and has a different methodology to collect data."
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#diagram",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#diagram",
    "title": "Ch2: Formulating a Research Question",
    "section": "Diagram",
    "text": "Diagram\n\nTry drawing the reserach question\nIt can be helpful to make a drawing of the research question to help visualize it."
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#discussion",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#discussion",
    "title": "Ch2: Formulating a Research Question",
    "section": "Discussion",
    "text": "Discussion\n\n\nA researcher is focused on how sleep (measured in hours) influences academic performance (measured in GPA).\nA researcher wants to find whether academic performance (measured in GPA) influences self-esteem (measured on a 1 to 10 scale).\nA researcher is studying the influence of television watching (measured in hours) on speech onset of toddlers (measured in number of words spoken by age of 2).\nA researcher is attempting to see whether self-esteem (measured on a scale of 1 to 10) relates to and/or influences substance abuse (measured in frequency of substance use and abuse) among adolescents.\nA researcher is interested in understanding whether exposure to a large variety of food (measured in the number of different food textures and food types) influences the level of pickiness among children younger than 6 years old (measured on a scale of 1 to 10).\n\n\nFind the dependent and the independent variables in these examples:\nWhat is the difference between the null and alternative hypotheses?\nWhat is the purpose of the null hypothesis?\nHow can we distinguish between disturbance and confounding variables? Illustrate with an example.\nWhat are some ways to operationalize constructs, such as sleep, time, and anxiety?\nWhat is the purpose of having a direction when we design an alternative hypothesis?\nWhat do directional and nondirectional hypotheses tell us?"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#summary",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#summary",
    "title": "Ch2: Formulating a Research Question",
    "section": "Summary",
    "text": "Summary\nThis chapter introduced the concepts of fundamental and applied research. We consider fundamental research those broad theories that attempt to explain how life works and can be applied to many different things rather than one specific problem.\n\nApplied research refers to studies that investigate one particular issue or problem. Once we have a topic of interest and have expressed this topic in the form of a question or a simple sentence, we are ready to operationalize our research. Operationalization means identifying the constructs of the study and expressing them in variables.\nConstructs are broad or general terms that cannot be measured straightforwardly, but need to be broken down to simpler measurements. Variables are simple, specific measures of one characteristic. A construct can have one variable or many variables. There are a few types of variables. Some basic variables are independent and dependent variables.\nAn independent variable attempts to predict the changes in the dependent variable. A dependent variable changes, or we expect it to change, as a result of the presence of the independent variable. Independent variables are known as predictors, whereas dependent variables are known as outcomes. Besides the main types of variables in our study, we also include other variables to make sure that our results are appropriate.\nControl variables, for example, are often not related to the main focus of the study, but they can influence the independent and dependent variables. To avoid biases and aim for higher accuracy in our work, we measure additional control variables. Confounding or intervening variables may influence the independent variable and question the findings of our study. In our attempt to protect the study from confounding or intervening variables, we look out for possible variables that may influence our results.\nDisturbance or extraneous variables are more difficult to distinguish and take measures before the start of the study. These are variables that are not related to the independent variable, but they can potentially create a problem for the dependent variable, leading us to false results.\nModerators and mediators can also influence the independent and dependent variables. Moderators are variables that can strengthen or weaken a relationship between the independent and dependent variable. These are powerful variables that can make results much stronger or weaker, but the relationship between the main variables is still present. Mediators are even stronger. Mediators can cause the relationship between the independent and dependent variable to completely disappear.\nThis chapter also discussed the concept of hypothesis and its types. A hypothesis is a statement that predicts the relationship between variables. Some hypotheses can have a direction and make specific predictions about how the variables will change. These are called directional hypotheses.\nOther types of hypotheses do not make specific assumptions about how the change in variables will be reflected—they just state the possibility of an association between variables. These are called nondirectional hypotheses or simply research questions, if stated in the form of a question.\nAnother categorization of hypotheses divides them into alternative and null. Alternative hypotheses are statements that predict some form of relationship between variables, either directional or nondirectional. Null hypotheses are the statements that claim no relationship between variables. Null hypotheses are the hypotheses we test in the study. Even though it seems like we are trying to prove the alternative hypotheses, we are in fact simply testing the null hypotheses or the possibility that there is no relationship between variables."
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#terms",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#terms",
    "title": "Ch2: Formulating a Research Question",
    "section": "Terms",
    "text": "Terms\n\nAlternative hypothesis: the hypothesis that attempts to predict how the variables will relate to each other after the data are collected.\nApplied research: research that seeks to solve a specific societal problem or uncover more information about a particular issue.\nConceptualization: the process of breaking a construct into smaller pieces and clarifying its specific meaning in our study.\nConfounding (intervening) variable: variable that influences that independent variable in a way that causes results from the dependent variable to become untrustworthy."
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#more-to-read",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#more-to-read",
    "title": "Ch2: Formulating a Research Question",
    "section": "More to Read",
    "text": "More to Read\n\nColquitt, J. A., & George, G. (2011). Publishing in AMJ—part 1: Topic choice. Academy of management journal. Academy of Management Briarcliff Manor, NY."
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#assignment",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#assignment",
    "title": "Ch2: Formulating a Research Question",
    "section": "Assignment",
    "text": "Assignment\n\nPresentation\n\n15 mins presentation\n5 mins questions and answers\n15 mins discussion\n\nDiscussion\n\nPresentation group lead discussion\nPrepare for some questions for students\nThink about how to facilitate other students participate discussion"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#section",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter2/chapter2.html#section",
    "title": "Ch2: Formulating a Research Question",
    "section": "",
    "text": "Exporing reserach topic\n\nSearch research articles of the subject that you may have interest in\nChoose three research topics/subjects\nWriting an academic report as an essay format, “three research topics” that includes (three topics in a report)\n\nMotivation(reason to choose)\nsummary of the related articles\nreference\n\n\n\n\n\nGroup presentation and refelection essay\n\nRead “Colquitt, J. A., & George, G. (2011). Publishing in AMJ—part 1: Topic choice. Academy of management journal. Academy of Management Briarcliff Manor, NY.” and prepare your discussion (all students in class)\nA group prepare for presentation of “Publishing in AMJ—part 1: Topic choice” and lead discussion.\nExcept students who present this week, other students submit a reflective essay of reading and presentation disuccsion."
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter4/chapter4.html#short-video-introduction",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter4/chapter4.html#short-video-introduction",
    "title": "TITLE",
    "section": "SHORT VIDEO INTRODUCTION",
    "text": "SHORT VIDEO INTRODUCTION"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter4/chapter4.html#lv1_1",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter4/chapter4.html#lv1_1",
    "title": "TITLE",
    "section": "Lv1_1",
    "text": "Lv1_1\n\nThis is level1 contents\nThis is level1 contents\nThis is level1 contents\nThis is level1 contents\nThis is level1 contents"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter4/chapter4.html#lv2_2",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter4/chapter4.html#lv2_2",
    "title": "TITLE",
    "section": "Lv2_2",
    "text": "Lv2_2"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter4/chapter4.html#discussion",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter4/chapter4.html#discussion",
    "title": "TITLE",
    "section": "Discussion",
    "text": "Discussion"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter4/chapter4.html#summary",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter4/chapter4.html#summary",
    "title": "TITLE",
    "section": "Summary",
    "text": "Summary"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter4/chapter4.html#terms",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter4/chapter4.html#terms",
    "title": "TITLE",
    "section": "Terms",
    "text": "Terms\n:::"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter4/chapter4.html#more-to-read",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter4/chapter4.html#more-to-read",
    "title": "TITLE",
    "section": "More to Read",
    "text": "More to Read"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter4/chapter4.html#assignment",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter4/chapter4.html#assignment",
    "title": "TITLE",
    "section": "Assignment",
    "text": "Assignment\n\n\n\nWhat to do\n\n\n\nRequirement\n\nPDF format\nfile name should be include your student id and name\n\nstuID_name_title.pdf (e.g. 1111111_ChungilChae_SelfIntroduction.pdf)\n\n\nDue date\n\nby Feb23(Sun) 11:59PM\nNO LATE SUBMISSION ALLOWED!!!!"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter14/chapter14.html#short-video-introduction",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter14/chapter14.html#short-video-introduction",
    "title": "TITLE",
    "section": "SHORT VIDEO INTRODUCTION",
    "text": "SHORT VIDEO INTRODUCTION"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter14/chapter14.html#lv1_1",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter14/chapter14.html#lv1_1",
    "title": "TITLE",
    "section": "Lv1_1",
    "text": "Lv1_1\n\nThis is level1 contents\nThis is level1 contents\nThis is level1 contents\nThis is level1 contents\nThis is level1 contents"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter14/chapter14.html#lv2_2",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter14/chapter14.html#lv2_2",
    "title": "TITLE",
    "section": "Lv2_2",
    "text": "Lv2_2"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter14/chapter14.html#discussion",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter14/chapter14.html#discussion",
    "title": "TITLE",
    "section": "Discussion",
    "text": "Discussion"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter14/chapter14.html#summary",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter14/chapter14.html#summary",
    "title": "TITLE",
    "section": "Summary",
    "text": "Summary"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter14/chapter14.html#terms",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter14/chapter14.html#terms",
    "title": "TITLE",
    "section": "Terms",
    "text": "Terms\n:::"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter14/chapter14.html#more-to-read",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter14/chapter14.html#more-to-read",
    "title": "TITLE",
    "section": "More to Read",
    "text": "More to Read"
  },
  {
    "objectID": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter14/chapter14.html#assignment",
    "href": "Course/GE2021/03_Lecture_Note/LectureNote_Chad/PPT/chapter14/chapter14.html#assignment",
    "title": "TITLE",
    "section": "Assignment",
    "text": "Assignment\n\n\n\nWhat to do\n\n\n\nRequirement\n\nPDF format\nfile name should be include your student id and name\n\nstuID_name_title.pdf (e.g. 1111111_ChungilChae_SelfIntroduction.pdf)\n\n\nDue date\n\nby Feb23(Sun) 11:59PM\nNO LATE SUBMISSION ALLOWED!!!!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Chad’s Course Portfolio",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  }
]